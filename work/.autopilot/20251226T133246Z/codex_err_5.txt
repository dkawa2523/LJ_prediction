OpenAI Codex v0.77.0 (research preview)
--------
workdir: /Users/kawahito/Desktop/LJ_prediction
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: xhigh
reasoning summaries: auto
session id: 019b5b21-7cee-7323-bdf4-a1f070bade78
--------
user
# ===== SESSION CONTEXT =====
# Codex Session Context（毎回貼る短文）

- このrepoは「化学構造（SDF/CSV）→ 物性予測」を行う CLI/Hydra 基盤。
- 不変条件は docs/00_INVARIANTS.md が唯一の正。
- 今回の作業範囲・要件は work/tasks/<TASK>.md が唯一の正。
- 学習と推論の特徴量/前処理は必ず一致させる（skew禁止）。
- 成果物（artifact）は docs/04_ARTIFACTS_AND_VERSIONING.md を守る。
- 破壊的変更が必要なら work/rfc → docs/adr を先に作る（勝手に壊さない）。
- すべての処理は Process（処理単位）としてHydraで管理し、単独実行できる形を守る。
- 将来ClearML Task化を想定し、各Processが artifact を明確に出す設計にする（実装は今しない）。

# ===== AUTOPILOT MODE (non-interactive) =====
あなたは自動実行ループの中で動いています。**停止しないための規約**として次を厳守してください：

【禁止】
- ユーザーへの確認・質問・承認依頼（"確認してください" / "進めてよいですか" / "Approve" 等）は一切しない。
- "今回は未実装" / "差分なし" のまま終了しない（必ず前進する）。

【single source of truth】
- タスクの status の唯一の真実は work/queue.json。task.md の 'Blocked' 記述は参考情報。
  - queue が todo/in_progress なら実装を進める（task.md 側の Blocked を理由に止めない）。
  - queue が blocked なら blocked-handling を行う（reason/unblock_condition/next_action を整備し、解除子タスクを起票）。

【stale Blocked の扱い】
- task.md に '## Blocked' が残っていても queue が todo/in_progress の場合、それは **stale**。
  - task.md を更新し、Blocked を Notes/History に移す or 解除した上で実装を進める。

【書き込み権限】
- **書き込みは可能**な前提。work/queue.json 更新を含む必要な変更は実施する。
  - もし実際にツール/環境エラーで書き込めない場合：
    1) エラーメッセージを短く引用
    2) 最小の対処（sandbox/approval/flags 見直し、権限確認）を提示
    3) タスクを blocked にして解除子タスクを起票（子に unblocks を付与）
    ※それでも『確認してください』は書かない。

【必ずどれかを実施（Decision Protocol）】
A) 実装を進める → テスト/検証 → work/queue.json を done に更新
B) 進められない → work/queue.json を blocked に更新し、解除子タスク（unblocks付き）を作成
C) 依存/状態がズレている → work/queue.json と task.md を修正して整合させた上で A へ

# ===== SELECTED TASK =====
- id: 075
- title: 結果集計Process（leaderboard/比較レポート生成）
- priority: P0
- status: in_progress
- path: work/tasks/075_leaderboard_process.md

# ===== CONTRACTS TO FOLLOW (open & read) =====
- docs/00_INVARIANTS.md
- docs/04_ARTIFACTS_AND_VERSIONING.md
- docs/10_PROCESS_CATALOG.md

# ===== SKILLS TO FOLLOW (open & follow) =====
- agentskills/skills/S70_evaluator_metrics.md
- agentskills/skills/S10_hydra_config.md
- agentskills/skills/S95_tests_ci.md


# ===== TASK FILE (single source of truth for *intent*) =====
# Task 075 (P0): 結果集計Process（leaderboard/比較レポート生成）

## 目的（Why）
- モデル/特徴量/タスクの比較を「人力でrunフォルダを見に行く」状態から脱却する。
- multirunで大量実験したとき、**どれが良いか即分かる**ようにする。
- 将来ClearMLに移行しても、同じ “比較レポート” の発想で繋げられる。

## 背景（Context）
- 比較の正しさには、(a)同一split、(b)同一指標定義、(c)metaが揃っていることが必要。
- `metrics.json` / `meta.json` / `config.yaml` を契約化しているので、集計は自動化できる。

## スコープ（Scope）
### In scope
- **新Process `leaderboard`（または `aggregate_results`）** を追加（1 script = 1 process）
  - `scripts/leaderboard.py`（例）
- 入力：`runs/` 配下（または指定root）をスキャンし、以下が揃うrunを集計
  - `meta.json`
  - `metrics.json`
  - （任意）`config.yaml`
- 出力（artifact）
  - `leaderboard.csv`
  - `leaderboard.md`（上位N件、条件付き）
  - `plots/`（任意：metric vs timeなど）

### Out of scope（今回はやらない）
- ClearML SDKでのアップロード（設計準拠だけ）
- 完全なWeb UI

## 影響（Contract Impact）
- Process追加なので `docs/10_PROCESS_CATALOG.md` を更新
- 既存runを壊さない（read-onlyで集計する）

## 実装計画（Plan）
1) `docs/10_PROCESS_CATALOG.md` に leaderboard を追記（入力/出力）
2) `scripts/leaderboard.py` を追加
   - `leaderboard.root_dir`（デフォルト runs/）
   - `leaderboard.metric_key`（例 r2）
   - `leaderboard.sort_order`（desc）
   - `leaderboard.filters`（task/model/features、期間など）
3) `configs/process/leaderboard.yaml`（入口）と `configs/leaderboard/default.yaml` を追加
4) テスト追加
   - `tests/` でテンポラリrunディレクトリを作り、ダミーの meta/metrics を置いて集計できること

## 受け入れ条件（Acceptance Criteria）
- [ ] `python scripts/leaderboard.py ...` が単独で実行できる（Hydra管理）
- [ ] `leaderboard.csv` が生成される
- [ ] 필터（task/model/featuresの少なくとも1つ）が動く
- [ ] `docs/10_PROCESS_CATALOG.md` が更新されている
- [ ] pytestに最低1つ追加テスト

## 検証手順（How to Verify）
- 例：
  - `python scripts/leaderboard.py leaderboard.root_dir=runs leaderboard.metric_key=r2`
  - `pytest -q`

## メモ
- これができると、HPOやアンサンブルの比較が一気に回しやすくなる

# ===== OUTPUT REQUIREMENTS (mandatory) =====
1) 変更計画（ファイル単位）
2) 実装（差分が分かるように）
3) 追加/更新したテスト
4) 検証コマンド
5) 互換性影響（config/CLI/artifact）
6) タスク完了時：work/queue.json の status を done に更新（満たせない場合は blocked と理由）


mcp: context7 starting
mcp: context7 ready
mcp startup: ready: context7

thinking
**Preparing to review required skill files**
exec
/bin/zsh -lc ls in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
README.md
agentskills
codex
configs
data
docs
learning_curve_val.png
objective.md
parity_test.png
parity_val.png
requirements.txt
residual_test.png
residual_val.png
runs
scripts
src
tests
tools
work
y_train_hist.png

thinking
**Opening skills file**
exec
/bin/zsh -lc 'cat agentskills/skills/S70_evaluator_metrics.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
# S70 Evaluator & Metrics

## Purpose
評価指標・可視化を追加し、比較可能性を高める。

## Allowed Changes
- src/common/metrics.py（存在するなら）または src/eval/**
- configs/eval/**
- tests/**

## Pitfalls
- metric 名の変更で互換を壊す
- CSV 出力の列を増やす時に契約更新を忘れる


## 比較評価（推奨）
- evaluate は `metrics.json` と `predictions.csv` を必ず出し、後から集計できる形にする
- 将来：複数runの集計（leaderboard）を別Processとして追加する

thinking
**Noticing brief skill mention**
exec
/bin/zsh -lc 'cat agentskills/skills/S10_hydra_config.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
# S10 Hydra Config

## Purpose
Hydra config を group 化し、設定の再利用と組み合わせを容易にする。

## When to Use
- config が増えて混乱してきた
- モデル/特徴量/タスクを切り替えたい

## Inputs
- docs/03_CONFIG_CONVENTIONS.md
- work/tasks/010_reorganize_hydra_configs.md

## Allowed Changes
- configs/**
- scripts/*（config読込部分の更新）
- src/utils/validate_config.py（追加）

## Common Pitfalls
- defaults の上書き順序ミス

thinking
**Requesting S95 tests**
exec
/bin/zsh -lc 'cat agentskills/skills/S95_tests_ci.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
# S95 Tests & CI

## Purpose
pytest + smoke + contract test を整備し、改修で壊れないようにする。

## Allowed Changes
- tests/**
- pyproject.toml（任意）
- .github/workflows/**（任意）

## Pitfalls
- smoke が重くなりすぎる
- 乱数により flaky になる

thinking
**Gathering relevant documentation**
exec
/bin/zsh -lc 'cat docs/00_INVARIANTS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
# Invariants（不変条件）

このドキュメントは **開発中に変えてはいけない契約（Codexも遵守）** です。  
迷ったらここへ戻り、ここに書かれていない“勝手な最適化”をしないでください。

---

## 0. 基本目的（この基盤のゴール）
- **入力**: 化学構造（主に **CSV + SDF**。将来もこの形式を軸にする）
- **出力**: 物性（LJパラメータに限らず、**同じCSV/SDF形式のまま目的変数を差し替える**）
- **操作**: **CLI**（**Hydra**で設定管理）
- **要求**:
  - 前処理・特徴量化・学習・評価・推論・可視化を **拡張しやすい**
  - モデル/特徴量の手法が複数あり、**選択して実行・比較評価しやすい**

---

## 1. もっとも重要な不変条件（破るなら RFC→ADR→更新）

### 1.1 「処理単位（Process）」が第一級（最重要）

thinking
**Reviewing artifacts documentation**
exec
/bin/zsh -lc 'cat docs/04_ARTIFACTS_AND_VERSIONING.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
# Artifacts & Versioning（成果物・再現性）

このドキュメントは「各Processが独立して実行でき、比較・再現できる」ための成果物（artifact）契約です。  
将来ClearML Task化を見越し、**Process単位で入力/出力が追跡できる**形にします。

---

## 1. 出力ディレクトリ（推奨形）
Hydraの出力ディレクトリ（例：`runs/<process>/<date>/<time>_<experiment>/`）に保存する。

- `<process>` は `train`/`predict`/`evaluate` 等の処理単位
- 1 run = 1 Process 実行

---

## 2. 必須ファイル（全Process共通）
- `config.yaml`：最終 config（Hydra合成結果）
- `meta.json`：メタ情報（下記必須キー）
- `logs/`：ログ（形式は自由だが存在推奨）


thinking
**Preparing to process catalog documentation**
exec
/bin/zsh -lc 'cat docs/10_PROCESS_CATALOG.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
# Process Catalog（処理単位の一覧とI/O）

このドキュメントは **Process（処理単位）** のカタログです。  
「どのProcessが存在し、何を入力として、何を出力するか」を固定し、拡張してもブレないようにします。

---

## 1. Process一覧（推奨）
| Process | Script（推奨） | 主入力 | 主出力（artifact） | ClearML化の単位 |
|---|---|---|---|---|
| build_dataset | scripts/build_dataset.py | raw CSV/SDF | dataset index/split, dataset_hash | Dataset Task |
| audit_dataset | scripts/audit_dataset.py | raw CSV/SDF or processed dataset | audit_report.json, audit_report.md, plots | Audit Task |
| featurize | scripts/featurize.py | dataset artifact | features cache/manifest | Task |
| train | scripts/train.py | dataset(+features), task, model, train cfg | model.ckpt, metrics | Training Task |
| evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
| predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
| visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
| collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |

※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。

thinking
**Opening task file 075**
exec
/bin/zsh -lc 'cat work/tasks/075_leaderboard_process.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
# Task 075 (P0): 結果集計Process（leaderboard/比較レポート生成）

## 目的（Why）
- モデル/特徴量/タスクの比較を「人力でrunフォルダを見に行く」状態から脱却する。
- multirunで大量実験したとき、**どれが良いか即分かる**ようにする。
- 将来ClearMLに移行しても、同じ “比較レポート” の発想で繋げられる。

## 背景（Context）
- 比較の正しさには、(a)同一split、(b)同一指標定義、(c)metaが揃っていることが必要。
- `metrics.json` / `meta.json` / `config.yaml` を契約化しているので、集計は自動化できる。

## スコープ（Scope）
### In scope
- **新Process `leaderboard`（または `aggregate_results`）** を追加（1 script = 1 process）
  - `scripts/leaderboard.py`（例）
- 入力：`runs/` 配下（または指定root）をスキャンし、以下が揃うrunを集計
  - `meta.json`
  - `metrics.json`
  - （任意）`config.yaml`
- 出力（artifact）

thinking
**Checking and updating work queue**
exec
/bin/zsh -lc 'cat work/queue.json' in /Users/kawahito/Desktop/LJ_prediction succeeded in 241ms:
{
  "version": 1,
  "updated_at": "2025-12-26T14:47:59.823070Z",
  "policy": {
    "priority_order": [
      "P0",
      "P1",
      "P2",
      "P3"
    ],
    "status_order": [
      "todo",
      "in_progress",
      "blocked",
      "done"
    ],
    "auto_set_in_progress_on_next": true
  },
  "tasks": [
    {

thinking
**Checking configuration conventions documentation**
exec
/bin/zsh -lc 'cat docs/03_CONFIG_CONVENTIONS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
# Hydra Config Conventions（設定規約）

このドキュメントは Hydra を用いた設定管理の規約です。  
**Process（処理単位）をHydraで統一管理**し、手法の切替・比較評価をしやすくします。

---

## 1. 設定のゴール
- **各Processが単独で実行可能**
- モデル/特徴量/タスクの組合せを安全に切替可能
- 比較評価（multirun）しても成果物が追跡可能
- 将来ClearMLで Task 化しても、同じconfigを使い回せる

---

## 2. config group 構造（必須）
- `process/`：どのProcessを実行するか（脚本ごとに固定でもよいが、共通鍵は持つ）
- `dataset/`：入力データ・列マッピング・split・フィルタ
- `task/`：目的変数/タスクタイプ（regression/classification/multitask）
- `preprocess/`：正規化/3D生成/欠損処理

thinking
**Inspecting leaderboard implementation approach**
exec
/bin/zsh -lc 'rg --files scripts configs src tests' in /Users/kawahito/Desktop/LJ_prediction succeeded in 279ms:
tests/test_fixture_dataset.py
tests/test_splitters.py
scripts/visualize.py
configs/config.yaml
configs/audit_dataset_fixture.yaml
scripts/evaluate.py
scripts/train.py
configs/dataset_quick.yaml
configs/audit_dataset.yaml
configs/dataset_fixture.yaml
configs/config_quick.yaml
scripts/build_dataset.py
scripts/predict.py
scripts/audit_dataset.py
configs/fp/visualize_quick.yaml
configs/fp/train_quick.yaml
configs/fp/visualize.yaml
configs/fp/evaluate.yaml
configs/fp/evaluate_quick.yaml
configs/fp/predict_quick.yaml

thinking
**Inspecting training and evaluation scripts**
exec
/bin/zsh -lc "sed -n '1,200p' scripts/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 260ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict

# Allow running as `python scripts/train.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import train as fp_train
from src.gnn import train as gnn_train


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}


thinking
**Checking script config loading**
exec
/bin/zsh -lc "sed -n '1,220p' src/common/config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
from __future__ import annotations

from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, Iterable

import yaml


class ConfigError(ValueError):
    pass


def load_yaml(path: str | Path) -> Dict[str, Any]:
    path = Path(path)
    if not path.exists():
        raise ConfigError(f"Config file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if cfg is None:

thinking
**Reviewing script patterns for config usage**
exec
/bin/zsh -lc "sed -n '1,220p' scripts/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path

# Allow running as `python scripts/evaluate.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import evaluate as fp_evaluate
from src.gnn import evaluate as gnn_evaluate


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}



thinking
**Guessing script purpose**
exec
/bin/zsh -lc "sed -n '1,220p' scripts/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict

# Allow running as `python scripts/predict.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import predict as fp_predict
from src.gnn import predict as gnn_predict


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}


thinking
**Visualizing script**
exec
/bin/zsh -lc "sed -n '1,220p' scripts/visualize.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path

import pandas as pd

# Allow running as `python scripts/visualize.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.plots import save_hist, save_parity_plot, save_residual_plot
from src.common.utils import ensure_dir, get_logger
from src.utils.validate_config import validate_config



thinking
**Inspecting build and audit scripts**
exec
/bin/zsh -lc "sed -n '1,220p' scripts/audit_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict

# Allow running as `python scripts/audit_dataset.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.plots import save_hist
from src.common.utils import ensure_dir, get_logger, save_json
from src.data.audit import audit_dataset
from src.utils.validate_config import validate_config



thinking
**Inspecting build_dataset script**
exec
/bin/zsh -lc "sed -n '1,220p' scripts/build_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 295ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

# Allow running as `python scripts/build_dataset.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import dump_yaml, load_config
from src.common.chemistry import elements_string, n_elements, parse_formula
from src.common.dataset_selectors import SelectorContext, apply_selectors
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv

thinking
**Preparing to inspect meta.py**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/meta.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:
from __future__ import annotations

from src.utils.artifacts import build_meta, save_meta, utc_now_iso

__all__ = ["build_meta", "save_meta", "utc_now_iso"]

thinking
**Inspecting utils module**
exec
/bin/zsh -lc "sed -n '1,260p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

import csv
import hashlib
import json
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Optional
from uuid import uuid4

from src.common.utils import save_json

REQUIRED_META_KEYS = [
    "run_id",
    "process_name",
    "created_at",
    "git_sha",
    "dataset_hash",
    "config_hash",

thinking
**Reviewing tests for leaderboard process**
exec
/bin/zsh -lc "sed -n '1,220p' tests/contract/test_artifacts_contract.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
from __future__ import annotations

from pathlib import Path

from src.utils.artifacts import (
    REQUIRED_META_KEYS,
    build_meta,
    compute_config_hash,
    compute_dataset_hash,
    save_meta,
    validate_evaluate_artifacts,
    validate_predict_artifacts,
    validate_train_artifacts,
)


def _write_indices(indices_dir: Path) -> None:
    indices_dir.mkdir(parents=True, exist_ok=True)
    (indices_dir / "train.txt").write_text("0\n", encoding="utf-8")
    (indices_dir / "val.txt").write_text("0\n", encoding="utf-8")

thinking
**Searching for validate_* functions**
exec
/bin/zsh -lc 'rg -n "validate_.*_artifacts" -n src/utils/artifacts.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 294ms:
267:def validate_common_artifacts(run_dir: str | Path) -> None:
278:def validate_train_artifacts(run_dir: str | Path) -> None:
280:    validate_common_artifacts(run_dir)
289:def validate_evaluate_artifacts(run_dir: str | Path) -> None:
291:    validate_common_artifacts(run_dir)
299:def validate_predict_artifacts(run_dir: str | Path) -> None:
301:    validate_common_artifacts(run_dir)
exec
/bin/zsh -lc "sed -n '240,340p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
        "model_version": train_meta.get("model_version") or train_meta.get("run_id"),
    }
    if context["model_version"] is None and model_artifact_dir is not None:
        context["model_version"] = model_artifact_dir.name
    return context


def validate_meta(meta: Dict[str, Any]) -> None:
    missing = [k for k in REQUIRED_META_KEYS if k not in meta]
    if missing:
        raise ValueError(f"meta.json missing keys: {', '.join(missing)}")


def validate_predictions_csv(path: str | Path) -> None:
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"predictions.csv not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader, [])

thinking
**Inspecting configuration setup**
exec
/bin/zsh -lc "sed -n '1,200p' configs/config.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_desc
  - model: fp_lightgbm
  - train: fp_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"

thinking
**Reviewing configuration and process flow**
exec
/bin/zsh -lc "sed -n '1,200p' configs/process/train.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 256ms:
process:
  name: "train"
  kind: "train"

output:
  run_dir: "runs/train"

thinking
**Assessing next steps**
exec
/bin/zsh -lc 'cat configs/process/evaluate.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
process:
  name: "evaluate"
  kind: "evaluate"

output:
  run_dir: "runs/evaluate"

thinking
**Estimating next steps**
exec
/bin/zsh -lc 'cat configs/process/predict.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
process:
  name: "predict"
  kind: "predict"

output:
  out_dir: "runs/predict"

thinking
**Visualizing**
exec
/bin/zsh -lc 'cat configs/process/visualize.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:
process:
  name: "visualize"
  kind: "visualize"

output:
  out_dir: "runs/visualize"

thinking
**Auditing dataset process**
exec
/bin/zsh -lc 'cat configs/process/audit_dataset.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
process:
  name: "audit_dataset"
  kind: "audit"

output:
  run_dir: "runs/audit"

thinking
**Building dataset process**
exec
/bin/zsh -lc 'cat configs/process/build_dataset.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
process:
  name: "build_dataset"
  kind: "build_dataset"

thinking
**Investigating config setup for leaderboard**
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/evaluate.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
defaults:
  - process: evaluate
  - dataset: default
  - task: lj_epsilon
  - eval: default
  - hydra: default
  - _self_

model_artifact_dir: "runs/train/fp/lgbm_morgan_desc_v1"

process:
  backend: "fp"

experiment:
  name: "fp_evaluate"

output:
  exp_name: "fp_evaluate"

thinking
**Predicting configuration**
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/predict.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
defaults:
  - process: predict
  - dataset: default
  - infer: fp_default
  - hydra: default
  - _self_

model_artifact_dir: "runs/train/fp/lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  exp_name: "fp_predict"

thinking
**Visualizing configuration**
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/visualize.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
defaults:
  - process: visualize
  - hydra: default
  - _self_

input:
  evaluate_run_dir: "runs/evaluate/fp_evaluate"

plots:
  splits: ["val", "test"]
  include_train_hist: true

process:
  backend: "fp"

experiment:
  name: "fp_visualize"

output:
  exp_name: "fp_visualize"

thinking
**Planning leaderboard config structure**
exec
/bin/zsh -lc "sed -n '1,200p' configs/dataset.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
defaults:
  - process: build_dataset
  - dataset: default
  - task: lj_epsilon
  - hydra: default
  - _self_

thinking
**Checking for audit_dataset.yaml**
exec
/bin/zsh -lc "sed -n '1,200p' configs/audit_dataset.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 293ms:
defaults:
  - process: audit_dataset
  - dataset: default
  - task: lj_epsilon
  - audit: default
  - hydra: default
  - _self_

experiment:
  name: "audit_dataset"

output:
  exp_name: "audit_dataset"

thinking
**Clarifying leaderboard config structure**
exec
/bin/zsh -lc "sed -n '1,200p' configs/eval/default.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 278ms:
ad:
  tanimoto_warn_threshold: 0.5
  top_k: 5

output:
  plots: false

thinking
**Evaluating config structure and planning additions**

I’m sorting out how leaderboard configs fit into the existing structure—there’s a group default but no top-level config yet. I’m leaning toward adding a top-level `configs/leaderboard.yaml` for consistency and easy script running, keeping in mind the plan’s scope and minimizing required changes. I’ll also prepare to add related tests and utilities.
exec
/bin/zsh -lc "sed -n '1,240p' src/common/utils.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
from __future__ import annotations

import json
import logging
import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np


def ensure_dir(path: str | Path) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def set_seed(seed: int) -> None:

thinking
**Checking ASCII handling and metrics alignment**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/metrics.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


def regression_metrics(y_true, y_pred) -> Dict[str, float]:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = float(mean_squared_error(y_true, y_pred))
    return {
        "mae": float(mean_absolute_error(y_true, y_pred)),
        "rmse": float(np.sqrt(mse)),
        "r2": float(r2_score(y_true, y_pred)),
    }

thinking
**Inspecting evaluation code**
exec
/bin/zsh -lc "sed -n '1,240p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.metrics import regression_metrics
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.feature_utils import hash_cfg
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config


thinking
**Inspecting nested metrics structure**
exec
/bin/zsh -lc 'rg -n "metrics.json" -n src/fp src/gnn' in /Users/kawahito/Desktop/LJ_prediction succeeded in 279ms:
src/gnn/train.py:391:        run_dir / "metrics.json",
src/gnn/evaluate.py:244:        run_dir / "metrics.json",
src/fp/evaluate.py:147:        run_dir / "metrics.json",
src/fp/train.py:244:        run_dir / "metrics.json",
exec
/bin/zsh -lc "sed -n '200,320p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
                pass

    # Save artifacts
    artifacts_dir = ensure_dir(run_dir / "artifacts")
    save_feature_pipeline(pipeline, artifacts_dir)
    pipeline.save_preprocess_artifacts(artifacts_dir)
    model_path = artifacts_dir / "model.pkl"
    with open(model_path, "wb") as f:
        pickle.dump(model, f)

    # AD artifacts: training fingerprints and training elements
    # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
    # We'll store only training set for speed.
    train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
    train_fps = [
        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
    ]
    # Filter None
    train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
    train_fps = [p[0] for p in train_pairs]

thinking
**Comparing metric structures**
exec
/bin/zsh -lc "sed -n '350,440p' src/gnn/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
        history_train.append(train_loss)
        history_val.append(val_rmse)
        logger.info(f"Epoch {epoch:04d}: train_loss={train_loss:.6g} val_rmse={val_rmse:.6g}")

        if (val_rmse < best_val_rmse) or (not best_path.exists()):
            best_val_rmse = val_rmse
            torch.save(model.state_dict(), best_path)
            bad_epochs = 0
        else:
            bad_epochs += 1
            if bad_epochs >= patience:
                logger.info(f"Early stopping at epoch {epoch} (patience={patience})")
                break

    # Load best
    model.load_state_dict(torch.load(best_path, map_location=device))

    val_metrics, yv, pv = eval_loader(val_loader)
    test_metrics, yt, pt = eval_loader(test_loader)
    logger.info(f"Best val metrics: {val_metrics}")

thinking
**Checking GNN metrics format**
exec
/bin/zsh -lc "sed -n '220,300p' src/gnn/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
        y_true, y_pred = eval_loader(data_list)
        if len(y_true) == 0:
            metrics_by_split[split_name] = {}
            continue
        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
        for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
            rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})

    pred_df = pd.DataFrame(rows)
    pred_df["model_name"] = train_context.get("model_name")
    pred_df["model_version"] = train_context.get("model_version")
    pred_df["dataset_hash"] = dataset_hash
    pred_df["run_id"] = meta["run_id"]
    pred_path = run_dir / "predictions.csv"
    pred_df.to_csv(pred_path, index=False)
    logger.info(f"Saved predictions to {pred_path}")

    for split_name, metrics in metrics_by_split.items():
        if not metrics:
            continue

thinking
**Designing leaderboard parsing and output**

I’m mapping out how to robustly parse varied `metrics.json` structures and filter runs by metadata, planning a flexible config with keys for metric paths and filters like task and model names. I’ll implement recursive scanning of run directories for paired `meta.json` and `metrics.json`, read them read-only, and produce standardized output directories with metadata and logs following artifact contracts. I also want to add tests that simulate runs to verify filtering and output correctness. There is a lot here to ensure consistency and flexibility!
exec
/bin/zsh -lc "sed -n '1,200p' tests/conftest.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
from __future__ import annotations

import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

thinking
**Planning leaderboard implementation and config validation**
exec
/bin/zsh -lc "sed -n '1,240p' src/utils/validate_config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
from __future__ import annotations

from typing import Any, Dict, Iterable

from src.common.config import ConfigError


def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
    missing = [k for k in keys if k not in cfg]
    if missing:
        raise ConfigError(f"{context} missing keys: {', '.join(missing)}")


def _has_target(cfg: Dict[str, Any]) -> bool:
    data = cfg.get("data", {})
    task = cfg.get("task", {})
    return bool(data.get("target_col") or task.get("target_col"))


def validate_config(cfg: Dict[str, Any]) -> None:

thinking
**Designing leaderboard validation and config**

I'll plan to add validation for the leaderboard config in `validate_config` since it's allowed under S10 changes. I'll design new config files under `configs/process/leaderboard.yaml` and `configs/leaderboard/default.yaml` with clear structures for process info, output directories, and leaderboard-specific keys like metric_key, split, sorting, filters, and include_missing_metric. I'll also think about how to process metrics from various formats and generate leaderboard.csv and leaderboard.md outputs combining meta and metrics info. It’s a detailed setup but necessary for consistent leaderboard processing.
exec
/bin/zsh -lc 'rg -n "leaderboard|aggregate_results" -S .' in /Users/kawahito/Desktop/LJ_prediction succeeded in 312ms:
./agentskills/skills/S70_evaluator_metrics.md:18:- 将来：複数runの集計（leaderboard）を別Processとして追加する
./work/queue.json:226:      "title": "結果集計Process（leaderboard/比較レポート生成）",
./work/queue.json:227:      "path": "work/tasks/075_leaderboard_process.md",
./work/tasks/075_leaderboard_process.md:1:# Task 075 (P0): 結果集計Process（leaderboard/比較レポート生成）
./work/tasks/075_leaderboard_process.md:14:- **新Process `leaderboard`（または `aggregate_results`）** を追加（1 script = 1 process）
./work/tasks/075_leaderboard_process.md:15:  - `scripts/leaderboard.py`（例）
./work/tasks/075_leaderboard_process.md:21:  - `leaderboard.csv`
./work/tasks/075_leaderboard_process.md:22:  - `leaderboard.md`（上位N件、条件付き）
./work/tasks/075_leaderboard_process.md:34:1) `docs/10_PROCESS_CATALOG.md` に leaderboard を追記（入力/出力）
./work/tasks/075_leaderboard_process.md:35:2) `scripts/leaderboard.py` を追加
./work/tasks/075_leaderboard_process.md:36:   - `leaderboard.root_dir`（デフォルト runs/）
./work/tasks/075_leaderboard_process.md:37:   - `leaderboard.metric_key`（例 r2）
./work/tasks/075_leaderboard_process.md:38:   - `leaderboard.sort_order`（desc）
./work/tasks/075_leaderboard_process.md:39:   - `leaderboard.filters`（task/model/features、期間など）
./work/tasks/075_leaderboard_process.md:40:3) `configs/process/leaderboard.yaml`（入口）と `configs/leaderboard/default.yaml` を追加
./work/tasks/075_leaderboard_process.md:45:- [ ] `python scripts/leaderboard.py ...` が単独で実行できる（Hydra管理）
./work/tasks/075_leaderboard_process.md:46:- [ ] `leaderboard.csv` が生成される
./work/tasks/075_leaderboard_process.md:53:  - `python scripts/leaderboard.py leaderboard.root_dir=runs leaderboard.metric_key=r2`
./docs/04_ARTIFACTS_AND_VERSIONING.md:95:将来、複数runの `metrics.json` を集計して leaderboard を作る想定。  

thinking
**Investigating config parsing and CLI usage**
exec
/bin/zsh -lc 'rg -n "--config" scripts' in /Users/kawahito/Desktop/LJ_prediction exited 2 in 269ms:
rg: unrecognized flag --config

similar flags that are available: --no-config

thinking
**Correcting ripgrep command syntax**
exec
/bin/zsh -lc 'rg -n -- "--config" scripts' in /Users/kawahito/Desktop/LJ_prediction succeeded in 262ms:
scripts/build_dataset.py:48:    ap.add_argument("--config", required=True, help="Path to configs/dataset.yaml")
scripts/evaluate.py:40:    ap.add_argument("--config", required=True, help="Path to a composed evaluate config.")
scripts/train.py:41:    ap.add_argument("--config", required=True, help="Path to a composed train config.")
scripts/visualize.py:34:    ap.add_argument("--config", required=True, help="Path to a composed visualize config.")
scripts/predict.py:41:    ap.add_argument("--config", required=True, help="Path to a composed predict config.")
scripts/audit_dataset.py:43:    ap.add_argument("--config", required=True, help="Path to configs/audit_dataset.yaml")

thinking
**Analyzing config usage and next steps**

I'm sorting out how the project handles configs—there's mention of Hydra, but the existing pattern uses a simple --config path without CLI overrides. I'll stick to that to keep consistent and check docs for process names to extend. Since the tasks are moderately complex but straightforward, I’ll skip formal planning and dive into creating the leaderboard script, making sure to explore helpers like any CSV writers in `src/common/io.py` first.
exec
/bin/zsh -lc "sed -n '1,200p' src/common/io.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
from __future__ import annotations

from pathlib import Path
from typing import Optional

import pandas as pd

try:
    from rdkit import Chem
except Exception:  # pragma: no cover
    Chem = None


def read_csv(path: str | Path) -> pd.DataFrame:
    return pd.read_csv(Path(path))


def write_csv(df: pd.DataFrame, path: str | Path) -> None:
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)

thinking
**Designing leaderboard aggregation and output**

I'm working out how to scan run directories for meta and metrics, extract and filter the leaderboard metric, apply user filters from config, and sort results. I'm planning to include detailed metadata per run, handle missing metrics gracefully, and output both CSV and markdown formats, balancing completeness with simplicity while leveraging pandas for easy DataFrame manipulation.
exec
/bin/zsh -lc 'cat requirements.txt' in /Users/kawahito/Desktop/LJ_prediction succeeded in 285ms:
# Core
numpy
pandas
pyyaml
tqdm
matplotlib
scikit-learn
joblib

# Chemistry
rdkit  # usually installed via conda-forge; pip availability depends on platform

# FP models
lightgbm
catboost

# GNN (optional; install matching versions manually)
torch
# torch-geometric  # install via official instructions

thinking
**Designing leaderboard script and tests**
exec
/bin/zsh -lc 'cat src/common/__init__.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 288ms:
# Package init

thinking
**Reviewing config and output plans for leaderboard**

I’m thinking about adding a top-level `configs/leaderboard.yaml` for easier script runs, even though it’s not explicitly required—this would follow patterns from other processes like dataset and audit. I’ll also align the run output with artifact contracts by saving config and meta files, maybe skipping optional plots, and plan to update the process catalog docs to include the leaderboard process. It feels like a good balance between minimal changes and user convenience.
exec
/bin/zsh -lc 'cat configs/hydra/default.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 282ms:
hydra:
  run:
    dir: "runs/${process.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${experiment.name}"
  job:
    name: "${process.name}"



thinking
**Clarifying config and leaderboard design**
thinking
**Designing run function and tests with metric extraction**

I'm laying out the run(cfg) workflow including config parsing, scanning run dirs, filtering runs, extracting metrics with fallback splits, sorting, ranking, CSV and markdown output, and metadata saving. I'm also planning detailed tests to cover filtering and different metric formats (`by_split` vs direct), ensuring outputs exist and filters work properly. The script skeleton and helper functions are clearly defined to build a robust leaderboard process.
thinking
**Finalizing leaderboard output and test setup**

I’m finalizing markdown formatting with key summary info and clean CSV row construction (including tags and metric fields). I’m planning detailed tests that write controlled meta/metrics files for multiple runs, then run the leaderboard process with filters to verify correct filtering, metric extraction (including `by_split`), sorting, and outputs creation. This will ensure the leaderboard aggregator works robustly end to end.
file update
A /Users/kawahito/Desktop/LJ_prediction/src/common/leaderboard.py
from __future__ import annotations

import csv
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

from src.common.config import dump_yaml
from src.common.meta import build_meta, save_meta
from src.common.utils import ensure_dir, get_logger, load_json


DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
CSV_COLUMNS = (
    "rank",
    "metric_value",
    "metric_key",
    "metric_split",
    "metric_source",
    "process_name",
    "task_name",
    "model_name",
    "featureset_name",
    "dataset_hash",
    "n_train",
    "n_val",
    "n_test",
    "seed",
    "run_id",
    "created_at",
    "git_sha",
    "config_hash",
    "run_dir",
    "tags",
)
MD_COLUMNS = (
    "rank",
    "metric_value",
    "process_name",
    "model_name",
    "featureset_name",
    "task_name",
    "run_dir",
)


def run(cfg: Dict[str, Any]) -> Path:
    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
    top_n = int(leaderboard_cfg.get("top_n", 20))
    filters = leaderboard_cfg.get("filters", {}) or {}

    out_cfg = cfg.get("output", {}) or {}
    experiment_cfg = cfg.get("experiment", {}) or {}
    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
    run_dir = ensure_dir(run_dir_root / exp_name)
    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")

    dump_yaml(run_dir / "config.yaml", cfg)

    rows, upstream = _collect_rows(
        root_dirs=root_dirs,
        metric_key=metric_key,
        metric_split=metric_split,
        filters=filters,
        logger=logger,
    )

    sorted_rows = _sort_rows(rows, sort_order=sort_order)
    _assign_ranks(sorted_rows)

    csv_path = run_dir / "leaderboard.csv"
    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)

    md_path = run_dir / "leaderboard.md"
    md_payload = _build_markdown(
        rows=sorted_rows,
        metric_key=metric_key,
        metric_split=metric_split,
        sort_order=sort_order,
        top_n=top_n,
    )
    md_path.write_text(md_payload, encoding="utf-8")

    meta = build_meta(
        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
        cfg=cfg,
        upstream_artifacts=upstream,
        extra={
            "leaderboard_metric_key": metric_key,
            "leaderboard_metric_split": metric_split,
            "leaderboard_sort_order": sort_order,
        },
    )
    save_meta(run_dir, meta)

    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
    logger.info("Saved leaderboard to %s", csv_path)
    return run_dir


def _normalize_roots(root_value: Any) -> List[Path]:
    if root_value is None or root_value == "":
        return [Path("runs")]
    if isinstance(root_value, (list, tuple, set)):
        return [Path(str(v)) for v in root_value]
    return [Path(str(root_value))]


def _normalize_optional_str(value: Any) -> Optional[str]:
    if value is None:
        return None
    value = str(value).strip()
    if not value:
        return None
    return value


def _collect_rows(
    root_dirs: Iterable[Path],
    metric_key: str,
    metric_split: Optional[str],
    filters: Dict[str, Any],
    logger,
) -> Tuple[List[Dict[str, Any]], List[str]]:
    rows: List[Dict[str, Any]] = []
    upstream: List[str] = []
    seen_dirs: set[Path] = set()
    repo_root = Path(__file__).resolve().parents[2]

    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
        if run_dir in seen_dirs:
            continue
        seen_dirs.add(run_dir)

        meta = _load_json_safe(meta_path, logger=logger)
        metrics = _load_json_safe(metrics_path, logger=logger)
        if not isinstance(meta, dict) or not isinstance(metrics, dict):
            continue

        if not _matches_filters(meta, filters):
            continue

        metric_value, metric_source, resolved_split = _extract_metric(
            metrics,
            metric_key=metric_key,
            metric_split=metric_split,
            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
        )

        run_dir_str = _format_path(run_dir, repo_root)
        upstream.append(run_dir_str)

        row = {
            "rank": None,
            "metric_value": metric_value,
            "metric_key": metric_key,
            "metric_split": metric_split or resolved_split,
            "metric_source": metric_source,
            "process_name": meta.get("process_name"),
            "task_name": meta.get("task_name"),
            "model_name": meta.get("model_name"),
            "featureset_name": meta.get("featureset_name"),
            "dataset_hash": meta.get("dataset_hash"),
            "n_train": metrics.get("n_train"),
            "n_val": metrics.get("n_val"),
            "n_test": metrics.get("n_test"),
            "seed": metrics.get("seed"),
            "run_id": meta.get("run_id"),
            "created_at": meta.get("created_at"),
            "git_sha": meta.get("git_sha"),
            "config_hash": meta.get("config_hash"),
            "run_dir": run_dir_str,
            "tags": _normalize_tags(meta.get("tags")),
        }
        rows.append(row)

    return rows, upstream


def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
    for root in root_dirs:
        if not root.exists():
            continue
        for meta_path in root.rglob("meta.json"):
            run_dir = meta_path.parent
            metrics_path = run_dir / "metrics.json"
            if metrics_path.exists():
                yield run_dir, meta_path, metrics_path


def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
    try:
        return load_json(path)
    except Exception as exc:
        logger.warning("Failed to load %s (%s)", path, exc)
        return None


def _normalize_tags(tags: Any) -> str:
    if tags is None:
        return ""
    if isinstance(tags, str):
        return tags
    if isinstance(tags, (list, tuple, set)):
        return ",".join(str(t) for t in tags)
    return str(tags)


def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
    if not filters:
        return True

    def _match_value(value: Any, criterion: Any) -> bool:
        if criterion is None or criterion == "" or criterion == []:
            return True
        if isinstance(criterion, (list, tuple, set)):
            acceptable = {str(v) for v in criterion}
            return str(value) in acceptable
        return str(value) == str(criterion)

    for key in (
        "task_name",
        "model_name",
        "featureset_name",
        "process_name",
        "dataset_hash",
        "run_id",
        "git_sha",
    ):
        if not _match_value(meta.get(key), filters.get(key)):
            return False

    tags_filter = filters.get("tags")
    if tags_filter:
        tags = meta.get("tags") or []
        if isinstance(tags, str):
            tags = [tags]
        if isinstance(tags_filter, str):
            tags_filter = [tags_filter]
        tag_set = {str(t) for t in tags}
        if not tag_set.intersection({str(t) for t in tags_filter}):
            return False

    created_after = _parse_iso(filters.get("created_after"))
    created_before = _parse_iso(filters.get("created_before"))
    if created_after or created_before:
        created_at = _parse_iso(meta.get("created_at"))
        if created_after and (created_at is None or created_at < created_after):
            return False
        if created_before and (created_at is None or created_at > created_before):
            return False

    return True


def _parse_iso(value: Any) -> Optional[datetime]:
    if not value:
        return None
    try:
        text = str(value)
        if text.endswith("Z"):
            text = text[:-1] + "+00:00"
        return datetime.fromisoformat(text)
    except Exception:
        return None


def _extract_metric(
    metrics: Dict[str, Any],
    metric_key: str,
    metric_split: Optional[str],
    fallback_splits: List[str],
) -> Tuple[Optional[float], str, Optional[str]]:
    if metric_split:
        value, source = _find_metric(metrics, metric_key, metric_split)
        if value is not None:
            return value, source, metric_split

    by_split = metrics.get("by_split")
    if isinstance(by_split, dict):
        for split_name in fallback_splits:
            value, source = _find_metric(metrics, metric_key, split_name)
            if value is not None:
                return value, source, split_name
        for split_name, split_metrics in by_split.items():
            if isinstance(split_metrics, dict) and metric_key in split_metrics:
                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name

    for split_name in fallback_splits:
        value, source = _find_metric(metrics, metric_key, split_name)
        if value is not None:
            return value, source, split_name

    if metric_key in metrics:
        value = _coerce_float(metrics.get(metric_key))
        if value is not None:
            return value, "metrics", None

    return None, "", None


def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
    by_split = metrics.get("by_split")
    if isinstance(by_split, dict):
        split_metrics = by_split.get(split_name)
        if isinstance(split_metrics, dict) and metric_key in split_metrics:
            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"

    direct_split = metrics.get(split_name)
    if isinstance(direct_split, dict) and metric_key in direct_split:
        return _coerce_float(direct_split.get(metric_key)), split_name

    return None, ""


def _coerce_float(value: Any) -> Optional[float]:
    if value is None:
        return None
    try:
        return float(value)
    except (TypeError, ValueError):
        return None


def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
    reverse = sort_order != "asc"

    def sort_key(row: Dict[str, Any]) -> float:
        value = row.get("metric_value")
        if value is None:
            return float("-inf") if reverse else float("inf")
        return float(value)

    return sorted(rows, key=sort_key, reverse=reverse)


def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
    rank = 0
    for row in rows:
        if row.get("metric_value") is None:
            row["rank"] = None
            continue
        rank += 1
        row["rank"] = rank


def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=list(columns))
        writer.writeheader()
        for row in rows:
            writer.writerow({col: row.get(col) for col in columns})


def _build_markdown(
    rows: List[Dict[str, Any]],
    metric_key: str,
    metric_split: Optional[str],
    sort_order: str,
    top_n: int,
) -> str:
    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
    md_rows = rows_with_metric[: max(top_n, 0)]
    header_lines = [
        "# Leaderboard",
        f"- metric_key: {metric_key}",
        f"- metric_split: {metric_split or 'auto'}",
        f"- sort_order: {sort_order}",
        f"- total_runs: {len(rows)}",
        f"- with_metric: {len(rows_with_metric)}",
        "",
    ]

    if not md_rows:
        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"

    table = _format_markdown_table(md_rows, MD_COLUMNS)
    return "\n".join(header_lines + [table, ""])


def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
    col_list = list(columns)
    lines = [
        "| " + " | ".join(col_list) + " |",
        "| " + " | ".join(["---"] * len(col_list)) + " |",
    ]
    for row in rows:
        values = []
        for col in col_list:
            value = row.get(col)
            if value is None:
                value_str = ""
            else:
                value_str = str(value)
            value_str = value_str.replace("\n", " ").replace("|", "\\|")
            values.append(value_str)
        lines.append("| " + " | ".join(values) + " |")
    return "\n".join(lines)


def _format_path(path: Path, repo_root: Path) -> str:
    try:
        return str(path.relative_to(repo_root))
    except ValueError:
        return str(path)
apply_patch(auto_approved=true) exited 0 in 57ms:
Success. Updated the following files:
A src/common/leaderboard.py
file update:
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)

file update:
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)


thinking
**Evaluating Python typing compatibility**
file update
A /Users/kawahito/Desktop/LJ_prediction/scripts/leaderboard.py
from __future__ import annotations

import argparse
import sys
from pathlib import Path

# Allow running as `python scripts/leaderboard.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.common.leaderboard import run


def main() -> None:
    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
    args = ap.parse_args()

    cfg = load_config(args.config)
    run(cfg)


if __name__ == "__main__":
    main()
apply_patch(auto_approved=true) exited 0 in 64ms:
Success. Updated the following files:
A scripts/leaderboard.py
file update:
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)

file update:
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)


thinking
**Adding default leaderboard config file**
file update
A /Users/kawahito/Desktop/LJ_prediction/configs/leaderboard/default.yaml
leaderboard:
  root_dir: "runs"
  metric_key: "r2"
  metric_split: "val"
  sort_order: "desc"
  top_n: 20
  filters:
    task_name: null
    model_name: null
    featureset_name: null
    process_name: null
    dataset_hash: null
    run_id: null
    git_sha: null
    tags: []
    created_after: null
    created_before: null
apply_patch(auto_approved=true) exited 0 in 46ms:
Success. Updated the following files:
A configs/leaderboard/default.yaml
file update:
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)

file update:
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)


thinking
**Adding leaderboard config YAML**
file update
A /Users/kawahito/Desktop/LJ_prediction/configs/process/leaderboard.yaml
process:
  name: "leaderboard"
  kind: "leaderboard"

output:
  run_dir: "runs/leaderboard"
apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
A configs/process/leaderboard.yaml
file update:
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)

file update:
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)


thinking
**Adding top-level leaderboard config**
file update
A /Users/kawahito/Desktop/LJ_prediction/configs/leaderboard.yaml
defaults:
  - process: leaderboard
  - leaderboard: default
  - hydra: default
  - _self_

experiment:
  name: "leaderboard"

output:
  exp_name: "leaderboard"
apply_patch(auto_approved=true) exited 0 in 57ms:
Success. Updated the following files:
A configs/leaderboard.yaml
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)


thinking
**Updating validate_config for leaderboard check**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/utils/validate_config.py
@@ -47,2 +47,6 @@
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
apply_patch(auto_approved=true) exited 0 in 46ms:
Success. Updated the following files:
M src/utils/validate_config.py
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return


thinking
**Updating process catalog documentation**
file update
M /Users/kawahito/Desktop/LJ_prediction/docs/10_PROCESS_CATALOG.md
@@ -17,2 +17,3 @@
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
apply_patch(auto_approved=true) exited 0 in 51ms:
Success. Updated the following files:
M docs/10_PROCESS_CATALOG.md
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..767dab278e33ccb1b1d8f45b30e6cdc6aee4a084
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,411 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return


thinking
**Implementing and refining leaderboard tests and validation**

I’m planning to add test cases for the leaderboard in a new test file, covering filtered and unfiltered runs, while ensuring metric values are handled correctly. I also want to improve `run` in `src/common/leaderboard.py` by importing and calling `validate_config` for consistency and better config validation before executing.
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/leaderboard.py
@@ -10,2 +10,3 @@
 from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
 
@@ -47,2 +48,4 @@
 def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
     leaderboard_cfg = cfg.get("leaderboard", {}) or {}
apply_patch(auto_approved=true) exited 0 in 74ms:
Success. Updated the following files:
M src/common/leaderboard.py
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..4011f4b4aa4f6393ae74e56a7b87582e3bad1856
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,414 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..4011f4b4aa4f6393ae74e56a7b87582e3bad1856
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,414 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return


thinking
**Creating test file for apply_patch**
file update
A /Users/kawahito/Desktop/LJ_prediction/tests/test_leaderboard.py
from __future__ import annotations

import csv
from pathlib import Path

from src.common.leaderboard import run
from src.common.utils import save_json


def _write_meta(run_dir: Path, **overrides) -> None:
    meta = {
        "run_id": overrides.get("run_id", "run"),
        "process_name": overrides.get("process_name", "train"),
        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
        "git_sha": overrides.get("git_sha", "deadbeef"),
        "dataset_hash": overrides.get("dataset_hash", "dataset"),
        "config_hash": overrides.get("config_hash", "config"),
        "task_name": overrides.get("task_name", "lj_epsilon"),
        "model_name": overrides.get("model_name", "rf"),
        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
        "upstream_artifacts": [],
        "tags": overrides.get("tags", []),
    }
    meta.update(overrides)
    save_json(run_dir / "meta.json", meta)


def _write_metrics(run_dir: Path, payload: dict) -> None:
    save_json(run_dir / "metrics.json", payload)


def _read_csv(path: Path) -> list[dict]:
    with path.open("r", encoding="utf-8", newline="") as f:
        return list(csv.DictReader(f))


def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
    runs_root = tmp_path / "runs"
    run_a = runs_root / "train" / "run_a"
    run_a.mkdir(parents=True)
    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
    _write_metrics(
        run_a,
        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
    )

    run_b = runs_root / "evaluate" / "run_b"
    run_b.mkdir(parents=True)
    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})

    cfg_filtered = {
        "process": {"name": "leaderboard"},
        "leaderboard": {
            "root_dir": str(runs_root),
            "metric_key": "r2",
            "metric_split": "val",
            "sort_order": "desc",
            "top_n": 10,
            "filters": {"model_name": "rf"},
        },
        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
    }
    out_dir = run(cfg_filtered)
    rows = _read_csv(out_dir / "leaderboard.csv")
    assert len(rows) == 1
    assert rows[0]["model_name"] == "rf"
    assert float(rows[0]["metric_value"]) == 0.7

    cfg_all = {
        "process": {"name": "leaderboard"},
        "leaderboard": {
            "root_dir": str(runs_root),
            "metric_key": "r2",
            "metric_split": "val",
            "sort_order": "desc",
            "top_n": 10,
            "filters": {},
        },
        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
    }
    out_dir_all = run(cfg_all)
    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
    assert len(rows_all) == 2
    assert float(rows_all[0]["metric_value"]) == 0.9
    assert rows_all[0]["metric_source"].startswith("by_split")
apply_patch(auto_approved=true) exited 0 in 80ms:
Success. Updated the following files:
A tests/test_leaderboard.py
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..4011f4b4aa4f6393ae74e56a7b87582e3bad1856
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,414 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..4011f4b4aa4f6393ae74e56a7b87582e3bad1856
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,414 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Reviewing leaderboard logging and paths**

I’m checking that the leaderboard code handles relative and absolute paths correctly, especially in tests where run directories lie outside the repo root. I also found the log message about scanned runs is misleading since it counts only matched runs, so I plan to update it for clarity to avoid confusion.
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/leaderboard.py
@@ -103,3 +103,4 @@
 
-    logger.info("Scanned %d runs (matched %d).", len(upstream), len(sorted_rows))
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
     logger.info("Saved leaderboard to %s", csv_path)
apply_patch(auto_approved=true) exited 0 in 73ms:
Success. Updated the following files:
M src/common/leaderboard.py
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..50abc14b641484c98a20986d04ebab473aa80c38
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,415 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..50abc14b641484c98a20986d04ebab473aa80c38
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,415 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: row.get(col) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Preparing CSV serialization improvements**

I'm making sure to handle None and complex types correctly before writing CSVs by converting None to empty strings and serializing lists or dicts as strings, so the CSV output stays clean and readable. This should prevent issues from Python's default `str(None)` output and make the data easier to consume downstream.
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/leaderboard.py
@@ -361,3 +361,3 @@
         for row in rows:
-            writer.writerow({col: row.get(col) for col in columns})
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
 
@@ -415 +415,9 @@
         return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
apply_patch(auto_approved=true) exited 0 in 76ms:
Success. Updated the following files:
M src/common/leaderboard.py
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..e79dae63ee93f9154e5ce29a7a59e6517cfabd68
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..e79dae63ee93f9154e5ce29a7a59e6517cfabd68
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Reviewing CSV and config updates**
exec
/bin/zsh -lc "sed -n '1,80p' docs/10_PROCESS_CATALOG.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 362ms:
# Process Catalog（処理単位の一覧とI/O）

このドキュメントは **Process（処理単位）** のカタログです。  
「どのProcessが存在し、何を入力として、何を出力するか」を固定し、拡張してもブレないようにします。

---

## 1. Process一覧（推奨）
| Process | Script（推奨） | 主入力 | 主出力（artifact） | ClearML化の単位 |
|---|---|---|---|---|
| build_dataset | scripts/build_dataset.py | raw CSV/SDF | dataset index/split, dataset_hash | Dataset Task |
| audit_dataset | scripts/audit_dataset.py | raw CSV/SDF or processed dataset | audit_report.json, audit_report.md, plots | Audit Task |
| featurize | scripts/featurize.py | dataset artifact | features cache/manifest | Task |
| train | scripts/train.py | dataset(+features), task, model, train cfg | model.ckpt, metrics | Training Task |
| evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
| predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
| visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
| collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..e79dae63ee93f9154e5ce29a7a59e6517cfabd68
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": metric_split or resolved_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Reviewing metric_split fallback logic**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/leaderboard.py
@@ -165,3 +165,3 @@
             "metric_key": metric_key,
-            "metric_split": metric_split or resolved_split,
+            "metric_split": resolved_split or metric_split,
             "metric_source": metric_source,
apply_patch(auto_approved=true) exited 0 in 84ms:
Success. Updated the following files:
M src/common/leaderboard.py
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Reviewing metric split handling and config implications**
exec
/bin/zsh -lc "sed -n '1,200p' tests/test_config_loader.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 301ms:
from pathlib import Path

from src.common.config import load_config


def test_load_config_resolves_groups_from_root() -> None:
    cfg = load_config(Path("configs/fp/train_fixture.yaml"))

    assert cfg["process"]["name"] == "train"
    assert cfg["process"]["backend"] == "fp"
    assert cfg["output"]["run_dir"] == "runs/train/fp"
    assert cfg["paths"]["raw_csv"] == "tests/fixtures/data/raw/tc_pc_tb_fixture.csv"
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Verifying config group naming conventions**
exec
/bin/zsh -lc "sed -n '1,40p' configs/train/fp_default.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 297ms:
train:
  seed: 42
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Checking config group structures**
exec
/bin/zsh -lc 'cat configs/audit/default.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 312ms:
audit:
  input:
    source: "processed"   # processed / raw
  columns:
    sample_id: "sample_id"
    cas: "CAS"
    smiles: "smiles"
    formula: "MolecularFormula"
    sdf_path: "sdf_path"
  duplicate:
    methods: ["canonical_smiles", "inchikey"]
    max_groups: 20
    max_examples: 10
  stats:
    quantiles: [0.01, 0.05, 0.5, 0.95, 0.99]
  report:
    max_invalid_examples: 20
  plots:
    target_hist: true
    mol_wt_hist: true
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Reviewing leaderboard config and queue update plan**
exec
/bin/zsh -lc 'date -u +"%Y-%m-%dT%H:%M:%SZ"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 313ms:
2025-12-26T15:01:31Z
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")


thinking
**Updating task status in queue.json**
file update
M /Users/kawahito/Desktop/LJ_prediction/work/queue.json
@@ -2,3 +2,3 @@
   "version": 1,
-  "updated_at": "2025-12-26T14:47:59.823070Z",
+  "updated_at": "2025-12-26T15:01:31Z",
   "policy": {
@@ -224,3 +224,3 @@
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "結果集計Process（leaderboard/比較レポート生成）",
apply_patch(auto_approved=true) exited 0 in 67ms:
Success. Updated the following files:
M work/queue.json
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")
diff --git a/work/queue.json b/work/queue.json
index df6d15b6463b0febfbbd56dec1b6bb4d27e0d68e..44d864b1353f9eece970d604383901a2711d501e
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:47:59.823070Z",
+  "updated_at": "2025-12-26T15:01:31Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -222,7 +222,7 @@
     {
       "id": "075",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "結果集計Process（leaderboard/比較レポート生成）",
       "path": "work/tasks/075_leaderboard_process.md",
       "skills": [

file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")
diff --git a/work/queue.json b/work/queue.json
index df6d15b6463b0febfbbd56dec1b6bb4d27e0d68e..44d864b1353f9eece970d604383901a2711d501e
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:47:59.823070Z",
+  "updated_at": "2025-12-26T15:01:31Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -222,7 +222,7 @@
     {
       "id": "075",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "結果集計Process（leaderboard/比較レポート生成）",
       "path": "work/tasks/075_leaderboard_process.md",
       "skills": [


thinking
**Preparing to run targeted tests**

I’m weighing the pros and cons of running tests here, considering environment limits and dependencies. Since some tests might fail due to missing RDKit but `tests/test_leaderboard.py` looks safe to run without those dependencies, I’m settling on running that specific test file to validate the work.
exec
/bin/zsh -lc 'pytest -q tests/test_leaderboard.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 980ms:
.                                                                        [100%]
1 passed in 0.46s
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")
diff --git a/work/queue.json b/work/queue.json
index df6d15b6463b0febfbbd56dec1b6bb4d27e0d68e..44d864b1353f9eece970d604383901a2711d501e
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:47:59.823070Z",
+  "updated_at": "2025-12-26T15:01:31Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -222,7 +222,7 @@
     {
       "id": "075",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "結果集計Process（leaderboard/比較レポート生成）",
       "path": "work/tasks/075_leaderboard_process.md",
       "skills": [


thinking
**Summarizing final steps and output plan**
codex
Added a new leaderboard aggregation process that scans run artifacts, extracts a target metric across split formats, filters by meta fields, and writes `leaderboard.csv`/`leaderboard.md` with proper `config.yaml`/`meta.json` in a dedicated run dir. This is wired as a new Process with configs, validation, docs update, and a focused pytest.

Using skills: S70_evaluator_metrics (metrics aggregation patterns), S10_hydra_config (process config wiring), S95_tests_ci (pytest coverage).

**Change Plan**
- `src/common/leaderboard.py` add aggregation logic (scan runs, metric extraction, filtering, CSV/MD output, meta).
- `scripts/leaderboard.py` add CLI entrypoint.
- `configs/leaderboard.yaml`, `configs/leaderboard/default.yaml`, `configs/process/leaderboard.yaml` add process/config defaults.
- `src/utils/validate_config.py` validate leaderboard configs.
- `docs/10_PROCESS_CATALOG.md` add leaderboard row.
- `tests/test_leaderboard.py` add coverage for filter + by_split extraction.
- `work/queue.json` mark task 075 done.

**Implementation (diff)**
- `src/common/leaderboard.py`
```diff
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+    ...
+    rows, upstream = _collect_rows(...)
+    sorted_rows = _sort_rows(...)
+    _write_csv(.../leaderboard.csv)
+    _build_markdown(.../leaderboard.md)
+    save_meta(...)
+
+def _extract_metric(...):
+    # supports val/test and by_split formats
+    ...
+
+def _matches_filters(...):
+    # task/model/features/tags/period filters
+    ...
```

- `scripts/leaderboard.py`
```diff
+def main() -> None:
+    ap.add_argument("--config", required=True, ...)
+    cfg = load_config(args.config)
+    run(cfg)
```

- `configs/leaderboard.yaml`
```yaml
defaults:
  - process: leaderboard
  - leaderboard: default
  - hydra: default
  - _self_
experiment:
  name: "leaderboard"
output:
  exp_name: "leaderboard"
```

- `configs/leaderboard/default.yaml`
```yaml
leaderboard:
  root_dir: "runs"
  metric_key: "r2"
  metric_split: "val"
  sort_order: "desc"
  top_n: 20
  filters:
    task_name: null
    model_name: null
    featureset_name: null
    process_name: null
    dataset_hash: null
    run_id: null
    git_sha: null
    tags: []
    created_after: null
    created_before: null
```

- `src/utils/validate_config.py`
```diff
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
```

- `docs/10_PROCESS_CATALOG.md`
```diff
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
```

**Tests Added/Updated**
- `tests/test_leaderboard.py` covers filtering by `model_name` and `by_split` metric extraction.

**Verification Commands**
- `pytest -q tests/test_leaderboard.py` (ran)
- `python scripts/leaderboard.py --config configs/leaderboard.yaml` (not run)

**Compatibility Impact**
- Config: new group `leaderboard` + entry `configs/leaderboard.yaml`.
- CLI: new `scripts/leaderboard.py --config ...`.
- Artifacts: outputs `leaderboard.csv`, `leaderboard.md`, `config.yaml`, `meta.json`, `leaderboard.log` under `runs/leaderboard/<exp_name>`.

**Queue**
- Marked task 075 as done in `work/queue.json`.

Next steps:
1) Run `python scripts/leaderboard.py --config configs/leaderboard.yaml` on real `runs/` and adjust `leaderboard.filters` as needed.
2) (Optional) add plots in `runs/leaderboard/<exp_name>/plots/` if you want visual comparisons.
file update:
diff --git a/configs/leaderboard.yaml b/configs/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..59cfa6e851d6eb080599d7901a2568f44773c142
--- /dev/null
+++ b/configs/leaderboard.yaml
@@ -0,0 +1,11 @@
+defaults:
+  - process: leaderboard
+  - leaderboard: default
+  - hydra: default
+  - _self_
+
+experiment:
+  name: "leaderboard"
+
+output:
+  exp_name: "leaderboard"
diff --git a/configs/leaderboard/default.yaml b/configs/leaderboard/default.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..2fc5ff59a7f40edd5f5538b1c532db8610f58079
--- /dev/null
+++ b/configs/leaderboard/default.yaml
@@ -0,0 +1,17 @@
+leaderboard:
+  root_dir: "runs"
+  metric_key: "r2"
+  metric_split: "val"
+  sort_order: "desc"
+  top_n: 20
+  filters:
+    task_name: null
+    model_name: null
+    featureset_name: null
+    process_name: null
+    dataset_hash: null
+    run_id: null
+    git_sha: null
+    tags: []
+    created_after: null
+    created_before: null
diff --git a/configs/process/leaderboard.yaml b/configs/process/leaderboard.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..5ae74ddbf8ab252baa71e21a5764df034d82cd4e
--- /dev/null
+++ b/configs/process/leaderboard.yaml
@@ -0,0 +1,6 @@
+process:
+  name: "leaderboard"
+  kind: "leaderboard"
+
+output:
+  run_dir: "runs/leaderboard"
diff --git a/docs/10_PROCESS_CATALOG.md b/docs/10_PROCESS_CATALOG.md
index ee227ff159971d9bde452fb19423da5fa6339e19..36f792b5206137944046916662e17c1a39b7ab4c
--- a/docs/10_PROCESS_CATALOG.md
+++ b/docs/10_PROCESS_CATALOG.md
@@ -15,6 +15,7 @@
 | evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
 | predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
 | visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
+| leaderboard | scripts/leaderboard.py | runs/ (meta.json + metrics.json) | leaderboard.csv, leaderboard.md | Reporting Task |
 | collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |
 
 ※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
diff --git a/scripts/leaderboard.py b/scripts/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..8149fbf8cb5f49c0a8eb90e70c319f2c9a13692b
--- /dev/null
+++ b/scripts/leaderboard.py
@@ -0,0 +1,26 @@
+from __future__ import annotations
+
+import argparse
+import sys
+from pathlib import Path
+
+# Allow running as `python scripts/leaderboard.py ...` without installing the package.
+REPO_ROOT = Path(__file__).resolve().parents[1]
+if str(REPO_ROOT) not in sys.path:
+    sys.path.insert(0, str(REPO_ROOT))
+
+from src.common.config import load_config
+from src.common.leaderboard import run
+
+
+def main() -> None:
+    ap = argparse.ArgumentParser(description="Aggregate runs into a leaderboard.")
+    ap.add_argument("--config", required=True, help="Path to a composed leaderboard config.")
+    args = ap.parse_args()
+
+    cfg = load_config(args.config)
+    run(cfg)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/common/leaderboard.py b/src/common/leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..a7aafadd3fb0dcb1f07c3b34cf35bf1a87927b12
--- /dev/null
+++ b/src/common/leaderboard.py
@@ -0,0 +1,423 @@
+from __future__ import annotations
+
+import csv
+from datetime import datetime
+from pathlib import Path
+from typing import Any, Dict, Iterable, List, Optional, Tuple
+
+from src.common.config import dump_yaml
+from src.common.meta import build_meta, save_meta
+from src.common.utils import ensure_dir, get_logger, load_json
+from src.utils.validate_config import validate_config
+
+
+DEFAULT_FALLBACK_SPLITS = ("val", "test", "train")
+CSV_COLUMNS = (
+    "rank",
+    "metric_value",
+    "metric_key",
+    "metric_split",
+    "metric_source",
+    "process_name",
+    "task_name",
+    "model_name",
+    "featureset_name",
+    "dataset_hash",
+    "n_train",
+    "n_val",
+    "n_test",
+    "seed",
+    "run_id",
+    "created_at",
+    "git_sha",
+    "config_hash",
+    "run_dir",
+    "tags",
+)
+MD_COLUMNS = (
+    "rank",
+    "metric_value",
+    "process_name",
+    "model_name",
+    "featureset_name",
+    "task_name",
+    "run_dir",
+)
+
+
+def run(cfg: Dict[str, Any]) -> Path:
+    validate_config(cfg)
+
+    leaderboard_cfg = cfg.get("leaderboard", {}) or {}
+    root_dirs = _normalize_roots(leaderboard_cfg.get("root_dir", "runs"))
+    metric_key = str(leaderboard_cfg.get("metric_key", "r2"))
+    metric_split = _normalize_optional_str(leaderboard_cfg.get("metric_split"))
+    sort_order = str(leaderboard_cfg.get("sort_order", "desc")).lower()
+    top_n = int(leaderboard_cfg.get("top_n", 20))
+    filters = leaderboard_cfg.get("filters", {}) or {}
+
+    out_cfg = cfg.get("output", {}) or {}
+    experiment_cfg = cfg.get("experiment", {}) or {}
+    exp_name = str(out_cfg.get("exp_name", experiment_cfg.get("name", "leaderboard")))
+    run_dir_root = Path(out_cfg.get("run_dir", "runs/leaderboard"))
+    run_dir = ensure_dir(run_dir_root / exp_name)
+    logger = get_logger("leaderboard", log_file=run_dir / "leaderboard.log")
+
+    dump_yaml(run_dir / "config.yaml", cfg)
+
+    rows, upstream = _collect_rows(
+        root_dirs=root_dirs,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        filters=filters,
+        logger=logger,
+    )
+
+    sorted_rows = _sort_rows(rows, sort_order=sort_order)
+    _assign_ranks(sorted_rows)
+
+    csv_path = run_dir / "leaderboard.csv"
+    _write_csv(csv_path, sorted_rows, CSV_COLUMNS)
+
+    md_path = run_dir / "leaderboard.md"
+    md_payload = _build_markdown(
+        rows=sorted_rows,
+        metric_key=metric_key,
+        metric_split=metric_split,
+        sort_order=sort_order,
+        top_n=top_n,
+    )
+    md_path.write_text(md_payload, encoding="utf-8")
+
+    meta = build_meta(
+        process_name=str(cfg.get("process", {}).get("name", "leaderboard")),
+        cfg=cfg,
+        upstream_artifacts=upstream,
+        extra={
+            "leaderboard_metric_key": metric_key,
+            "leaderboard_metric_split": metric_split,
+            "leaderboard_sort_order": sort_order,
+        },
+    )
+    save_meta(run_dir, meta)
+
+    with_metric = sum(1 for row in sorted_rows if row.get("metric_value") is not None)
+    logger.info("Collected %d runs (with metric: %d).", len(sorted_rows), with_metric)
+    logger.info("Saved leaderboard to %s", csv_path)
+    return run_dir
+
+
+def _normalize_roots(root_value: Any) -> List[Path]:
+    if root_value is None or root_value == "":
+        return [Path("runs")]
+    if isinstance(root_value, (list, tuple, set)):
+        return [Path(str(v)) for v in root_value]
+    return [Path(str(root_value))]
+
+
+def _normalize_optional_str(value: Any) -> Optional[str]:
+    if value is None:
+        return None
+    value = str(value).strip()
+    if not value:
+        return None
+    return value
+
+
+def _collect_rows(
+    root_dirs: Iterable[Path],
+    metric_key: str,
+    metric_split: Optional[str],
+    filters: Dict[str, Any],
+    logger,
+) -> Tuple[List[Dict[str, Any]], List[str]]:
+    rows: List[Dict[str, Any]] = []
+    upstream: List[str] = []
+    seen_dirs: set[Path] = set()
+    repo_root = Path(__file__).resolve().parents[2]
+
+    for run_dir, meta_path, metrics_path in _iter_runs(root_dirs):
+        if run_dir in seen_dirs:
+            continue
+        seen_dirs.add(run_dir)
+
+        meta = _load_json_safe(meta_path, logger=logger)
+        metrics = _load_json_safe(metrics_path, logger=logger)
+        if not isinstance(meta, dict) or not isinstance(metrics, dict):
+            continue
+
+        if not _matches_filters(meta, filters):
+            continue
+
+        metric_value, metric_source, resolved_split = _extract_metric(
+            metrics,
+            metric_key=metric_key,
+            metric_split=metric_split,
+            fallback_splits=list(DEFAULT_FALLBACK_SPLITS),
+        )
+
+        run_dir_str = _format_path(run_dir, repo_root)
+        upstream.append(run_dir_str)
+
+        row = {
+            "rank": None,
+            "metric_value": metric_value,
+            "metric_key": metric_key,
+            "metric_split": resolved_split or metric_split,
+            "metric_source": metric_source,
+            "process_name": meta.get("process_name"),
+            "task_name": meta.get("task_name"),
+            "model_name": meta.get("model_name"),
+            "featureset_name": meta.get("featureset_name"),
+            "dataset_hash": meta.get("dataset_hash"),
+            "n_train": metrics.get("n_train"),
+            "n_val": metrics.get("n_val"),
+            "n_test": metrics.get("n_test"),
+            "seed": metrics.get("seed"),
+            "run_id": meta.get("run_id"),
+            "created_at": meta.get("created_at"),
+            "git_sha": meta.get("git_sha"),
+            "config_hash": meta.get("config_hash"),
+            "run_dir": run_dir_str,
+            "tags": _normalize_tags(meta.get("tags")),
+        }
+        rows.append(row)
+
+    return rows, upstream
+
+
+def _iter_runs(root_dirs: Iterable[Path]) -> Iterable[Tuple[Path, Path, Path]]:
+    for root in root_dirs:
+        if not root.exists():
+            continue
+        for meta_path in root.rglob("meta.json"):
+            run_dir = meta_path.parent
+            metrics_path = run_dir / "metrics.json"
+            if metrics_path.exists():
+                yield run_dir, meta_path, metrics_path
+
+
+def _load_json_safe(path: Path, logger) -> Optional[Dict[str, Any]]:
+    try:
+        return load_json(path)
+    except Exception as exc:
+        logger.warning("Failed to load %s (%s)", path, exc)
+        return None
+
+
+def _normalize_tags(tags: Any) -> str:
+    if tags is None:
+        return ""
+    if isinstance(tags, str):
+        return tags
+    if isinstance(tags, (list, tuple, set)):
+        return ",".join(str(t) for t in tags)
+    return str(tags)
+
+
+def _matches_filters(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    if not filters:
+        return True
+
+    def _match_value(value: Any, criterion: Any) -> bool:
+        if criterion is None or criterion == "" or criterion == []:
+            return True
+        if isinstance(criterion, (list, tuple, set)):
+            acceptable = {str(v) for v in criterion}
+            return str(value) in acceptable
+        return str(value) == str(criterion)
+
+    for key in (
+        "task_name",
+        "model_name",
+        "featureset_name",
+        "process_name",
+        "dataset_hash",
+        "run_id",
+        "git_sha",
+    ):
+        if not _match_value(meta.get(key), filters.get(key)):
+            return False
+
+    tags_filter = filters.get("tags")
+    if tags_filter:
+        tags = meta.get("tags") or []
+        if isinstance(tags, str):
+            tags = [tags]
+        if isinstance(tags_filter, str):
+            tags_filter = [tags_filter]
+        tag_set = {str(t) for t in tags}
+        if not tag_set.intersection({str(t) for t in tags_filter}):
+            return False
+
+    created_after = _parse_iso(filters.get("created_after"))
+    created_before = _parse_iso(filters.get("created_before"))
+    if created_after or created_before:
+        created_at = _parse_iso(meta.get("created_at"))
+        if created_after and (created_at is None or created_at < created_after):
+            return False
+        if created_before and (created_at is None or created_at > created_before):
+            return False
+
+    return True
+
+
+def _parse_iso(value: Any) -> Optional[datetime]:
+    if not value:
+        return None
+    try:
+        text = str(value)
+        if text.endswith("Z"):
+            text = text[:-1] + "+00:00"
+        return datetime.fromisoformat(text)
+    except Exception:
+        return None
+
+
+def _extract_metric(
+    metrics: Dict[str, Any],
+    metric_key: str,
+    metric_split: Optional[str],
+    fallback_splits: List[str],
+) -> Tuple[Optional[float], str, Optional[str]]:
+    if metric_split:
+        value, source = _find_metric(metrics, metric_key, metric_split)
+        if value is not None:
+            return value, source, metric_split
+
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        for split_name in fallback_splits:
+            value, source = _find_metric(metrics, metric_key, split_name)
+            if value is not None:
+                return value, source, split_name
+        for split_name, split_metrics in by_split.items():
+            if isinstance(split_metrics, dict) and metric_key in split_metrics:
+                return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}", split_name
+
+    for split_name in fallback_splits:
+        value, source = _find_metric(metrics, metric_key, split_name)
+        if value is not None:
+            return value, source, split_name
+
+    if metric_key in metrics:
+        value = _coerce_float(metrics.get(metric_key))
+        if value is not None:
+            return value, "metrics", None
+
+    return None, "", None
+
+
+def _find_metric(metrics: Dict[str, Any], metric_key: str, split_name: str) -> Tuple[Optional[float], str]:
+    by_split = metrics.get("by_split")
+    if isinstance(by_split, dict):
+        split_metrics = by_split.get(split_name)
+        if isinstance(split_metrics, dict) and metric_key in split_metrics:
+            return _coerce_float(split_metrics.get(metric_key)), f"by_split.{split_name}"
+
+    direct_split = metrics.get(split_name)
+    if isinstance(direct_split, dict) and metric_key in direct_split:
+        return _coerce_float(direct_split.get(metric_key)), split_name
+
+    return None, ""
+
+
+def _coerce_float(value: Any) -> Optional[float]:
+    if value is None:
+        return None
+    try:
+        return float(value)
+    except (TypeError, ValueError):
+        return None
+
+
+def _sort_rows(rows: List[Dict[str, Any]], sort_order: str) -> List[Dict[str, Any]]:
+    reverse = sort_order != "asc"
+
+    def sort_key(row: Dict[str, Any]) -> float:
+        value = row.get("metric_value")
+        if value is None:
+            return float("-inf") if reverse else float("inf")
+        return float(value)
+
+    return sorted(rows, key=sort_key, reverse=reverse)
+
+
+def _assign_ranks(rows: List[Dict[str, Any]]) -> None:
+    rank = 0
+    for row in rows:
+        if row.get("metric_value") is None:
+            row["rank"] = None
+            continue
+        rank += 1
+        row["rank"] = rank
+
+
+def _write_csv(path: Path, rows: List[Dict[str, Any]], columns: Iterable[str]) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("w", encoding="utf-8", newline="") as f:
+        writer = csv.DictWriter(f, fieldnames=list(columns))
+        writer.writeheader()
+        for row in rows:
+            writer.writerow({col: _csv_value(row.get(col)) for col in columns})
+
+
+def _build_markdown(
+    rows: List[Dict[str, Any]],
+    metric_key: str,
+    metric_split: Optional[str],
+    sort_order: str,
+    top_n: int,
+) -> str:
+    rows_with_metric = [row for row in rows if row.get("metric_value") is not None]
+    md_rows = rows_with_metric[: max(top_n, 0)]
+    header_lines = [
+        "# Leaderboard",
+        f"- metric_key: {metric_key}",
+        f"- metric_split: {metric_split or 'auto'}",
+        f"- sort_order: {sort_order}",
+        f"- total_runs: {len(rows)}",
+        f"- with_metric: {len(rows_with_metric)}",
+        "",
+    ]
+
+    if not md_rows:
+        return "\n".join(header_lines + ["No runs matched filters or metric_key."]) + "\n"
+
+    table = _format_markdown_table(md_rows, MD_COLUMNS)
+    return "\n".join(header_lines + [table, ""])
+
+
+def _format_markdown_table(rows: List[Dict[str, Any]], columns: Iterable[str]) -> str:
+    col_list = list(columns)
+    lines = [
+        "| " + " | ".join(col_list) + " |",
+        "| " + " | ".join(["---"] * len(col_list)) + " |",
+    ]
+    for row in rows:
+        values = []
+        for col in col_list:
+            value = row.get(col)
+            if value is None:
+                value_str = ""
+            else:
+                value_str = str(value)
+            value_str = value_str.replace("\n", " ").replace("|", "\\|")
+            values.append(value_str)
+        lines.append("| " + " | ".join(values) + " |")
+    return "\n".join(lines)
+
+
+def _format_path(path: Path, repo_root: Path) -> str:
+    try:
+        return str(path.relative_to(repo_root))
+    except ValueError:
+        return str(path)
+
+
+def _csv_value(value: Any) -> Any:
+    if value is None:
+        return ""
+    if isinstance(value, (list, dict, tuple, set)):
+        return str(value)
+    return value
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index fecab08a83842ba5f63265e36102b1300f7b36cb..54a8f28b3cce70a21b306155a09be31971c3bfee
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -45,6 +45,10 @@
         _require_keys(cfg, ["output"], f"{process_name} config")
         return
 
+    if process_name == "leaderboard":
+        _require_keys(cfg, ["leaderboard", "output"], f"{process_name} config")
+        return
+
     if process_name == "audit_dataset":
         _require_keys(cfg, ["audit", "output"], f"{process_name} config")
         return
diff --git a/tests/test_leaderboard.py b/tests/test_leaderboard.py
new file mode 100644
index 0000000000000000000000000000000000000000..0119d4ea5a4bf88a2a9a9da80b4f94f85cfc3cff
--- /dev/null
+++ b/tests/test_leaderboard.py
@@ -0,0 +1,86 @@
+from __future__ import annotations
+
+import csv
+from pathlib import Path
+
+from src.common.leaderboard import run
+from src.common.utils import save_json
+
+
+def _write_meta(run_dir: Path, **overrides) -> None:
+    meta = {
+        "run_id": overrides.get("run_id", "run"),
+        "process_name": overrides.get("process_name", "train"),
+        "created_at": overrides.get("created_at", "2025-01-01T00:00:00Z"),
+        "git_sha": overrides.get("git_sha", "deadbeef"),
+        "dataset_hash": overrides.get("dataset_hash", "dataset"),
+        "config_hash": overrides.get("config_hash", "config"),
+        "task_name": overrides.get("task_name", "lj_epsilon"),
+        "model_name": overrides.get("model_name", "rf"),
+        "featureset_name": overrides.get("featureset_name", "fp_morgan"),
+        "upstream_artifacts": [],
+        "tags": overrides.get("tags", []),
+    }
+    meta.update(overrides)
+    save_json(run_dir / "meta.json", meta)
+
+
+def _write_metrics(run_dir: Path, payload: dict) -> None:
+    save_json(run_dir / "metrics.json", payload)
+
+
+def _read_csv(path: Path) -> list[dict]:
+    with path.open("r", encoding="utf-8", newline="") as f:
+        return list(csv.DictReader(f))
+
+
+def test_leaderboard_collects_and_filters(tmp_path: Path) -> None:
+    runs_root = tmp_path / "runs"
+    run_a = runs_root / "train" / "run_a"
+    run_a.mkdir(parents=True)
+    _write_meta(run_a, run_id="run_a", process_name="train", model_name="rf")
+    _write_metrics(
+        run_a,
+        {"val": {"r2": 0.7}, "test": {"r2": 0.6}, "n_train": 10, "n_val": 2, "n_test": 2, "seed": 0},
+    )
+
+    run_b = runs_root / "evaluate" / "run_b"
+    run_b.mkdir(parents=True)
+    _write_meta(run_b, run_id="run_b", process_name="evaluate", model_name="lgbm")
+    _write_metrics(run_b, {"by_split": {"val": {"r2": 0.9}}, "n_train": 10, "n_val": 2, "n_test": 2})
+
+    cfg_filtered = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {"model_name": "rf"},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "filtered"},
+    }
+    out_dir = run(cfg_filtered)
+    rows = _read_csv(out_dir / "leaderboard.csv")
+    assert len(rows) == 1
+    assert rows[0]["model_name"] == "rf"
+    assert float(rows[0]["metric_value"]) == 0.7
+
+    cfg_all = {
+        "process": {"name": "leaderboard"},
+        "leaderboard": {
+            "root_dir": str(runs_root),
+            "metric_key": "r2",
+            "metric_split": "val",
+            "sort_order": "desc",
+            "top_n": 10,
+            "filters": {},
+        },
+        "output": {"run_dir": str(tmp_path / "out"), "exp_name": "all"},
+    }
+    out_dir_all = run(cfg_all)
+    rows_all = _read_csv(out_dir_all / "leaderboard.csv")
+    assert len(rows_all) == 2
+    assert float(rows_all[0]["metric_value"]) == 0.9
+    assert rows_all[0]["metric_source"].startswith("by_split")
diff --git a/work/queue.json b/work/queue.json
index df6d15b6463b0febfbbd56dec1b6bb4d27e0d68e..44d864b1353f9eece970d604383901a2711d501e
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:47:59.823070Z",
+  "updated_at": "2025-12-26T15:01:31Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -222,7 +222,7 @@
     {
       "id": "075",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "結果集計Process（leaderboard/比較レポート生成）",
       "path": "work/tasks/075_leaderboard_process.md",
       "skills": [

tokens used
112,303
