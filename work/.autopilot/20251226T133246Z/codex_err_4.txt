OpenAI Codex v0.77.0 (research preview)
--------
workdir: /Users/kawahito/Desktop/LJ_prediction
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: xhigh
reasoning summaries: auto
session id: 019b5b0e-b86f-7de3-8ad8-580c11f9d649
--------
user
# ===== SESSION CONTEXT =====
# Codex Session Context（毎回貼る短文）

- このrepoは「化学構造（SDF/CSV）→ 物性予測」を行う CLI/Hydra 基盤。
- 不変条件は docs/00_INVARIANTS.md が唯一の正。
- 今回の作業範囲・要件は work/tasks/<TASK>.md が唯一の正。
- 学習と推論の特徴量/前処理は必ず一致させる（skew禁止）。
- 成果物（artifact）は docs/04_ARTIFACTS_AND_VERSIONING.md を守る。
- 破壊的変更が必要なら work/rfc → docs/adr を先に作る（勝手に壊さない）。
- すべての処理は Process（処理単位）としてHydraで管理し、単独実行できる形を守る。
- 将来ClearML Task化を想定し、各Processが artifact を明確に出す設計にする（実装は今しない）。

# ===== AUTOPILOT MODE (non-interactive) =====
あなたは自動実行ループの中で動いています。**停止しないための規約**として次を厳守してください：

【禁止】
- ユーザーへの確認・質問・承認依頼（"確認してください" / "進めてよいですか" / "Approve" 等）は一切しない。
- "今回は未実装" / "差分なし" のまま終了しない（必ず前進する）。

【single source of truth】
- タスクの status の唯一の真実は work/queue.json。task.md の 'Blocked' 記述は参考情報。
  - queue が todo/in_progress なら実装を進める（task.md 側の Blocked を理由に止めない）。
  - queue が blocked なら blocked-handling を行う（reason/unblock_condition/next_action を整備し、解除子タスクを起票）。

【stale Blocked の扱い】
- task.md に '## Blocked' が残っていても queue が todo/in_progress の場合、それは **stale**。
  - task.md を更新し、Blocked を Notes/History に移す or 解除した上で実装を進める。

【書き込み権限】
- **書き込みは可能**な前提。work/queue.json 更新を含む必要な変更は実施する。
  - もし実際にツール/環境エラーで書き込めない場合：
    1) エラーメッセージを短く引用
    2) 最小の対処（sandbox/approval/flags 見直し、権限確認）を提示
    3) タスクを blocked にして解除子タスクを起票（子に unblocks を付与）
    ※それでも『確認してください』は書かない。

【必ずどれかを実施（Decision Protocol）】
A) 実装を進める → テスト/検証 → work/queue.json を done に更新
B) 進められない → work/queue.json を blocked に更新し、解除子タスク（unblocks付き）を作成
C) 依存/状態がズレている → work/queue.json と task.md を修正して整合させた上で A へ

# ===== SELECTED TASK =====
- id: 035
- title: split戦略の追加（random/scaffold/group、seed固定、漏洩防止）
- priority: P0
- status: in_progress
- path: work/tasks/035_split_strategies.md

# ===== CONTRACTS TO FOLLOW (open & read) =====
- docs/00_INVARIANTS.md
- docs/02_DATA_CONTRACTS.md
- docs/03_CONFIG_CONVENTIONS.md
- docs/04_ARTIFACTS_AND_VERSIONING.md

# ===== SKILLS TO FOLLOW (open & follow) =====
- agentskills/skills/S20_dataset_loader.md
- agentskills/skills/S10_hydra_config.md
- agentskills/skills/S95_tests_ci.md


# ===== TASK FILE (single source of truth for *intent*) =====
# Task 035 (P0): split戦略の追加（random/scaffold/group、seed固定、漏洩防止）

## 目的（Why）
- R²の比較が意味を持つように、splitを戦略的に選べるようにする。
- splitが変わるとスコアが大きく変動するため、**同一splitで比較**できる仕組みが必須。

## 背景（Context）
- random splitだけだと簡単に“高スコア”が出る（漏洩/類似分子が跨る）。
- 分子タスクでは scaffold split が標準的な比較になりやすい。
- 将来の比較（multirun, ClearML）でも split の追跡が必須（dataset_hash + split.json）。

## スコープ（Scope）
### In scope
- `build_dataset`（またはsplit生成部分）に split戦略を追加
  - `random`（seed固定）
  - `scaffold`（Murcko scaffoldでグルーピングし、グループ単位で割当）
  - `group`（指定列：例 cas / inchikey / formula 等、同一グループは同一split）
- split成果物の保存（契約）
  - `split.json` などとして artifact に保存（docs/04準拠）
- splitのバリデーション
  - split間重複がない
  - scaffold split では同一scaffoldが跨らない（少なくとも検査できる）

### Out of scope（今回はやらない）
- Nested CV、完全なk-fold運用（これはP1でもOK）
  - ただし “将来追加できる拡張ポイント” は設計しておく

## 影響（Contract Impact）
- dataset artifactの内容が増える（split保存）
- 既存のsplit生成がある場合、互換性を壊さずに拡張する（旧キーを残す/変換する）

## 実装計画（Plan）
1) `src/data/splitting.py`（案）を追加し、splitメソッドを関数として実装
   - `make_split_random(...)`
   - `make_split_scaffold(...)`（RDKit: Murcko scaffold）
   - `make_split_group(...)`
2) `configs/dataset/*.yaml` に以下のキーを追加（例）
   - `dataset.split.method: random|scaffold|group`
   - `dataset.split.seed: 0`
   - `dataset.split.fractions: [0.8, 0.1, 0.1]`
   - `dataset.split.group_key: "cas"`（groupの場合）
3) `build_dataset` Process に組込み、splitを生成して artifact に保存
4) テスト追加（重要）
   - 小さな分子集合で scaffold split を作り、同一scaffoldが跨らないことを検査
   - seed固定で split が再現されることを検査

## 受け入れ条件（Acceptance Criteria）
- [ ] random/scaffold/group を config で切替できる
- [ ] seed固定で split が再現される
- [ ] split成果物が artifact として保存される（docs/04準拠）
- [ ] scaffold split の漏洩検査ができる（テストで担保）
- [ ] pytestに最低1つ追加テスト

## 検証手順（How to Verify）
- 例：
  - `python scripts/build_dataset.py dataset.split.method=scaffold dataset.split.seed=0`
  - `python scripts/build_dataset.py dataset.split.method=group dataset.split.group_key=cas`
  - `pytest -q`

## メモ
- このタスク完了後、train/evaluate は “dataset artifactのsplitを必ず使う” 方針に寄せる（比較可能性）

# ===== OUTPUT REQUIREMENTS (mandatory) =====
1) 変更計画（ファイル単位）
2) 実装（差分が分かるように）
3) 追加/更新したテスト
4) 検証コマンド
5) 互換性影響（config/CLI/artifact）
6) タスク完了時：work/queue.json の status を done に更新（満たせない場合は blocked と理由）


mcp: context7 starting
mcp: context7 ready
mcp startup: ready: context7

thinking
**Preparing to read skill files**
exec
/bin/zsh -lc ls in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
README.md
agentskills
codex
configs
data
docs
learning_curve_val.png
objective.md
parity_test.png
parity_val.png
requirements.txt
residual_test.png
residual_val.png
runs
scripts
src
tests
tools
work
y_train_hist.png

thinking
**Reading skill files**
exec
/bin/zsh -lc 'cat agentskills/skills/S20_dataset_loader.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
# S20 Dataset Loader

## Purpose
CSV/SDF 入力を共通表現へ変換するローダを追加/修正し、目的変数の差し替えに耐える基盤を作る。

## Inputs
- docs/02_DATA_CONTRACTS.md
- work/tasks/040_add_new_property_task.md（または該当）

## Allowed Changes
- src/data/**
- configs/dataset/**
- tests/**

## Steps
1) 現状の CSV/SDF の読み込み箇所を特定
2) column mapping を config で指定できるようにする
3) 欠損/不一致の扱いをログ化
4) unit test（最小CSV）を追加

thinking
**Identifying required configuration file**
exec
/bin/zsh -lc 'cat agentskills/skills/S10_hydra_config.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
# S10 Hydra Config

## Purpose
Hydra config を group 化し、設定の再利用と組み合わせを容易にする。

## When to Use
- config が増えて混乱してきた
- モデル/特徴量/タスクを切り替えたい

## Inputs
- docs/03_CONFIG_CONVENTIONS.md
- work/tasks/010_reorganize_hydra_configs.md

## Allowed Changes
- configs/**
- scripts/*（config読込部分の更新）
- src/utils/validate_config.py（追加）

## Common Pitfalls
- defaults の上書き順序ミス

thinking
**Searching for S95_tests_ci**
exec
/bin/zsh -lc 'cat agentskills/skills/S95_tests_ci.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
# S95 Tests & CI

## Purpose
pytest + smoke + contract test を整備し、改修で壊れないようにする。

## Allowed Changes
- tests/**
- pyproject.toml（任意）
- .github/workflows/**（任意）

## Pitfalls
- smoke が重くなりすぎる
- 乱数により flaky になる

thinking
**Checking contract documents**
exec
/bin/zsh -lc 'cat docs/00_INVARIANTS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
# Invariants（不変条件）

このドキュメントは **開発中に変えてはいけない契約（Codexも遵守）** です。  
迷ったらここへ戻り、ここに書かれていない“勝手な最適化”をしないでください。

---

## 0. 基本目的（この基盤のゴール）
- **入力**: 化学構造（主に **CSV + SDF**。将来もこの形式を軸にする）
- **出力**: 物性（LJパラメータに限らず、**同じCSV/SDF形式のまま目的変数を差し替える**）
- **操作**: **CLI**（**Hydra**で設定管理）
- **要求**:
  - 前処理・特徴量化・学習・評価・推論・可視化を **拡張しやすい**
  - モデル/特徴量の手法が複数あり、**選択して実行・比較評価しやすい**

---

## 1. もっとも重要な不変条件（破るなら RFC→ADR→更新）

### 1.1 「処理単位（Process）」が第一級（最重要）

thinking
**Identifying missing documentation**
exec
/bin/zsh -lc 'cat docs/02_DATA_CONTRACTS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
# Data Contracts（データ契約）

このドキュメントは **CSV/SDF を中心としたデータ形式の共通契約** を定義します。  
目的変数（物性）が増えても、ここを守れば基盤が壊れないようにします。

## 1. 入力データの基本形（推奨）
### 1.1 CSV（テーブル）
- 1行 = 1分子（サンプル）
- 必須列（推奨名）
  - `sample_id`：一意ID（無ければ生成）
  - `cas`：CAS番号（ある場合）
  - `smiles`：SMILES（ある場合）
  - `formula`：分子式（ある場合）
  - `sdf_path`：SDF ファイルへのパス（行ごと or 共通）
  - `target.<name>`：目的変数（例 `target.lj_sigma`, `target.lj_epsilon`）
- 任意列
  - `meta.*`：データソース、温度条件、参考値など

※ 既存CSVの列名が異なる場合は `configs/dataset/*.yaml` でマッピングする。


thinking
**Checking for docs/03 directory**
exec
/bin/zsh -lc 'cat docs/03_CONFIG_CONVENTIONS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
# Hydra Config Conventions（設定規約）

このドキュメントは Hydra を用いた設定管理の規約です。  
**Process（処理単位）をHydraで統一管理**し、手法の切替・比較評価をしやすくします。

---

## 1. 設定のゴール
- **各Processが単独で実行可能**
- モデル/特徴量/タスクの組合せを安全に切替可能
- 比較評価（multirun）しても成果物が追跡可能
- 将来ClearMLで Task 化しても、同じconfigを使い回せる

---

## 2. config group 構造（必須）
- `process/`：どのProcessを実行するか（脚本ごとに固定でもよいが、共通鍵は持つ）
- `dataset/`：入力データ・列マッピング・split・フィルタ
- `task/`：目的変数/タスクタイプ（regression/classification/multitask）
- `preprocess/`：正規化/3D生成/欠損処理

thinking
**Checking for docs/04**
exec
/bin/zsh -lc 'cat docs/04_ARTIFACTS_AND_VERSIONING.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:
# Artifacts & Versioning（成果物・再現性）

このドキュメントは「各Processが独立して実行でき、比較・再現できる」ための成果物（artifact）契約です。  
将来ClearML Task化を見越し、**Process単位で入力/出力が追跡できる**形にします。

---

## 1. 出力ディレクトリ（推奨形）
Hydraの出力ディレクトリ（例：`runs/<process>/<date>/<time>_<experiment>/`）に保存する。

- `<process>` は `train`/`predict`/`evaluate` 等の処理単位
- 1 run = 1 Process 実行

---

## 2. 必須ファイル（全Process共通）
- `config.yaml`：最終 config（Hydra合成結果）
- `meta.json`：メタ情報（下記必須キー）
- `logs/`：ログ（形式は自由だが存在推奨）


thinking
**Opening task file**
exec
/bin/zsh -lc 'cat work/tasks/035_split_strategies.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
# Task 035 (P0): split戦略の追加（random/scaffold/group、seed固定、漏洩防止）

## 目的（Why）
- R²の比較が意味を持つように、splitを戦略的に選べるようにする。
- splitが変わるとスコアが大きく変動するため、**同一splitで比較**できる仕組みが必須。

## 背景（Context）
- random splitだけだと簡単に“高スコア”が出る（漏洩/類似分子が跨る）。
- 分子タスクでは scaffold split が標準的な比較になりやすい。
- 将来の比較（multirun, ClearML）でも split の追跡が必須（dataset_hash + split.json）。

## スコープ（Scope）
### In scope
- `build_dataset`（またはsplit生成部分）に split戦略を追加
  - `random`（seed固定）
  - `scaffold`（Murcko scaffoldでグルーピングし、グループ単位で割当）
  - `group`（指定列：例 cas / inchikey / formula 等、同一グループは同一split）
- split成果物の保存（契約）
  - `split.json` などとして artifact に保存（docs/04準拠）
- splitのバリデーション

thinking
**Checking for work queue file**
exec
/bin/zsh -lc 'cat work/queue.json' in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
{
  "version": 1,
  "updated_at": "2025-12-26T14:27:29.824618Z",
  "policy": {
    "priority_order": [
      "P0",
      "P1",
      "P2",
      "P3"
    ],
    "status_order": [
      "todo",
      "in_progress",
      "blocked",
      "done"
    ],
    "auto_set_in_progress_on_next": true
  },
  "tasks": [
    {

thinking
**Planning skill usage and dataset inspection**
exec
/bin/zsh -lc 'rg --files' in /Users/kawahito/Desktop/LJ_prediction succeeded in 306ms:
parity_val.png
README.md
y_train_hist.png
parity_test.png
learning_curve_val.png
objective.md
tools/codex_prompt.py
tools/codex_autopilot.py
tools/autopilot.sh
work/AUTOMATION.md
work/REPO_ORIENTATION.md
work/BACKLOG.md
tests/test_fixture_dataset.py
src/data/__init__.py
src/data/audit.py
scripts/build_dataset.py
scripts/evaluate.py
scripts/train.py
scripts/visualize.py
scripts/predict.py

thinking
**Searching for splitting logic**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/splitters.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd

from .chemistry import murcko_scaffold_smiles
from .io import load_sdf_mol, sdf_path_from_cas


def random_split(
    df: pd.DataFrame,
    ratios: Sequence[float] = (0.8, 0.1, 0.1),
    seed: int = 42,
) -> Dict[str, List[int]]:
    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
    rng = np.random.default_rng(seed)

thinking
**Investigating dataset splitting integration**
exec
/bin/zsh -lc "sed -n '1,240p' scripts/build_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

# Allow running as `python scripts/build_dataset.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import dump_yaml, load_config
from src.common.chemistry import elements_string, n_elements, parse_formula
from src.common.dataset_selectors import SelectorContext, apply_selectors
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv

thinking
**Investigating dataset artifact handling**

I’m looking into how dataset splits are saved and noticed some functions write indices to files, but the expected artifact is a split.json file. Current builds output CSVs instead, so I want to explore the artifact system in src/utils/artifacts.py and see if build_dataset relates to it or if it’s changed. I’ll check the build_dataset script next to clarify this.
exec
/bin/zsh -lc "sed -n '1,240p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
from __future__ import annotations

import csv
import hashlib
import json
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Optional
from uuid import uuid4

from src.common.utils import save_json

REQUIRED_META_KEYS = [
    "run_id",
    "process_name",
    "created_at",
    "git_sha",
    "dataset_hash",
    "config_hash",

thinking
**Checking artifact usage in build_dataset**
exec
/bin/zsh -lc "sed -n '1,200p' scripts/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict

# Allow running as `python scripts/train.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import train as fp_train
from src.gnn import train as gnn_train


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}


thinking
**Searching for dataset split usage**
exec
/bin/zsh -lc 'rg "split" -n src | head' in /Users/kawahito/Desktop/LJ_prediction succeeded in 285ms:
src/utils/validate_config.py:27:        _require_keys(cfg, ["paths", "columns", "split"], "build_dataset config")
src/fp/evaluate.py:15:from src.common.splitters import load_split_indices
src/fp/evaluate.py:79:    indices = load_split_indices(indices_dir)
src/fp/evaluate.py:103:    metrics_by_split: Dict[str, Dict[str, float]] = {}
src/fp/evaluate.py:105:    for split_name in ["train", "val", "test"]:
src/fp/evaluate.py:106:        if split_name not in indices:
src/fp/evaluate.py:108:        split_idx = indices[split_name]
src/fp/evaluate.py:109:        split_df = df.loc[split_idx]
src/fp/evaluate.py:110:        idx = split_df.index.to_numpy()
src/fp/evaluate.py:112:        y = split_df[target_col].astype(float).to_numpy()

thinking
**Inspecting dataset splitting and loading**
exec
/bin/zsh -lc "sed -n '1,200p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.metrics import regression_metrics
from src.common.plots import save_parity_plot, save_residual_plot, save_hist
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json, set_seed

thinking
**Investigating dataset loading and config structure**
exec
/bin/zsh -lc 'cat configs/process/build_dataset.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
process:
  name: "build_dataset"
  kind: "build_dataset"

thinking
**Inspecting dataset configuration files**
exec
/bin/zsh -lc 'cat configs/dataset/default.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
paths:
  raw_csv: "data/raw/tc_pc_tb_pubchem.csv"
  sdf_dir: "data/raw/sdf_files"
  out_csv: "data/processed/dataset_with_lj.csv"
  out_indices_dir: "data/processed/indices"

columns:
  cas: "CAS"
  formula: "MolecularFormula"
  tc: "Tc [K]"
  pc: "Pc [Pa]"
  tb: "Tb [K]"

seed: 42

lj:
  epsilon_method: "bird_critical"
  sigma_method: "bird_critical"
  epsilon_col: "lj_epsilon_over_k_K"
  sigma_col: "lj_sigma_A"

thinking
**Investigating dataset splitting and artifacts**
exec
/bin/zsh -lc 'rg "build_dataset" -n src scripts' in /Users/kawahito/Desktop/LJ_prediction succeeded in 294ms:
src/utils/validate_config.py:26:    if process_name == "build_dataset":
src/utils/validate_config.py:27:        _require_keys(cfg, ["paths", "columns", "split"], "build_dataset config")
src/gnn/evaluate.py:60:def _build_dataset(
src/gnn/evaluate.py:150:    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
src/gnn/evaluate.py:151:    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
src/gnn/evaluate.py:152:    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
scripts/build_dataset.py:12:# Allow running as `python scripts/build_dataset.py ...` without installing the package.
scripts/build_dataset.py:52:    logger = get_logger("build_dataset", log_file=out_csv.parent / "build_dataset.log")
src/gnn/train.py:174:    def build_dataset(split_name: str) -> List[Any]:
src/gnn/train.py:188:    train_data = build_dataset("train")
src/gnn/train.py:189:    val_data = build_dataset("val")
src/gnn/train.py:190:    test_data = build_dataset("test")

thinking
**Checking artifact usage and tests**
exec
/bin/zsh -lc "sed -n '1,220p' tests/contract/test_artifacts_contract.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
from __future__ import annotations

from pathlib import Path

from src.utils.artifacts import (
    REQUIRED_META_KEYS,
    build_meta,
    compute_config_hash,
    compute_dataset_hash,
    save_meta,
    validate_evaluate_artifacts,
    validate_predict_artifacts,
    validate_train_artifacts,
)


def _write_indices(indices_dir: Path) -> None:
    indices_dir.mkdir(parents=True, exist_ok=True)
    (indices_dir / "train.txt").write_text("0\n", encoding="utf-8")
    (indices_dir / "val.txt").write_text("0\n", encoding="utf-8")

thinking
**Searching for dataset validation**
exec
/bin/zsh -lc 'rg "validate_.*artifact" -n src/utils/artifacts.py src' in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
src/utils/artifacts.py:267:def validate_common_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:278:def validate_train_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:280:    validate_common_artifacts(run_dir)
src/utils/artifacts.py:289:def validate_evaluate_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:291:    validate_common_artifacts(run_dir)
src/utils/artifacts.py:299:def validate_predict_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:301:    validate_common_artifacts(run_dir)
src/utils/artifacts.py:267:def validate_common_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:278:def validate_train_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:280:    validate_common_artifacts(run_dir)
src/utils/artifacts.py:289:def validate_evaluate_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:291:    validate_common_artifacts(run_dir)
src/utils/artifacts.py:299:def validate_predict_artifacts(run_dir: str | Path) -> None:
src/utils/artifacts.py:301:    validate_common_artifacts(run_dir)

thinking
**Checking dataset build process**
exec
/bin/zsh -lc 'rg "build_dataset" -n docs/10_PROCESS_CATALOG.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
11:| build_dataset | scripts/build_dataset.py | raw CSV/SDF | dataset index/split, dataset_hash | Dataset Task |
26:- `train` は raw CSV/SDF を直接読むよりも、`build_dataset` の artifact を参照する（推奨）
exec
/bin/zsh -lc "sed -n '1,160p' docs/10_PROCESS_CATALOG.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
# Process Catalog（処理単位の一覧とI/O）

このドキュメントは **Process（処理単位）** のカタログです。  
「どのProcessが存在し、何を入力として、何を出力するか」を固定し、拡張してもブレないようにします。

---

## 1. Process一覧（推奨）
| Process | Script（推奨） | 主入力 | 主出力（artifact） | ClearML化の単位 |
|---|---|---|---|---|
| build_dataset | scripts/build_dataset.py | raw CSV/SDF | dataset index/split, dataset_hash | Dataset Task |
| audit_dataset | scripts/audit_dataset.py | raw CSV/SDF or processed dataset | audit_report.json, audit_report.md, plots | Audit Task |
| featurize | scripts/featurize.py | dataset artifact | features cache/manifest | Task |
| train | scripts/train.py | dataset(+features), task, model, train cfg | model.ckpt, metrics | Training Task |
| evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
| predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
| visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
| collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |

※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。

thinking
**Inspecting dataset build configuration**
exec
/bin/zsh -lc 'cat configs/dataset.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
defaults:
  - process: build_dataset
  - dataset: default
  - task: lj_epsilon
  - hydra: default
  - _self_

thinking
**Inspecting config loading approach**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
from __future__ import annotations

from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, Iterable

import yaml


class ConfigError(ValueError):
    pass


def load_yaml(path: str | Path) -> Dict[str, Any]:
    path = Path(path)
    if not path.exists():
        raise ConfigError(f"Config file not found: {path}")
    with path.open("r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if cfg is None:

thinking
**Analyzing dataset splitting needs**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/chemistry.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
from __future__ import annotations

import re
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, Iterable, List, Tuple

try:
    from rdkit import Chem
    from rdkit.Chem.Scaffolds import MurckoScaffold
except Exception:  # pragma: no cover
    Chem = None
    MurckoScaffold = None


_ELEMENT_RE = re.compile(r"([A-Z][a-z]?)(\d*)")


def parse_formula(formula: str) -> Dict[str, int]:
    """

thinking
**Evaluating where to add splitting logic**
exec
/bin/zsh -lc "sed -n '1,200p' src/utils/validate_config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 287ms:
from __future__ import annotations

from typing import Any, Dict, Iterable

from src.common.config import ConfigError


def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
    missing = [k for k in keys if k not in cfg]
    if missing:
        raise ConfigError(f"{context} missing keys: {', '.join(missing)}")


def _has_target(cfg: Dict[str, Any]) -> bool:
    data = cfg.get("data", {})
    task = cfg.get("task", {})
    return bool(data.get("target_col") or task.get("target_col"))


def validate_config(cfg: Dict[str, Any]) -> None:

thinking
**Considering dataset split method updates**
exec
/bin/zsh -lc 'cat configs/dataset/quick.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
paths:
  raw_csv: "data/raw/tc_pc_tb_pubchem.csv"
  sdf_dir: "data/raw/sdf_files"
  out_csv: "data/processed/quick/dataset_with_lj.csv"
  out_indices_dir: "data/processed/quick/indices"

columns:
  cas: "CAS"
  formula: "MolecularFormula"
  tc: "Tc [K]"
  pc: "Pc [Pa]"
  tb: "Tb [K]"

seed: 42

limit_rows: 2000

lj:
  epsilon_method: "bird_critical"
  sigma_method: "bird_critical"

thinking
**Preparing fixture dataset configuration**
exec
/bin/zsh -lc 'cat configs/dataset/fixture.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
paths:
  raw_csv: "tests/fixtures/data/raw/tc_pc_tb_fixture.csv"
  sdf_dir: "tests/fixtures/data/raw/sdf_files"
  out_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
  out_indices_dir: "tests/fixtures/data/processed/indices"

columns:
  cas: "CAS"
  formula: "MolecularFormula"
  tc: "Tc [K]"
  pc: "Pc [Pa]"
  tb: "Tb [K]"

seed: 42

lj:
  epsilon_method: "bird_critical"
  sigma_method: "bird_critical"
  epsilon_col: "lj_epsilon_over_k_K"
  sigma_col: "lj_sigma_A"

thinking
**Considering group_key addition in dataset configs**
exec
/bin/zsh -lc 'cat configs/dataset_quick.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
defaults:
  - process: build_dataset
  - dataset: quick
  - task: lj_epsilon
  - hydra: default
  - _self_

thinking
**Noting dataset fixture file**
exec
/bin/zsh -lc 'cat configs/dataset_fixture.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
defaults:
  - process: build_dataset
  - dataset: fixture
  - task: lj_epsilon
  - hydra: default
  - _self_

thinking
**Searching for dataset building tests**
exec
/bin/zsh -lc 'rg "build_dataset" -n tests' in /Users/kawahito/Desktop/LJ_prediction succeeded in 292ms:
tests/fixtures/data/processed/dataset_config_snapshot.yaml:2:  name: build_dataset
tests/fixtures/data/processed/dataset_config_snapshot.yaml:3:  kind: build_dataset

thinking
**Checking available test files**
exec
/bin/zsh -lc "sed -n '1,200p' tests/test_fixture_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
from __future__ import annotations

from pathlib import Path

import pandas as pd


def test_fixture_dataset_files_exist() -> None:
    root = Path(__file__).resolve().parents[1]
    csv_path = root / "tests" / "fixtures" / "data" / "raw" / "tc_pc_tb_fixture.csv"
    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"

    assert csv_path.exists()
    assert sdf_dir.exists()

    df = pd.read_csv(csv_path)
    required_cols = {"CAS", "MolecularFormula", "Tc [K]", "Pc [Pa]", "Tb [K]"}
    assert required_cols.issubset(set(df.columns))

    missing = []

thinking
**Planning to add split tests**
exec
/bin/zsh -lc "sed -n '1,240p' tests/test_audit_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

from pathlib import Path

import numpy as np
import pandas as pd
import pytest

rdkit = pytest.importorskip("rdkit")
from rdkit import Chem

from src.data.audit import audit_dataset


def _write_sdf(path: Path, smiles: str) -> None:
    mol = Chem.MolFromSmiles(smiles)
    assert mol is not None
    writer = Chem.SDWriter(str(path))
    writer.write(mol)
    writer.close()

thinking
**Inspecting audit data module**
exec
/bin/zsh -lc "sed -n '1,240p' src/data/audit.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 280ms:
from __future__ import annotations

from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.io import load_sdf_mol
from src.common.splitters import load_split_indices
from src.utils.artifacts import compute_dataset_hash

try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors
except Exception:  # pragma: no cover
    Chem = None
    Descriptors = None


thinking
**Planning dataset splitting and artifact updates**

I'm thinking through how to implement new group splitting alongside existing scaffold and random splits, add leakage validation, and produce a split.json artifact without breaking backward compatibility with the current .txt indices files. I want to make sure the split.json ends up in the right dataset artifact folder per docs, while maintaining compatibility with training code that relies on the old indices. I'll also look for existing dataset artifact code to confirm how to integrate this properly.
exec
/bin/zsh -lc 'rg "dataset_index" -n' in /Users/kawahito/Desktop/LJ_prediction succeeded in 305ms:
docs/04_ARTIFACTS_AND_VERSIONING.md:26:  - `dataset_index.csv`（sample_id一覧、split含めてもよい）

thinking
**Evaluating split.json placement and dataset hashing**

I'm thinking through where to save split.json for artifact compliance—whether inside indices_dir or a separate dataset folder—and how to update compute_dataset_hash accordingly to cover it. I’ll check the config for output directories to decide the best approach aligned with existing patterns.
exec
/bin/zsh -lc 'cat configs/hydra/default.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
hydra:
  run:
    dir: "runs/${process.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}_${experiment.name}"
  job:
    name: "${process.name}"

thinking
**Examining artifact handling and output locations**
exec
/bin/zsh -lc "sed -n '1,200p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.metrics import regression_metrics
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.feature_utils import hash_cfg
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config


thinking
**Investigating artifact handling for datasets**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/meta.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 262ms:
from __future__ import annotations

from src.utils.artifacts import build_meta, save_meta, utc_now_iso

__all__ = ["build_meta", "save_meta", "utc_now_iso"]

thinking
**Analyzing dataset splitting and config handling**

I'm exploring the current splitting strategies—random, scaffold, and group—to ensure reproducible splits with stable seeds and no overlap between splits or groups. I plan to implement validation functions and coverage tests, especially for scaffold splits using SDF files and rdkit. For configuration, I need to carefully handle the split fraction keys by supporting both "ratios" for backward compatibility and the newer "fractions" key, though this might cause some complexity in docs and tests, so I'll verify fixtures and references accordingly.
exec
/bin/zsh -lc 'rg "dataset_config_snapshot" -n' in /Users/kawahito/Desktop/LJ_prediction succeeded in 312ms:
objective.md:53:- `data/processed/dataset_config_snapshot.yaml`
scripts/build_dataset.py:189:    dump_yaml(out_csv.parent / "dataset_config_snapshot.yaml", cfg)
README.md:104:  - `data/processed/dataset_config_snapshot.yaml`
work/REPO_ORIENTATION.md:25:     -> data/processed/dataset_config_snapshot.yaml
work/REPO_ORIENTATION.md:42:| scripts/build_dataset.py | configs/dataset*.yaml | data/raw/tc_pc_tb_pubchem.csv, data/raw/sdf_files | data/processed/dataset_with_lj.csv, data/processed/indices/*.txt, data/processed/dataset_config_snapshot.yaml, data/processed/build_dataset.log |

thinking
**Planning split validation and module structure**

I'm thinking through adding split validation functions to check for overlapping indices, scaffold leakage, and group leakage, and considering the best file organization—whether to extend the existing src/common/splitters or create a new src/data/splitting module—while aiming to keep imports stable and avoid breaking existing code. I'll also look into how molecules are loaded for scaffold calculations and verify test fixtures handle RDKit properly.
exec
/bin/zsh -lc "sed -n '1,200p' src/common/io.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 327ms:
from __future__ import annotations

from pathlib import Path
from typing import Optional

import pandas as pd

try:
    from rdkit import Chem
except Exception:  # pragma: no cover
    Chem = None


def read_csv(path: str | Path) -> pd.DataFrame:
    return pd.read_csv(Path(path))


def write_csv(df: pd.DataFrame, path: str | Path) -> None:
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)

thinking
**Designing group split logic**

I’m exploring how to implement group-based splitting by mapping logical group keys to actual dataset columns and handling missing group values by making them unique to avoid leakage. I’ll create a general group splitting function that allocates groups to splits deterministically with a seed, and I’m also planning how to save the splitting metadata alongside dataset indices for reproducibility. This approach should align with existing scaffold splitting methods and handle edge cases like missing groups thoughtfully.
exec
/bin/zsh -lc "sed -n '1,120p' work/REPO_ORIENTATION.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 312ms:
# Repo Orientation (S00)

この文書は agentskills/S00_repo_orientation の出力です。

## 1) ルート構成マップ
- agentskills/: Codex向けスキル群（作業手順/ルーティング）
- codex/: セッション文脈・プロンプトテンプレ・運用チェック
- configs/: 実行設定（dataset/fp/gnn + quick）
- data/: raw/processed データ置き場（ローカル）
- docs/: 不変条件/契約/設計/ADR
- scripts/: CLIエントリポイント（dataset/train/evaluate/predict/visualize）
- src/: 実装本体（common/fp/gnn）
- tools/: 補助スクリプト（Codex向け）
- work/: タスク/バックログ/RFC/テンプレ
- runs/: 実行成果物の出力先（実行時に生成）
- requirements.txt: Python依存

## 2) scripts/ 実行動線（dataset → train → predict）

```

thinking
**Planning group and scaffold splitting logic**

I'm thinking through how to implement new splitters like group_split and improve scaffold_split, focusing on maintaining group constraints without fallback to random splits that cause leakage. I'm considering adjustments to ensure val/test are not empty by shifting group assignments, while handling edge cases with few groups, all to balance acceptance criteria and training needs.
exec
/bin/zsh -lc "sed -n '1,200p' src/common/metrics.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 290ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


def regression_metrics(y_true, y_pred) -> Dict[str, float]:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = float(mean_squared_error(y_true, y_pred))
    return {
        "mae": float(mean_absolute_error(y_true, y_pred)),
        "rmse": float(np.sqrt(mse)),
        "r2": float(r2_score(y_true, y_pred)),
    }

thinking
**Designing group-aware split algorithm**

I'm working through how to split dataset groups into train/val/test splits without any empty splits, especially for scaffold splitting. I want to ensure each split has at least one group when possible by assigning groups greedily based on target ratios and balancing sizes, possibly pre-assigning the largest groups to guarantee non-empty splits. I’ll also consider raising errors if splits can't be properly formed instead of falling back to random splits, to maintain clear leakage prevention. Let's explore this approach carefully!
exec
/bin/zsh -lc 'wc -l tests/fixtures/data/raw/tc_pc_tb_fixture.csv && head -n 5 tests/fixtures/data/raw/tc_pc_tb_fixture.csv' in /Users/kawahito/Desktop/LJ_prediction succeeded in 312ms:
       7 tests/fixtures/data/raw/tc_pc_tb_fixture.csv
CAS,MolecularFormula,Tc [K],Pc [Pa],Tb [K]
64-17-5,C2H6O,514.0,6140000,351.5
74-98-6,C3H8,369.8,4250000,231.1
74-85-1,C2H4,282.3,5040000,169.4
67-64-1,C3H6O,508.2,4700000,329.4

thinking
**Planning scaffold split and JSON saving**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/utils.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
from __future__ import annotations

import json
import logging
import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np


def ensure_dir(path: str | Path) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def set_seed(seed: int) -> None:

thinking
**Reviewing JSON save behavior**
exec
/bin/zsh -lc "sed -n '1,200p' tests/test_config_loader.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from pathlib import Path

from src.common.config import load_config


def test_load_config_resolves_groups_from_root() -> None:
    cfg = load_config(Path("configs/fp/train_fixture.yaml"))

    assert cfg["process"]["name"] == "train"
    assert cfg["process"]["backend"] == "fp"
    assert cfg["output"]["run_dir"] == "runs/train/fp"
    assert cfg["paths"]["raw_csv"] == "tests/fixtures/data/raw/tc_pc_tb_fixture.csv"


thinking
**Creating detailed multi-step plan for dataset splitting and validation**

I’m laying out a thorough plan to implement dataset splitting with group and scaffold awareness, including validation functions to prevent overlap and group leakage, save split metadata as JSON, and add reproducible tests using pytest and RDKit. This involves adjusting configs, updating build scripts, and ensuring the split artifacts integrate cleanly with hashes and indexes. Next, I’ll refine the plan and start implementing the substeps.
thinking
**Designing validation functions and split manifest format**

I’m writing detailed validation functions to check index overlaps and group leakage, including careful handling of missing group keys. I’m also sketching how to produce a split manifest JSON capturing split parameters and indices. Considering performance, I’m planning to optimize scaffold splits by optionally reusing precomputed molecule lists to avoid repeated SDF loading. Next, I’ll proceed to finalize the plan and implement these components step by step.
Plan update
Plan for adding split strategies + artifacts + tests.
  → Implement split helpers (group split, validation, split.json) and wire into build_dataset
  • Update dataset configs to include split method options/keys (group_key, fractions/ratios compatibility)
  • Add pytest coverage for reproducible splits and scaffold leakage check
  • Update work/queue.json to done after validation

thinking
**Designing group split implementation**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/splitters.py
@@ -4,3 +4,3 @@
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
@@ -11,2 +11,3 @@
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
@@ -53,2 +54,75 @@
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        key = _normalize_group_key(value, row_idx)
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
@@ -67,3 +141,2 @@
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
@@ -75,28 +148,67 @@
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    return {"train": train, "val": val, "test": test}
 
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
+
+
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
+
 
apply_patch(auto_approved=true) exited 0 in 30ms:
Success. Updated the following files:
M src/common/splitters.py
file update:
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..f97b368bf183e52ce16cf828035c128c2d15b041
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,79 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        key = _normalize_group_key(value, row_idx)
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +139,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,32 +146,71 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    return {"train": train, "val": val, "test": test}
 
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
+
+
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
+
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
     out_dir = Path(out_dir)

file update:
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..f97b368bf183e52ce16cf828035c128c2d15b041
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,79 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        key = _normalize_group_key(value, row_idx)
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +139,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,32 +146,71 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    return {"train": train, "val": val, "test": test}
 
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
+
+
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
+
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
     out_dir = Path(out_dir)


thinking
**Improving dataset splitting with group handling**

I'm refining the dataset build process to support group-based splits, including normalizing group keys consistently by adding a helper to build the group map. This will ensure group leakage validation works reliably, and I'll update splitters and validations to use this shared mapping while carefully handling missing or empty groups. I'll also enhance config parsing to flexibly handle split settings like method, ratios, and group keys.
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/splitters.py
@@ -65,2 +65,9 @@
 
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
 def _split_groups_by_count(
@@ -118,4 +125,4 @@
     groups: Dict[str, List[int]] = {}
-    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
-        key = _normalize_group_key(value, row_idx)
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
         groups.setdefault(key, []).append(row_idx)
apply_patch(auto_approved=true) exited 0 in 59ms:
Success. Updated the following files:
M src/common/splitters.py
file update:
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:

file update:
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:


thinking
**Enhancing dataset build for group splits**

I'm updating build_dataset to import and use new split-related functions, adding support for group-based splitting with associated validations and metadata saving. I'll adjust the config parsing to handle "fractions" consistently, ensure proper error checks for group keys, log split details, and save split metadata as JSON alongside indices.
file update
M /Users/kawahito/Desktop/LJ_prediction/scripts/build_dataset.py
@@ -21,3 +21,13 @@
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
@@ -173,3 +183,3 @@
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
@@ -181,2 +191,13 @@
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
@@ -184,3 +205,8 @@
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
apply_patch(auto_approved=true) exited 0 in 40ms:
Success. Updated the following files:
M scripts/build_dataset.py
file update:
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:

file update:
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:


thinking
**Planning dataset config updates**

I’m mapping out changes to dataset configs to switch from using `ratios` to `fractions` and add an optional `group_key` (like "cas"). I want to update all relevant configs for consistency while keeping backward compatibility smooth. Starting with the default config to apply these patches carefully.
file update
M /Users/kawahito/Desktop/LJ_prediction/configs/dataset/default.yaml
@@ -40,3 +40,4 @@
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
apply_patch(auto_approved=true) exited 0 in 31ms:
Success. Updated the following files:
M configs/dataset/default.yaml
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:


thinking
**Updating quick and fixture**
file update
M /Users/kawahito/Desktop/LJ_prediction/configs/dataset/quick.yaml
@@ -42,3 +42,4 @@
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
M configs/dataset/quick.yaml
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:


thinking
**Updating test fixture**
file update
M /Users/kawahito/Desktop/LJ_prediction/configs/dataset/fixture.yaml
@@ -31,3 +31,4 @@
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
apply_patch(auto_approved=true) exited 0 in 48ms:
Success. Updated the following files:
M configs/dataset/fixture.yaml
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:


thinking
**Updating plan status after step1 completion**
Plan update
  ✓ Implement split helpers (group split, validation, split.json) and wire into build_dataset
  ✓ Update dataset configs to include split method options/keys (group_key, fractions/ratios compatibility)
  → Add pytest coverage for reproducible splits and scaffold leakage check
  • Update work/queue.json to done after validation
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:


thinking
**Designing tests for dataset splitters**

I’m planning to add a new test file covering random, group, and scaffold splits, carefully choosing split ratios that avoid empty validation sets due to group size constraints. I’ll implement helpers for SDF writing with RDKit and set up checks to validate leakage and reproducibility, aiming for thorough but stable tests.
file update
A /Users/kawahito/Desktop/LJ_prediction/tests/test_splitters.py
from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from src.common.splitters import (
    build_group_map,
    group_split,
    random_split,
    scaffold_split,
    validate_group_leakage,
    validate_scaffold_split,
    validate_split_indices,
)


def test_random_split_reproducible() -> None:
    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)

    assert first == second
    validate_split_indices(first)


def test_group_split_keeps_groups() -> None:
    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)

    group_map = build_group_map(df, group_col="group")
    validate_group_leakage(indices, group_map, label="group")


rdkit = pytest.importorskip("rdkit")
from rdkit import Chem


def _write_sdf(path: Path, smiles: str) -> None:
    mol = Chem.MolFromSmiles(smiles)
    assert mol is not None
    writer = Chem.SDWriter(str(path))
    writer.write(mol)
    writer.close()


def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
    sdf_dir = tmp_path / "sdf"
    sdf_dir.mkdir()

    cas_values = ["A", "B", "C", "D", "E", "F"]
    smiles_list = [
        "c1ccccc1",
        "c1ccccc1O",
        "C1CCCCC1",
        "C1CCCCC1O",
        "n1ccccc1",
        "n1ccccc1O",
    ]
    for cas, smiles in zip(cas_values, smiles_list):
        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)

    df = pd.DataFrame({"CAS": cas_values})
    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)

    validate_split_indices(indices)
    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
apply_patch(auto_approved=true) exited 0 in 72ms:
Success. Updated the following files:
A tests/test_splitters.py
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..42c4eefe7c9d039acf1829a082eaddd9bb84e28f
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,68 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+rdkit = pytest.importorskip("rdkit")
+from rdkit import Chem
+
+
+def _write_sdf(path: Path, smiles: str) -> None:
+    mol = Chem.MolFromSmiles(smiles)
+    assert mol is not None
+    writer = Chem.SDWriter(str(path))
+    writer.write(mol)
+    writer.close()
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..42c4eefe7c9d039acf1829a082eaddd9bb84e28f
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,68 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+rdkit = pytest.importorskip("rdkit")
+from rdkit import Chem
+
+
+def _write_sdf(path: Path, smiles: str) -> None:
+    mol = Chem.MolFromSmiles(smiles)
+    assert mol is not None
+    writer = Chem.SDWriter(str(path))
+    writer.write(mol)
+    writer.close()
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Adjusting rdkit import handling in tests**

I'm sorting out how to prevent the entire test module from being skipped if rdkit is missing by moving the `pytest.importorskip("rdkit")` call inside specific scaffold tests instead of at module level. This way, tests unrelated to rdkit can still run, and the helper `_write_sdf` can be defined inside the test that imports rdkit to access Chem conveniently.
file update
M /Users/kawahito/Desktop/LJ_prediction/tests/test_splitters.py
@@ -35,15 +35,13 @@
 
-rdkit = pytest.importorskip("rdkit")
-from rdkit import Chem
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
 
-
-def _write_sdf(path: Path, smiles: str) -> None:
-    mol = Chem.MolFromSmiles(smiles)
-    assert mol is not None
-    writer = Chem.SDWriter(str(path))
-    writer.write(mol)
-    writer.close()
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
 
-
-def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
     sdf_dir = tmp_path / "sdf"
apply_patch(auto_approved=true) exited 0 in 81ms:
Success. Updated the following files:
M tests/test_splitters.py
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Updating test strategy and plan**
Plan update
  ✓ Implement split helpers (group split, validation, split.json) and wire into build_dataset
  ✓ Update dataset configs to include split method options/keys (group_key, fractions/ratios compatibility)
  ✓ Add pytest coverage for reproducible splits and scaffold leakage check
  → Update work/queue.json to done after validation
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Evaluating split validation and testing approach**

I’m carefully checking how the group leakage validation handles missing or unique keys and confirming that duplicate indices within splits are detected properly. I’m also considering unused imports cleanup and deciding whether to run targeted tests on splitters to verify my changes, weighing environment constraints but leaning toward running them.
exec
/bin/zsh -lc 'pytest -q tests/test_splitters.py' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 2.33s:
F..                                                                      [100%]
=================================== FAILURES ===================================
________________________ test_random_split_reproducible ________________________

    def test_random_split_reproducible() -> None:
        df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
        first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
        second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
    
>       assert first == second
E       AssertionError: assert {'test': [3, ...7, 5, 14, 22]} == {'test': [7, ...0, 11, 8, 26]}
E         
E         Differing items:
E         {'train': [2, 29, 18, 7, 27, 11, ...]} != {'train': [18, 13, 16, 24, 3, 1, ...]}
E         {'val': [12, 26, 17, 5, 14, 22]} != {'val': [15, 22, 20, 11, 8, 26]}
E         {'test': [3, 25, 13]} != {'test': [7, 14, 0]}
E         Use -v to get more diff

tests/test_splitters.py:24: AssertionError
=========================== short test summary info ============================
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..d564d2965cc6c4553541763100997fdc57c16f63
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Diagnosing non-reproducible random_split**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/splitters.py
@@ -21,3 +21,3 @@
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
apply_patch(auto_approved=true) exited 0 in 58ms:
Success. Updated the following files:
M src/common/splitters.py
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..f1c644a2576e8a71762e9c4a84ce7cf959df4f48
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +19,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
+
+
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..f1c644a2576e8a71762e9c4a84ce7cf959df4f48
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +19,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
+
+
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Rerunning tests**
exec
/bin/zsh -lc 'pytest -q tests/test_splitters.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 1.61s:
...                                                                      [100%]
3 passed in 1.15s
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..f1c644a2576e8a71762e9c4a84ce7cf959df4f48
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -2,13 +2,14 @@
 
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +19,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +52,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +146,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +153,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
+
+
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
+
+
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Preparing to update task status and clean imports**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/splitters.py
@@ -2,3 +2,2 @@
 
-from dataclasses import dataclass
 from pathlib import Path
apply_patch(auto_approved=true) exited 0 in 57ms:
Success. Updated the following files:
M src/common/splitters.py
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Preparing to update queue.json status**
exec
/bin/zsh -lc 'date -u "+%Y-%m-%dT%H:%M:%S.%6NZ"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 287ms:
2025-12-26T14:40:18.6NZ
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Analyzing date command output quirks**
exec
/bin/zsh -lc 'date -u "+%Y-%m-%dT%H:%M:%SZ"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
2025-12-26T14:40:26Z
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)


thinking
**Preparing to update task status in JSON**
file update
M /Users/kawahito/Desktop/LJ_prediction/work/queue.json
@@ -2,3 +2,3 @@
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
@@ -146,3 +146,3 @@
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
apply_patch(auto_approved=true) exited 0 in 50ms:
Success. Updated the following files:
M work/queue.json
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [

file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [


thinking
**Updating plan progress**
Plan update
  ✓ Implement split helpers (group split, validation, split.json) and wire into build_dataset
  ✓ Update dataset configs to include split method options/keys (group_key, fractions/ratios compatibility)
  ✓ Add pytest coverage for reproducible splits and scaffold leakage check
  ✓ Update work/queue.json to done after validation
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [


thinking
**Checking for unintended changes**
exec
/bin/zsh -lc 'git status -sb' in /Users/kawahito/Desktop/LJ_prediction succeeded in 280ms:
## test_chemkit
 M README.md
 M configs/dataset.yaml
 M configs/dataset_quick.yaml
 M configs/fp/predict.yaml
 M configs/fp/predict_quick.yaml
 M configs/fp/train.yaml
 M configs/fp/train_quick.yaml
 M configs/gnn/predict.yaml
 M configs/gnn/train.yaml
 M configs/gnn/train_mpnn_quick.yaml
 M configs/gnn/train_quick.yaml
 M objective.md
 M scripts/build_dataset.py
 D scripts/fp_predict.py
 D scripts/fp_train.py
 D scripts/gnn_predict.py
 D scripts/gnn_train.py
 M src/common/config.py
 M src/common/splitters.py
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [


thinking
**Reviewing configuration and code updates**

I'm making sure all necessary config files and related code reflect the recent changes, especially focusing on dataset splits and fraction keys. I’ll also verify scripts and utilities to confirm compatibility without reverting or disturbing unrelated git changes.
exec
/bin/zsh -lc "sed -n '1,240p' src/common/splitters.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
from __future__ import annotations

from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd

from .chemistry import murcko_scaffold_smiles
from .io import load_sdf_mol, sdf_path_from_cas
from .utils import save_json


def random_split(
    df: pd.DataFrame,
    ratios: Sequence[float] = (0.8, 0.1, 0.1),
    seed: int = 42,
) -> Dict[str, List[int]]:
    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
    rng = np.random.default_rng(seed)
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [



thinking
**Summarizing comprehensive update plan**

I've gathered a detailed overview of changes, tests, verification, compatibility impacts, and the final queue update; now I’m preparing to consolidate this into a structured summary that includes file-level plans, implementation diffs, test details, verification results, compatibility notes, and next steps suggestions in a clear, bullet-point format.
thinking
**Finalizing structured summary plan**

I'm organizing the summary with six clear bullets covering tests, verification, compatibility, queue update, implementation diffs, and skills used, including a unified diff snippet to visibly show key code changes for the splitters and build_dataset updates.
exec
/bin/zsh -lc "sed -n '150,240p' scripts/build_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 285ms:
    df.loc[df[eps_col].isna() | df[sig_col].isna(), "lj_valid_flag"] = False
    if valid_eps_range:
        lo, hi = float(valid_eps_range[0]), float(valid_eps_range[1])
        df.loc[(df[eps_col] < lo) | (df[eps_col] > hi), "lj_valid_flag"] = False
    if valid_sig_range:
        lo, hi = float(valid_sig_range[0]), float(valid_sig_range[1])
        df.loc[(df[sig_col] < lo) | (df[sig_col] > hi), "lj_valid_flag"] = False

    # Apply selectors
    selectors = cfg.get("selectors", []) or []
    ctx = SelectorContext(element_counts=element_counts_global, mols=mols if _needs_mols(selectors) else None, fps=None)

    # Always filter invalid LJ first unless user opted out
    if cfg.get("filter_invalid_lj", True):
        before = df.shape[0]
        df = df[df["lj_valid_flag"]].copy()
        logger.info(f"Filtered invalid LJ rows: {before} -> {df.shape[0]}")

    if selectors:
        before = df.shape[0]
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [



thinking
**Summarizing changes and next steps**

I'm gathering all key changes made: split strategy improvements with group/scaffold validation, config updates for fractions and group_key, new split.json artifact saving, and added tests with successful pytest runs. I'll organize a clear, detailed bullet list covering implementation diffs, testing, compatibility notes, and task queue status along with sensible next-step commands for verification.
thinking
**Compiling detailed implementation summary**

I’m preparing a thorough overview including code diffs with key changes for splitters and build_dataset, clear testing info with test names and verification results, compatibility notes about config keys and artifacts, plus queue updates and actionable next steps—showing actual diff lines where possible for clarity while keeping it concise and easy to follow.
codex
Using S20_dataset_loader, S10_hydra_config, S95_tests_ci because this adds split logic, updates dataset configs, and introduces new tests.
Implemented configurable random/scaffold/group splits with validation and `split.json` artifact output, plus config updates and reproducibility/leakage tests.
- **変更計画** `src/common/splitters.py` `scripts/build_dataset.py` `configs/dataset/default.yaml` `configs/dataset/quick.yaml` `configs/dataset/fixture.yaml` `tests/test_splitters.py` `work/queue.json`
- **実装** `src/common/splitters.py` `scripts/build_dataset.py` の主要差分
```diff
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+def save_split_json(indices: Dict[str, List[int]], out_path: str | Path, metadata: Optional[Dict[str, object]] = None) -> None:
```
```diff
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
```
- **追加/更新したテスト** `tests/test_splitters.py`（`test_random_split_reproducible` `test_group_split_keeps_groups` `test_scaffold_split_no_leakage`）
- **検証コマンド** `pytest -q tests/test_splitters.py`（pass）
- **互換性影響** `split.fractions` を追加し `split.ratios` は後方互換で保持、`split.group_key` は group 方法で必須、`split.json` が `indices` に追加され dataset_hash が変わる、scaffold/group は空split時にエラー
- **Queue更新** `work/queue.json` の task 035 を done に更新

Next steps:
1) `pytest -q` で全体テストを確認
2) `configs/dataset/default.yaml` の `split.method` を `group` に切り替えて `python scripts/build_dataset.py --config configs/dataset.yaml` を実行し split.json を確認
file update:
diff --git a/configs/dataset/default.yaml b/configs/dataset/default.yaml
index 420716a1af14818d1220b8a67d9779ba55f1c28b..5ef287943770255fd82437608a45a1705e7bd091
--- a/configs/dataset/default.yaml
+++ b/configs/dataset/default.yaml
@@ -38,7 +38,8 @@
 split:
   method: "scaffold"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/fixture.yaml b/configs/dataset/fixture.yaml
index 829982958878cb80b175082a46c1d5a9aad79722..502766f0954d284403a2dc253d4994e3bac6f380
--- a/configs/dataset/fixture.yaml
+++ b/configs/dataset/fixture.yaml
@@ -29,7 +29,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.6, 0.2, 0.2]
+  fractions: [0.6, 0.2, 0.2]
+  group_key: "cas"
 
 data:
   dataset_csv: "tests/fixtures/data/processed/dataset_with_lj.csv"
diff --git a/configs/dataset/quick.yaml b/configs/dataset/quick.yaml
index 141fa1641811725749e6abebf7587ef7cd5004da..287252fd2bc83a3f4ea78965bb86ea0cc7dfeba2
--- a/configs/dataset/quick.yaml
+++ b/configs/dataset/quick.yaml
@@ -40,7 +40,8 @@
 split:
   method: "random"
   seed: 42
-  ratios: [0.8, 0.1, 0.1]
+  fractions: [0.8, 0.1, 0.1]
+  group_key: "cas"
 
 data:
   dataset_csv: "data/processed/quick/dataset_with_lj.csv"
diff --git a/scripts/build_dataset.py b/scripts/build_dataset.py
index df5e9b8e42b1f42bc7a1adae5f73863b9be2e4be..73503c6eac1e64a88c275809cb635531f3c214f4
--- a/scripts/build_dataset.py
+++ b/scripts/build_dataset.py
@@ -19,7 +19,17 @@
 from src.common.dataset_selectors import SelectorContext, apply_selectors
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv
 from src.common.lj import compute_lj
-from src.common.splitters import random_split, scaffold_split, save_split_indices
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    save_split_indices,
+    save_split_json,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
 from src.common.utils import get_logger, set_seed
 from src.utils.validate_config import validate_config
 
@@ -171,7 +181,7 @@
     # Split
     split_cfg = cfg.get("split", {})
     split_method = str(split_cfg.get("method", "random")).lower()
-    ratios = split_cfg.get("ratios", [0.8, 0.1, 0.1])
+    ratios = split_cfg.get("fractions", split_cfg.get("ratios", [0.8, 0.1, 0.1]))
     ratios = [float(x) for x in ratios]
     split_seed = int(split_cfg.get("seed", seed))
 
@@ -179,10 +189,26 @@
         indices = random_split(df, ratios=ratios, seed=split_seed)
     elif split_method == "scaffold":
         indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, ratios=ratios, seed=split_seed)
+        validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col=cas_col, indices=indices)
+    elif split_method == "group":
+        group_key = split_cfg.get("group_key")
+        if not group_key:
+            raise ValueError("split.method=group requires split.group_key")
+        group_col = cols.get(str(group_key), str(group_key))
+        if group_col not in df.columns:
+            raise ValueError(f"split.group_key '{group_key}' resolved to '{group_col}' not in dataset columns")
+        indices = group_split(df, group_col=group_col, ratios=ratios, seed=split_seed)
+        group_map = build_group_map(df, group_col=group_col)
+        validate_group_leakage(indices, group_map, label=f"group:{group_col}")
     else:
         raise ValueError(f"Unknown split.method: {split_method}")
 
+    validate_split_indices(indices)
     save_split_indices(indices, out_indices_dir)
+    split_meta = {"method": split_method, "seed": split_seed, "fractions": ratios}
+    if split_method == "group":
+        split_meta["group_key"] = str(split_cfg.get("group_key"))
+    save_split_json(indices, out_indices_dir / "split.json", metadata=split_meta)
     logger.info(f"Saved split indices to {out_indices_dir}")
 
     # Save snapshot of config used
diff --git a/src/common/splitters.py b/src/common/splitters.py
index afda96077a0e7ca09bcbcc0ac2a6115ee3ef8015..352d3b96dbe57a1a10528676dc2fd8fae3fa2c25
--- a/src/common/splitters.py
+++ b/src/common/splitters.py
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
-from typing import Dict, List, Optional, Sequence, Tuple
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 import numpy as np
 import pandas as pd
 
 from .chemistry import murcko_scaffold_smiles
 from .io import load_sdf_mol, sdf_path_from_cas
+from .utils import save_json
 
 
 def random_split(
@@ -18,7 +18,7 @@
 ) -> Dict[str, List[int]]:
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
     rng = np.random.default_rng(seed)
-    idx = df.index.to_numpy()
+    idx = df.index.to_numpy().copy()
     rng.shuffle(idx)
     n = len(idx)
     if n == 0:
@@ -51,6 +51,86 @@
     return {"train": train_idx.tolist(), "val": val_idx.tolist(), "test": test_idx.tolist()}
 
 
+def _normalize_group_key(value: object, row_idx: int) -> str:
+    if value is None:
+        return f"__missing__{row_idx}"
+    if isinstance(value, float) and np.isnan(value):
+        return f"__missing__{row_idx}"
+    text = str(value).strip()
+    if not text or text.lower() in {"nan", "none"}:
+        return f"__missing__{row_idx}"
+    return text
+
+
+def build_group_map(df: pd.DataFrame, group_col: str) -> Dict[int, str]:
+    group_map: Dict[int, str] = {}
+    for row_idx, value in zip(df.index.tolist(), df[group_col].tolist()):
+        group_map[row_idx] = _normalize_group_key(value, row_idx)
+    return group_map
+
+
+def _split_groups_by_count(
+    groups: Iterable[List[int]],
+    ratios: Sequence[float],
+    seed: int,
+) -> Dict[str, List[int]]:
+    assert len(ratios) == 3, "ratios must be a 3-length sequence"
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    rng = np.random.default_rng(seed)
+
+    group_list = [list(g) for g in groups if g]
+    if not group_list:
+        return {"train": [], "val": [], "test": []}
+
+    group_list.sort(key=len, reverse=True)
+    rng.shuffle(group_list)
+
+    n_total = sum(len(g) for g in group_list)
+    n_train = int(ratios[0] * n_total)
+    n_val = int(ratios[1] * n_total)
+
+    train, val, test = [], [], []
+    for g in group_list:
+        if len(train) + len(g) <= n_train:
+            train.extend(g)
+        elif len(val) + len(g) <= n_val:
+            val.extend(g)
+        else:
+            test.extend(g)
+    return {"train": train, "val": val, "test": test}
+
+
+def _validate_non_empty_splits(indices: Dict[str, List[int]], ratios: Sequence[float]) -> None:
+    for split_name, ratio in zip(["train", "val", "test"], ratios):
+        if ratio > 0 and len(indices.get(split_name, [])) == 0:
+            raise ValueError(f"Split '{split_name}' is empty. Adjust ratios or split method.")
+
+
+def group_split(
+    df: pd.DataFrame,
+    group_col: str,
+    ratios: Sequence[float] = (0.8, 0.1, 0.1),
+    seed: int = 42,
+) -> Dict[str, List[int]]:
+    """
+    Group-based split.
+
+    Rows sharing the same group key are kept in the same split to prevent leakage.
+    """
+    assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
+    if group_col not in df.columns:
+        raise ValueError(f"group_col not in dataframe: {group_col}")
+
+    groups: Dict[str, List[int]] = {}
+    group_map = build_group_map(df, group_col)
+    for row_idx, key in group_map.items():
+        groups.setdefault(key, []).append(row_idx)
+
+    indices = _split_groups_by_count(groups.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
 def scaffold_split(
     df: pd.DataFrame,
     sdf_dir: str | Path,
@@ -65,7 +145,6 @@
     This helps reduce train-test leakage by placing similar scaffolds together.
     """
     assert abs(sum(ratios) - 1.0) < 1e-6, "ratios must sum to 1.0"
-    rng = np.random.default_rng(seed)
 
     scaffolds: Dict[str, List[int]] = {}
     for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
@@ -73,31 +152,70 @@
         scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
         scaffolds.setdefault(scaf, []).append(row_idx)
 
-    # Sort scaffold groups by size descending to pack large scaffolds first
-    groups = list(scaffolds.values())
-    groups.sort(key=len, reverse=True)
-    # Shuffle groups with same size for randomness
-    # (deterministic but not critical; we do a simple shuffle of entire list after stable sort)
-    rng.shuffle(groups)
+    indices = _split_groups_by_count(scaffolds.values(), ratios=ratios, seed=seed)
+    _validate_non_empty_splits(indices, ratios)
+    return indices
+
+
+def validate_split_indices(indices: Dict[str, List[int]]) -> None:
+    seen = set()
+    duplicates = set()
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            if idx in seen:
+                duplicates.add(idx)
+            else:
+                seen.add(idx)
+    if duplicates:
+        dup_list = sorted(duplicates)
+        raise ValueError(f"Duplicate indices across splits: {dup_list[:5]}")
+
+
+def validate_group_leakage(indices: Dict[str, List[int]], group_map: Dict[int, str], label: str) -> None:
+    group_to_split: Dict[str, str] = {}
+    leakage: Dict[str, List[str]] = {}
+    for split_name, idxs in indices.items():
+        for idx in idxs:
+            group = group_map.get(idx, "")
+            prev = group_to_split.get(group)
+            if prev is None:
+                group_to_split[group] = split_name
+            elif prev != split_name:
+                leakage.setdefault(group, [prev]).append(split_name)
+    if leakage:
+        sample = list(leakage.items())[:3]
+        raise ValueError(f"{label} leakage across splits detected: {sample}")
 
-    n_total = df.shape[0]
-    n_train = int(ratios[0] * n_total)
-    n_val = int(ratios[1] * n_total)
 
-    train, val, test = [], [], []
-    for g in groups:
-        if len(train) + len(g) <= n_train:
-            train.extend(g)
-        elif len(val) + len(g) <= n_val:
-            val.extend(g)
-        else:
-            test.extend(g)
+def validate_scaffold_split(
+    df: pd.DataFrame,
+    sdf_dir: str | Path,
+    cas_col: str,
+    indices: Dict[str, List[int]],
+) -> None:
+    scaffold_map: Dict[int, str] = {}
+    for row_idx, cas in zip(df.index.tolist(), df[cas_col].astype(str).tolist()):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        scaf = murcko_scaffold_smiles(mol) if mol is not None else ""
+        scaffold_map[row_idx] = scaf
+    validate_group_leakage(indices, scaffold_map, label="scaffold")
 
-    # Safety: if val is empty (can happen for extreme grouping), fallback to random split
-    if len(val) == 0 or len(test) == 0:
-        return random_split(df, ratios=ratios, seed=seed)
 
-    return {"train": train, "val": val, "test": test}
+def save_split_json(
+    indices: Dict[str, List[int]],
+    out_path: str | Path,
+    metadata: Optional[Dict[str, object]] = None,
+) -> None:
+    payload = {
+        "indices": {
+            "train": indices.get("train", []),
+            "val": indices.get("val", []),
+            "test": indices.get("test", []),
+        }
+    }
+    if metadata:
+        payload.update(metadata)
+    save_json(out_path, payload)
 
 
 def save_split_indices(indices: Dict[str, List[int]], out_dir: str | Path) -> None:
diff --git a/tests/test_splitters.py b/tests/test_splitters.py
new file mode 100644
index 0000000000000000000000000000000000000000..6b9f9d180e7ef911444182dffb527489384271c5
--- /dev/null
+++ b/tests/test_splitters.py
@@ -0,0 +1,66 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pandas as pd
+import pytest
+
+from src.common.splitters import (
+    build_group_map,
+    group_split,
+    random_split,
+    scaffold_split,
+    validate_group_leakage,
+    validate_scaffold_split,
+    validate_split_indices,
+)
+
+
+def test_random_split_reproducible() -> None:
+    df = pd.DataFrame({"CAS": [f"X{i:02d}" for i in range(30)]})
+    first = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+    second = random_split(df, ratios=(0.7, 0.2, 0.1), seed=123)
+
+    assert first == second
+    validate_split_indices(first)
+
+
+def test_group_split_keeps_groups() -> None:
+    df = pd.DataFrame({"group": ["A", "A", "B", "B", "C", "C"]})
+    indices = group_split(df, group_col="group", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    group_map = build_group_map(df, group_col="group")
+    validate_group_leakage(indices, group_map, label="group")
+
+
+def test_scaffold_split_no_leakage(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+    from rdkit import Chem
+
+    def _write_sdf(path: Path, smiles: str) -> None:
+        mol = Chem.MolFromSmiles(smiles)
+        assert mol is not None
+        writer = Chem.SDWriter(str(path))
+        writer.write(mol)
+        writer.close()
+
+    sdf_dir = tmp_path / "sdf"
+    sdf_dir.mkdir()
+
+    cas_values = ["A", "B", "C", "D", "E", "F"]
+    smiles_list = [
+        "c1ccccc1",
+        "c1ccccc1O",
+        "C1CCCCC1",
+        "C1CCCCC1O",
+        "n1ccccc1",
+        "n1ccccc1O",
+    ]
+    for cas, smiles in zip(cas_values, smiles_list):
+        _write_sdf(sdf_dir / f"{cas}.sdf", smiles)
+
+    df = pd.DataFrame({"CAS": cas_values})
+    indices = scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", ratios=(0.34, 0.34, 0.32), seed=0)
+
+    validate_split_indices(indices)
+    validate_scaffold_split(df, sdf_dir=sdf_dir, cas_col="CAS", indices=indices)
diff --git a/work/queue.json b/work/queue.json
index d34b113530d0a33311e290c74d780578254cc1d9..06070498216eb5ed27dee45ee14006119512072c
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:27:29.824618Z",
+  "updated_at": "2025-12-26T14:40:26Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -144,7 +144,7 @@
     {
       "id": "035",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "split戦略の追加（random/scaffold/group、seed固定、漏洩防止）",
       "path": "work/tasks/035_split_strategies.md",
       "skills": [

tokens used
162,919
