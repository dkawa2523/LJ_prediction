OpenAI Codex v0.77.0 (research preview)
--------
workdir: /Users/kawahito/Desktop/LJ_prediction
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: xhigh
reasoning summaries: auto
session id: 019b5b31-3e17-7682-9ee4-16f5119ac74f
--------
user
# ===== SESSION CONTEXT =====
# Codex Session Context（毎回貼る短文）

- このrepoは「化学構造（SDF/CSV）→ 物性予測」を行う CLI/Hydra 基盤。
- 不変条件は docs/00_INVARIANTS.md が唯一の正。
- 今回の作業範囲・要件は work/tasks/<TASK>.md が唯一の正。
- 学習と推論の特徴量/前処理は必ず一致させる（skew禁止）。
- 成果物（artifact）は docs/04_ARTIFACTS_AND_VERSIONING.md を守る。
- 破壊的変更が必要なら work/rfc → docs/adr を先に作る（勝手に壊さない）。
- すべての処理は Process（処理単位）としてHydraで管理し、単独実行できる形を守る。
- 将来ClearML Task化を想定し、各Processが artifact を明確に出す設計にする（実装は今しない）。

# ===== AUTOPILOT MODE (non-interactive) =====
あなたは自動実行ループの中で動いています。**停止しないための規約**として次を厳守してください：

【禁止】
- ユーザーへの確認・質問・承認依頼（"確認してください" / "進めてよいですか" / "Approve" 等）は一切しない。
- "今回は未実装" / "差分なし" のまま終了しない（必ず前進する）。

【single source of truth】
- タスクの status の唯一の真実は work/queue.json。task.md の 'Blocked' 記述は参考情報。
  - queue が todo/in_progress なら実装を進める（task.md 側の Blocked を理由に止めない）。
  - queue が blocked なら blocked-handling を行う（reason/unblock_condition/next_action を整備し、解除子タスクを起票）。

【stale Blocked の扱い】
- task.md に '## Blocked' が残っていても queue が todo/in_progress の場合、それは **stale**。
  - task.md を更新し、Blocked を Notes/History に移す or 解除した上で実装を進める。

【書き込み権限】
- **書き込みは可能**な前提。work/queue.json 更新を含む必要な変更は実施する。
  - もし実際にツール/環境エラーで書き込めない場合：
    1) エラーメッセージを短く引用
    2) 最小の対処（sandbox/approval/flags 見直し、権限確認）を提示
    3) タスクを blocked にして解除子タスクを起票（子に unblocks を付与）
    ※それでも『確認してください』は書かない。

【必ずどれかを実施（Decision Protocol）】
A) 実装を進める → テスト/検証 → work/queue.json を done に更新
B) 進められない → work/queue.json を blocked に更新し、解除子タスク（unblocks付き）を作成
C) 依存/状態がズレている → work/queue.json と task.md を修正して整合させた上で A へ

# ===== SELECTED TASK =====
- id: 040
- title: 新しい目的変数（物性）追加の枠組み
- priority: P1
- status: in_progress
- path: work/tasks/040_add_new_property_task.md

# ===== CONTRACTS TO FOLLOW (open & read) =====
- docs/00_INVARIANTS.md
- docs/02_DATA_CONTRACTS.md

# ===== SKILLS TO FOLLOW (open & follow) =====
- agentskills/skills/S20_dataset_loader.md
- agentskills/skills/S60_trainer_loop.md
- agentskills/skills/S95_tests_ci.md


# ===== TASK FILE (single source of truth for *intent*) =====
# Task 040 (P1): 新しい目的変数（物性）追加の枠組み

## 目的
- CSV/SDF 形式は同じまま、目的変数だけを差し替えて学習できるようにする

## Plan
1) `configs/task/<property>.yaml` を追加できる形へ
2) `src/tasks/` に Task I/F を導入（loss/metrics/target_columns）
3) 既存 LJ タスクを Task 化し、後方互換を保つ
4) サンプルとして 1つ新規物性（ダミーでも可）を追加する

## Acceptance Criteria
- [ ] `task=<property>` で学習が走る
- [ ] metrics が出る

# ===== OUTPUT REQUIREMENTS (mandatory) =====
1) 変更計画（ファイル単位）
2) 実装（差分が分かるように）
3) 追加/更新したテスト
4) 検証コマンド
5) 互換性影響（config/CLI/artifact）
6) タスク完了時：work/queue.json の status を done に更新（満たせない場合は blocked と理由）


mcp: context7 starting
mcp: context7 ready
mcp startup: ready: context7

thinking
**Preparing to inspect skill-related files**
exec
/bin/zsh -lc ls in /Users/kawahito/Desktop/LJ_prediction succeeded in 279ms:
README.md
agentskills
codex
configs
data
docs
learning_curve_val.png
objective.md
parity_test.png
parity_val.png
requirements.txt
residual_test.png
residual_val.png
runs
scripts
src
tests
tools
work
y_train_hist.png

thinking
**Reviewing documentation files**
exec
/bin/zsh -lc "sed -n '1,200p' docs/00_INVARIANTS.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
# Invariants（不変条件）

このドキュメントは **開発中に変えてはいけない契約（Codexも遵守）** です。  
迷ったらここへ戻り、ここに書かれていない“勝手な最適化”をしないでください。

---

## 0. 基本目的（この基盤のゴール）
- **入力**: 化学構造（主に **CSV + SDF**。将来もこの形式を軸にする）
- **出力**: 物性（LJパラメータに限らず、**同じCSV/SDF形式のまま目的変数を差し替える**）
- **操作**: **CLI**（**Hydra**で設定管理）
- **要求**:
  - 前処理・特徴量化・学習・評価・推論・可視化を **拡張しやすい**
  - モデル/特徴量の手法が複数あり、**選択して実行・比較評価しやすい**

---

## 1. もっとも重要な不変条件（破るなら RFC→ADR→更新）

### 1.1 「処理単位（Process）」が第一級（最重要）
exec
/bin/zsh -lc "sed -n '1,200p' docs/02_DATA_CONTRACTS.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
# Data Contracts（データ契約）

このドキュメントは **CSV/SDF を中心としたデータ形式の共通契約** を定義します。  
目的変数（物性）が増えても、ここを守れば基盤が壊れないようにします。

## 1. 入力データの基本形（推奨）
### 1.1 CSV（テーブル）
- 1行 = 1分子（サンプル）
- 必須列（推奨名）
  - `sample_id`：一意ID（無ければ生成）
  - `cas`：CAS番号（ある場合）
  - `smiles`：SMILES（ある場合）
  - `formula`：分子式（ある場合）
  - `sdf_path`：SDF ファイルへのパス（行ごと or 共通）
  - `target.<name>`：目的変数（例 `target.lj_sigma`, `target.lj_epsilon`）
- 任意列
  - `meta.*`：データソース、温度条件、参考値など

※ 既存CSVの列名が異なる場合は `configs/dataset/*.yaml` でマッピングする。

exec
/bin/zsh -lc "sed -n '1,240p' work/tasks/040_add_new_property_task.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 284ms:
# Task 040 (P1): 新しい目的変数（物性）追加の枠組み

## 目的
- CSV/SDF 形式は同じまま、目的変数だけを差し替えて学習できるようにする

## Plan
1) `configs/task/<property>.yaml` を追加できる形へ
2) `src/tasks/` に Task I/F を導入（loss/metrics/target_columns）
3) 既存 LJ タスクを Task 化し、後方互換を保つ
4) サンプルとして 1つ新規物性（ダミーでも可）を追加する

## Acceptance Criteria
- [ ] `task=<property>` で学習が走る
- [ ] metrics が出る
exec
/bin/zsh -lc "sed -n '1,240p' agentskills/skills/S20_dataset_loader.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 260ms:
# S20 Dataset Loader

## Purpose
CSV/SDF 入力を共通表現へ変換するローダを追加/修正し、目的変数の差し替えに耐える基盤を作る。

## Inputs
- docs/02_DATA_CONTRACTS.md
- work/tasks/040_add_new_property_task.md（または該当）

## Allowed Changes
- src/data/**
- configs/dataset/**
- tests/**

## Steps
1) 現状の CSV/SDF の読み込み箇所を特定
2) column mapping を config で指定できるようにする
3) 欠損/不一致の扱いをログ化
4) unit test（最小CSV）を追加
exec
/bin/zsh -lc "sed -n '1,240p' agentskills/skills/S60_trainer_loop.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:
# S60 Trainer Loop

## Purpose
学習ループの拡張（scheduler、early stopping、AMP等）を、設定で切替可能にする。

## Inputs
- docs/03_CONFIG_CONVENTIONS.md
- work/tasks/010 / 020 / 030 など

## Allowed Changes
- src/**/train*.py or src/core/**
- configs/train/**
- tests/**

## Common Pitfalls
- デフォルト挙動が変わって精度が落ちる
- 再現性（seed）を壊す
exec
/bin/zsh -lc "sed -n '1,240p' agentskills/skills/S95_tests_ci.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 277ms:
# S95 Tests & CI

## Purpose
pytest + smoke + contract test を整備し、改修で壊れないようにする。

## Allowed Changes
- tests/**
- pyproject.toml（任意）
- .github/workflows/**（任意）

## Pitfalls
- smoke が重くなりすぎる
- 乱数により flaky になる

thinking
**Planning skill usage and inspection**
exec
/bin/zsh -lc 'rg -n "task" configs -g"*.yaml"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 293ms:
configs/audit_dataset_quick.yaml:4:  - task: lj_epsilon
configs/task/lj_epsilon.yaml:1:task:
configs/audit_dataset_fixture.yaml:4:  - task: lj_epsilon
configs/fp/train_quick.yaml:4:  - task: lj_epsilon
configs/gnn/train_quick.yaml:4:  - task: lj_epsilon
configs/audit_dataset.yaml:4:  - task: lj_epsilon
configs/fp/train_fixture.yaml:4:  - task: lj_epsilon
configs/dataset.yaml:4:  - task: lj_epsilon
configs/dataset_fixture.yaml:4:  - task: lj_epsilon
configs/fp/evaluate.yaml:4:  - task: lj_epsilon
configs/gnn/evaluate.yaml:4:  - task: lj_epsilon
configs/fp/evaluate_quick.yaml:4:  - task: lj_epsilon
configs/config_quick.yaml:4:  - task: lj_epsilon
configs/gnn/evaluate_quick.yaml:4:  - task: lj_epsilon
configs/dataset_quick.yaml:4:  - task: lj_epsilon
configs/gnn/train.yaml:4:  - task: lj_epsilon
configs/fp/evaluate_fixture.yaml:4:  - task: lj_epsilon
configs/leaderboard/default.yaml:8:    task_name: null
configs/fp/train.yaml:4:  - task: lj_epsilon
configs/config.yaml:4:  - task: lj_epsilon

thinking
**Inspecting lj_epsilon.yaml configuration**
exec
/bin/zsh -lc "sed -n '1,200p' configs/task/lj_epsilon.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 262ms:
task:
  name: "lj_epsilon"
  type: "regression"
  target_col: "lj_epsilon_over_k_K"

data:
  target_col: "lj_epsilon_over_k_K"

thinking
**Inspecting task usage in source**
exec
/bin/zsh -lc 'rg -n "target_col|task" src -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
src/data/audit.py:46:def _resolve_target_col(cfg: Dict[str, Any], cols: Dict[str, str]) -> Optional[str]:
src/data/audit.py:48:    if audit_cfg.get("target_col"):
src/data/audit.py:49:        return str(audit_cfg.get("target_col"))
src/data/audit.py:52:    task_cfg = cfg.get("task", {}) or {}
src/data/audit.py:53:    if task_cfg.get("target_col"):
src/data/audit.py:54:        return str(task_cfg.get("target_col"))
src/data/audit.py:56:    if data_cfg.get("target_col"):
src/data/audit.py:57:        return str(data_cfg.get("target_col"))
src/data/audit.py:139:    target_col = _resolve_target_col(cfg, cols)
src/data/audit.py:266:    if target_col and target_col in df.columns:
src/data/audit.py:267:        target_vals = pd.to_numeric(df[target_col], errors="coerce")
src/utils/validate_config.py:16:    task = cfg.get("task", {})
src/utils/validate_config.py:17:    return bool(data.get("target_col") or task.get("target_col"))
src/utils/validate_config.py:33:            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
src/fp/predict.py:81:            "task_name": train_context.get("task_name"),
src/utils/artifacts.py:21:    "task_name",
src/utils/artifacts.py:84:def resolve_task_name(cfg: Optional[Dict[str, Any]]) -> Optional[str]:
src/utils/artifacts.py:87:    task_cfg = cfg.get("task", {})
src/utils/artifacts.py:88:    if isinstance(task_cfg, dict) and task_cfg.get("name"):
src/utils/artifacts.py:89:        return str(task_cfg["name"])

thinking
**Checking for existing tasks**
exec
/bin/zsh -lc 'ls src' in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
__init__.py
__pycache__
common
data
fp
gnn
utils

thinking
**Preparing to create task directory**
exec
/bin/zsh -lc 'ls src/data' in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
__init__.py
__pycache__
audit.py

thinking
**Searching for dataset loader**
exec
/bin/zsh -lc 'rg -n "read_csv|CSV|sdf" src -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 289ms:
src/data/audit.py:10:from src.common.io import load_sdf_mol
src/data/audit.py:37:        "sdf_path": "sdf_path",
src/data/audit.py:68:        sdf_dir = input_cfg.get("sdf_dir") or cfg.get("paths", {}).get("sdf_dir")
src/data/audit.py:72:        return Path(raw_csv), Path(sdf_dir) if sdf_dir else None, Path(indices_dir) if indices_dir else None
src/data/audit.py:75:    sdf_dir = input_cfg.get("sdf_dir") or cfg.get("data", {}).get("sdf_dir") or cfg.get("paths", {}).get("sdf_dir")
src/data/audit.py:79:    return Path(dataset_csv), Path(sdf_dir) if sdf_dir else None, Path(indices_dir) if indices_dir else None
src/data/audit.py:131:    dataset_csv, sdf_dir, indices_dir = _resolve_input(cfg)
src/data/audit.py:134:    if sdf_dir is not None and not sdf_dir.exists():
src/data/audit.py:135:        raise FileNotFoundError(f"sdf_dir not found: {sdf_dir}")
src/data/audit.py:141:    df = pd.read_csv(dataset_csv)
src/data/audit.py:148:    sdf_path_col = cols.get("sdf_path")
src/data/audit.py:166:    sdf_path_values = _series_values(df, sdf_path_col)
src/data/audit.py:177:        if mol is None and sdf_path_values is not None:
src/data/audit.py:178:            raw_path = sdf_path_values.loc[idx]
src/data/audit.py:180:                mol = load_sdf_mol(str(raw_path))
src/data/audit.py:182:                    invalid_reason_counts["invalid_sdf"] += 1
src/data/audit.py:183:        if mol is None and sdf_dir is not None and cas_values is not None:
src/data/audit.py:186:                mol = load_sdf_mol(sdf_dir / f"{cas}.sdf")
src/data/audit.py:188:                    invalid_reason_counts["missing_sdf"] += 1
src/common/feature_pipeline.py:81:        sdf_dir: Path,

thinking
**Reviewing dataset loader and config needs**
exec
/bin/zsh -lc "sed -n '1,200p' docs/03_CONFIG_CONVENTIONS.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 237ms:
# Hydra Config Conventions（設定規約）

このドキュメントは Hydra を用いた設定管理の規約です。  
**Process（処理単位）をHydraで統一管理**し、手法の切替・比較評価をしやすくします。

---

## 1. 設定のゴール
- **各Processが単独で実行可能**
- モデル/特徴量/タスクの組合せを安全に切替可能
- 比較評価（multirun）しても成果物が追跡可能
- 将来ClearMLで Task 化しても、同じconfigを使い回せる

---

## 2. config group 構造（必須）
- `process/`：どのProcessを実行するか（脚本ごとに固定でもよいが、共通鍵は持つ）
- `dataset/`：入力データ・列マッピング・split・フィルタ
- `task/`：目的変数/タスクタイプ（regression/classification/multitask）
- `preprocess/`：正規化/3D生成/欠損処理

thinking
**Inspecting work/queue.json**
exec
/bin/zsh -lc 'cat work/queue.json' in /Users/kawahito/Desktop/LJ_prediction succeeded in 282ms:
{
  "version": 1,
  "updated_at": "2025-12-26T15:05:12.339184Z",
  "policy": {
    "priority_order": [
      "P0",
      "P1",
      "P2",
      "P3"
    ],
    "status_order": [
      "todo",
      "in_progress",
      "blocked",
      "done"
    ],
    "auto_set_in_progress_on_next": true
  },
  "tasks": [
    {

thinking
**Planning multi-step implementation**
exec
/bin/zsh -lc "sed -n '1,260p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.metrics import regression_metrics
from src.common.plots import save_parity_plot, save_residual_plot, save_hist
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json, set_seed
exec
/bin/zsh -lc "sed -n '260,620p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 262ms:
                "imputer": pipeline.imputer,
                "scaler": pipeline.scaler,
                "standardize": pipeline.standardize,
                "impute_strategy": pipeline.impute_strategy,
            },
            f,
        )
    save_json(
        model_dir / "featurizer_state.json",
        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
    )
    dump_yaml(run_dir / "config_snapshot.yaml", cfg)

    logger.info(f"Saved model to {model_path}")
    logger.info("Done.")
    return run_dir


def main() -> None:
    ap = argparse.ArgumentParser(description="Train fingerprint-based regression model for LJ parameter.")

thinking
**Checking evaluation with regression metrics**
exec
/bin/zsh -lc "sed -n '1,240p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.metrics import regression_metrics
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.feature_utils import hash_cfg
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config


thinking
**Adapting training and evaluation code**
exec
/bin/zsh -lc "sed -n '1,200p' src/common/metrics.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


def regression_metrics(y_true, y_pred) -> Dict[str, float]:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mse = float(mean_squared_error(y_true, y_pred))
    return {
        "mae": float(mean_absolute_error(y_true, y_pred)),
        "rmse": float(np.sqrt(mse)),
        "r2": float(r2_score(y_true, y_pred)),
    }

thinking
**Exploring GNN training code**
exec
/bin/zsh -lc "sed -n '1,220p' src/gnn/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 281ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.metrics import regression_metrics
from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json, set_seed
exec
/bin/zsh -lc "sed -n '220,520p' src/gnn/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
    else:
        raise ValueError(f"Unknown model.name: {model_name}")

    train_cfg = cfg.get("train", {})
    device = _select_device(train_cfg)
    num_threads = train_cfg.get("num_threads", None)
    if num_threads is not None:
        try:
            torch.set_num_threads(int(num_threads))
        except Exception:
            pass
    n_params = int(sum(p.numel() for p in model.parameters() if p.requires_grad))
    logger.info(
        f"Model params: {n_params/1e6:.2f}M (~{(n_params*4)/1e6:.1f} MB fp32, excluding optimizer state)"
    )
    logger.info(f"torch={torch.__version__} device={device} num_threads={torch.get_num_threads()}")
    model = model.to(device)

    epochs = int(train_cfg.get("epochs", 200))
    batch_size = int(train_cfg.get("batch_size", 64))
exec
/bin/zsh -lc "sed -n '1,240p' src/gnn/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 246ms:
from __future__ import annotations

import argparse
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.meta import build_meta, save_meta
from src.common.metrics import regression_metrics
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.gnn.models import GCNRegressor, MPNNRegressor
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

exec
/bin/zsh -lc "sed -n '240,520p' src/gnn/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
        save_json(run_dir / f"metrics_{split_name}.json", metrics)

    split_counts = pred_df["split"].value_counts().to_dict() if "split" in pred_df.columns else {}
    save_json(
        run_dir / "metrics.json",
        {
            "by_split": metrics_by_split,
            "n_train": int(split_counts.get("train", 0)),
            "n_val": int(split_counts.get("val", 0)),
            "n_test": int(split_counts.get("test", 0)),
        },
    )

    logger.info("Done.")
    return run_dir


if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="Evaluate GNN model and write predictions/metrics.")
    ap.add_argument("--config", required=True, help="Path to configs/gnn/evaluate.yaml")

thinking
**Planning tasks interface changes**
exec
/bin/zsh -lc "sed -n '1,200p' src/utils/validate_config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 294ms:
from __future__ import annotations

from typing import Any, Dict, Iterable

from src.common.config import ConfigError


def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
    missing = [k for k in keys if k not in cfg]
    if missing:
        raise ConfigError(f"{context} missing keys: {', '.join(missing)}")


def _has_target(cfg: Dict[str, Any]) -> bool:
    data = cfg.get("data", {})
    task = cfg.get("task", {})
    return bool(data.get("target_col") or task.get("target_col"))


def validate_config(cfg: Dict[str, Any]) -> None:

thinking
**Reviewing task interface updates**
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/train.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_desc
  - model: fp_lightgbm
  - train: fp_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"

thinking
**Examining train.yaml configuration**
exec
/bin/zsh -lc "sed -n '1,200p' configs/gnn/train.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: none
  - features: gnn_graph
  - model: gnn_mpnn
  - train: gnn_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "mpnn_desc_v1"

process:
  backend: "gnn"

output:
  run_dir: "runs/train/gnn"

thinking
**Checking evaluation configurations**
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/evaluate.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
defaults:
  - process: evaluate
  - dataset: default
  - task: lj_epsilon
  - eval: default
  - hydra: default
  - _self_

model_artifact_dir: "runs/train/fp/lgbm_morgan_desc_v1"

process:
  backend: "fp"

experiment:
  name: "fp_evaluate"

output:
  exp_name: "fp_evaluate"

thinking
**Checking target column configuration**
exec
/bin/zsh -lc "sed -n '1,200p' configs/dataset/default.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 262ms:
paths:
  raw_csv: "data/raw/tc_pc_tb_pubchem.csv"
  sdf_dir: "data/raw/sdf_files"
  out_csv: "data/processed/dataset_with_lj.csv"
  out_indices_dir: "data/processed/indices"

columns:
  cas: "CAS"
  formula: "MolecularFormula"
  tc: "Tc [K]"
  pc: "Pc [Pa]"
  tb: "Tb [K]"

seed: 42

lj:
  epsilon_method: "bird_critical"
  sigma_method: "bird_critical"
  epsilon_col: "lj_epsilon_over_k_K"
  sigma_col: "lj_sigma_A"

thinking
**Investigating dataset building process**
exec
/bin/zsh -lc 'rg -n "build_dataset" -S' in /Users/kawahito/Desktop/LJ_prediction succeeded in 296ms:
src/utils/validate_config.py:26:    if process_name == "build_dataset":
src/utils/validate_config.py:27:        _require_keys(cfg, ["paths", "columns", "split"], "build_dataset config")
scripts/build_dataset.py:12:# Allow running as `python scripts/build_dataset.py ...` without installing the package.
scripts/build_dataset.py:62:    logger = get_logger("build_dataset", log_file=out_csv.parent / "build_dataset.log")
docs/00_INVARIANTS.md:23:- 例: `build_dataset` / `preprocess` / `featurize` / `train` / `evaluate` / `predict` / `visualize` / `collect_data`
docs/10_PROCESS_CATALOG.md:11:| build_dataset | scripts/build_dataset.py | raw CSV/SDF | dataset index/split, dataset_hash | Dataset Task |
docs/10_PROCESS_CATALOG.md:27:- `train` は raw CSV/SDF を直接読むよりも、`build_dataset` の artifact を参照する（推奨）
docs/04_ARTIFACTS_AND_VERSIONING.md:24:### 3.1 build_dataset
docs/01_ARCHITECTURE.md:24:- **P01 build_dataset**  
README.md:43:  build_dataset.py
README.md:96:python3 scripts/build_dataset.py --config configs/dataset.yaml
README.md:98:# python3 scripts/build_dataset.py --config configs/dataset.yaml --limit 500
README.md:216:python scripts/build_dataset.py --config configs/dataset_fixture.yaml
work/REPO_ORIENTATION.md:22:  -> scripts/build_dataset.py
work/REPO_ORIENTATION.md:42:| scripts/build_dataset.py | configs/dataset*.yaml | data/raw/tc_pc_tb_pubchem.csv, data/raw/sdf_files | data/processed/dataset_with_lj.csv, data/processed/indices/*.txt, data/processed/dataset_config_snapshot.yaml, data/processed/build_dataset.log |
src/gnn/evaluate.py:60:def _build_dataset(
src/gnn/evaluate.py:150:    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
src/gnn/evaluate.py:151:    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
src/gnn/evaluate.py:152:    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
configs/process/build_dataset.yaml:2:  name: "build_dataset"

thinking
**Inspecting build_dataset file**
exec
/bin/zsh -lc "sed -n '1,260p' scripts/build_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

# Allow running as `python scripts/build_dataset.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import dump_yaml, load_config
from src.common.chemistry import elements_string, n_elements, parse_formula
from src.common.dataset_selectors import SelectorContext, apply_selectors
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas, write_csv

thinking
**Designing task interface and dataset integration**

I'm working on implementing a Task interface that includes loss, metrics, and target columns to support the user’s request to switch target variables and unify task handling. I want to integrate this with existing dataset loaders while keeping backward compatibility, adding new task specs and configs—like for the `lj_sigma` property—and checking how tasks are currently defined and tested to plan the best solution.
exec
/bin/zsh -lc 'rg -n "task" tests -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 306ms:
tests/test_audit_dataset.py:56:        "task": {"target_col": "lj_epsilon_over_k_K"},
tests/test_leaderboard.py:18:        "task_name": overrides.get("task_name", "lj_epsilon"),
tests/contract/test_artifacts_contract.py:36:        "task": {"name": "lj_epsilon"},
tests/contract/test_artifacts_contract.py:48:    assert meta["task_name"] == "lj_epsilon"

thinking
**Opening audit dataset tests**
exec
/bin/zsh -lc "sed -n '1,200p' tests/test_audit_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
from __future__ import annotations

from pathlib import Path

import numpy as np
import pandas as pd
import pytest

rdkit = pytest.importorskip("rdkit")
from rdkit import Chem

from src.data.audit import audit_dataset


def _write_sdf(path: Path, smiles: str) -> None:
    mol = Chem.MolFromSmiles(smiles)
    assert mol is not None
    writer = Chem.SDWriter(str(path))
    writer.write(mol)
    writer.close()

thinking
**Reviewing audit task target_col usage**
exec
/bin/zsh -lc "sed -n '1,220p' src/data/audit.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
from __future__ import annotations

from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.io import load_sdf_mol
from src.common.splitters import load_split_indices
from src.utils.artifacts import compute_dataset_hash

try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors
except Exception:  # pragma: no cover
    Chem = None
    Descriptors = None


thinking
**Reviewing task target column handling**
exec
/bin/zsh -lc "sed -n '1,260p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 263ms:
from __future__ import annotations

import csv
import hashlib
import json
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Optional
from uuid import uuid4

from src.common.utils import save_json

REQUIRED_META_KEYS = [
    "run_id",
    "process_name",
    "created_at",
    "git_sha",
    "dataset_hash",
    "config_hash",


thinking
**Designing unified task interface and config updates**

I'm mapping out a consistent Task interface with TaskSpec dataclass to unify loss, metrics, and target columns from config, ensuring backward compatibility with existing keys while allowing new config fields. I plan to implement task resolution logic that reads config in layers, supporting defaults and overrides, plus updating training, evaluation, and validation to use this TaskSpec cleanly without breaking existing workflows. This includes adding support for regression tasks now and designing extensibility for others later.
thinking
**Clarifying task spec usage in evaluation**

I’m considering using the training config as the source of truth for task specs during evaluation to keep metrics consistent, while allowing overrides if explicitly set, but leaning toward simplicity by primarily using the evaluation config which usually includes the task group and keeps things aligned. I’ll review prediction code to check if task handling needs updating too.
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 319ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Tuple

import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import morgan_bitvect
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config


thinking
**Reviewing testing and configuration needs**
exec
/bin/zsh -lc 'rg -n "validate_config" tests -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 285ms:


thinking
**Outlining comprehensive update and testing plan**

I’m putting together a detailed plan covering adding tests for target column resolution, updating relevant modules like audit and dataset selectors, modifying configs, and ensuring compatibility without network use. This plan will include multiple steps with testing and verification, and I’ll announce the skills applied in the final response as instructed.
Plan update
Plan for Task 040 implementation.
  ✓ Review current task/target handling in configs and training/evaluation/audit code to identify integration points for Task I/F.
  → Introduce src/tasks TaskSpec/resolution helpers and wire them into train/evaluate/audit/validate_config, keeping backward compatibility.
  • Add new task config for a second property and adjust existing task config; update tests for target resolution.
  • Update queue status and summarize changes with verification guidance.

thinking
**Designing and implementing the tasks module**

I'm mapping out the tasks module structure, focusing on clean separation in files like `registry.py` and `spec.py`. I'll define a `TaskSpec` dataclass with methods to resolve targets, metrics, and loss names from config dictionaries, carefully handling metric configurations including lists and defaults. Then I'll integrate `resolve_task` into training and evaluation code, ensuring it supplies target columns, metrics functions, and loss names consistently, while managing potential config errors gracefully. This also includes updating related parts like training scripts and evaluation configs to use the new task specification logic.
exec
/bin/zsh -lc "sed -n '1,200p' configs/train/gnn_default.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 283ms:
train:
  epochs: 10
  batch_size: 32
  lr: 0.01
  weight_decay: 1.0e-5
  loss: "huber"
  early_stopping:
    patience: 20
  seed: 42
  device: "mps"

thinking
**Designing task config defaults and precedence**

I'm sorting out how to handle `loss`, `metrics`, and `target_columns` defaults between task and training configs, aiming for backward compatibility by letting training loss override task loss, while tasks provide sensible defaults. I’m also planning updates to config files, code for resolving these settings, and tests to confirm the intended precedence and behavior.
exec
/bin/zsh -lc 'ls configs/task' in /Users/kawahito/Desktop/LJ_prediction succeeded in 302ms:
lj_epsilon.yaml

thinking
**Preparing task spec and config updates**

I'm planning to add new YAML config files for tasks and update existing ones to handle new target column settings with backward compatibility. I’ll create a `tasks` module including `spec.py` to normalize and resolve target columns from configs, ensuring fallback checks across task and data sections, aiming for smooth integration with existing training and testing setups.
exec
/bin/zsh -lc "sed -n '1,200p' tests/contract/test_artifacts_contract.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 298ms:
from __future__ import annotations

from pathlib import Path

from src.utils.artifacts import (
    REQUIRED_META_KEYS,
    build_meta,
    compute_config_hash,
    compute_dataset_hash,
    save_meta,
    validate_evaluate_artifacts,
    validate_predict_artifacts,
    validate_train_artifacts,
)


def _write_indices(indices_dir: Path) -> None:
    indices_dir.mkdir(parents=True, exist_ok=True)
    (indices_dir / "train.txt").write_text("0\n", encoding="utf-8")
    (indices_dir / "val.txt").write_text("0\n", encoding="utf-8")

thinking
**Preparing task module integration**
file update
A /Users/kawahito/Desktop/LJ_prediction/src/tasks/spec.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence

from src.common.metrics import regression_metrics

MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]


def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
    return {}


def _normalize_columns(value: Any) -> List[str]:
    if value is None:
        return []
    if isinstance(value, (list, tuple)):
        out: List[str] = []
        for item in value:
            if item is None:
                continue
            item_str = str(item).strip()
            if item_str:
                out.append(item_str)
        return out
    item_str = str(value).strip()
    return [item_str] if item_str else []


def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
    task_cfg = cfg.get("task", {}) or {}
    data_cfg = cfg.get("data", {}) or {}

    for key in ("target_columns", "target_cols"):
        cols = _normalize_columns(task_cfg.get(key))
        if cols:
            return cols
    cols = _normalize_columns(task_cfg.get("target_col"))
    if cols:
        return cols

    for key in ("target_columns", "target_cols"):
        cols = _normalize_columns(data_cfg.get(key))
        if cols:
            return cols
    return _normalize_columns(data_cfg.get("target_col"))


def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
    task_type = str(task_type).lower()
    base_fn = regression_metrics if task_type == "regression" else _empty_metrics

    if metrics_cfg is None:
        return base_fn

    if isinstance(metrics_cfg, str):
        key = metrics_cfg.lower()
        if key in {"regression", "default"}:
            return base_fn
        if key in {"none", "null", "off"}:
            return _empty_metrics
        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")

    if isinstance(metrics_cfg, Iterable):
        names = [str(item) for item in metrics_cfg]

        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
            metrics = base_fn(y_true, y_pred)
            return {k: v for k, v in metrics.items() if k in names}

        return _filtered

    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")


def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
    train_loss = train_cfg.get("loss")
    if train_loss is not None:
        return str(train_loss).lower()
    task_loss = task_cfg.get("loss")
    if task_loss is not None:
        return str(task_loss).lower()
    if str(task_type).lower() == "regression":
        return "mse"
    return "mse"


@dataclass(frozen=True)
class TaskSpec:
    name: str
    task_type: str
    target_columns: List[str]
    metrics_fn: MetricsFn
    loss_name: str

    def primary_target(self) -> Optional[str]:
        if not self.target_columns:
            return None
        return self.target_columns[0]


def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
    task_cfg = cfg.get("task", {}) or {}
    train_cfg = cfg.get("train", {}) or {}

    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
    task_type = str(task_cfg.get("type", "regression")).lower()
    target_columns = resolve_target_columns(cfg)
    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)

    return TaskSpec(
        name=name,
        task_type=task_type,
        target_columns=target_columns,
        metrics_fn=metrics_fn,
        loss_name=loss_name,
    )
apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
A src/tasks/spec.py
file update:
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )

file update:
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )


thinking
**Adding __init__.py file**
file update
A /Users/kawahito/Desktop/LJ_prediction/src/tasks/__init__.py
from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task

__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
apply_patch(auto_approved=true) exited 0 in 47ms:
Success. Updated the following files:
A src/tasks/__init__.py
file update:
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )

file update:
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )


thinking
**Modifying validate_config to use resolve_target_columns**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/utils/validate_config.py
@@ -5,2 +5,3 @@
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
@@ -14,5 +15,3 @@
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
apply_patch(auto_approved=true) exited 0 in 38ms:
Success. Updated the following files:
M src/utils/validate_config.py
file update:
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Updating audit.py to use resolve_target_columns**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/data/audit.py
@@ -12,2 +12,3 @@
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
@@ -51,8 +52,5 @@
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
apply_patch(auto_approved=true) exited 0 in 49ms:
Success. Updated the following files:
M src/data/audit.py
file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Updating train.py for task spec integration**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -16,3 +16,2 @@
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -23,2 +22,3 @@
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
@@ -36,4 +36,6 @@
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
@@ -148,4 +150,4 @@
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Updating evaluation code to use task spec**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/evaluate.py
@@ -13,3 +13,2 @@
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
@@ -17,2 +16,3 @@
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
@@ -41,4 +41,6 @@
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
@@ -126,3 +128,3 @@
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
apply_patch(auto_approved=true) exited 0 in 56ms:
Success. Updated the following files:
M src/fp/evaluate.py
file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Refining task specification usage**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/train.py
@@ -16,3 +16,2 @@
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
@@ -21,2 +20,3 @@
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
@@ -133,4 +133,6 @@
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
@@ -241,3 +243,3 @@
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
@@ -285,3 +287,3 @@
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
apply_patch(auto_approved=true) exited 0 in 44ms:
Success. Updated the following files:
M src/gnn/train.py
file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Updating evaluation script imports and logic**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/evaluate.py
@@ -13,3 +13,2 @@
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
@@ -17,2 +16,3 @@
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
@@ -106,4 +106,6 @@
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
@@ -223,3 +225,3 @@
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
apply_patch(auto_approved=true) exited 0 in 54ms:
Success. Updated the following files:
M src/gnn/evaluate.py
file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Updating config files with target columns and metrics**

I’m refining the `lj_epsilon.yaml` config to include both `target_columns` and `target_col` for compatibility, adding a metrics field but skipping loss to avoid confusion. Then I plan to create a similar `lj_sigma.yaml` config with its specific target column and patch both files accordingly.
file update
M /Users/kawahito/Desktop/LJ_prediction/configs/task/lj_epsilon.yaml
@@ -3,2 +3,5 @@
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
apply_patch(auto_approved=true) exited 0 in 55ms:
Success. Updated the following files:
M configs/task/lj_epsilon.yaml
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Adding new file for lj_sigma**
file update
A /Users/kawahito/Desktop/LJ_prediction/configs/task/lj_sigma.yaml
task:
  name: "lj_sigma"
  type: "regression"
  target_columns:
    - "lj_sigma_A"
  metrics: "regression"
  target_col: "lj_sigma_A"

data:
  target_col: "lj_sigma_A"
apply_patch(auto_approved=true) exited 0 in 74ms:
Success. Updated the following files:
A configs/task/lj_sigma.yaml
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:

file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:


thinking
**Adding tests for task resolution logic**

I’m planning to add a new test file to cover the tasks module, focusing on verifying target column resolution priorities, metrics filtering, and loss precedence behavior using sample configs and test functions. This will help ensure the task resolution works as expected with different inputs.
file update
A /Users/kawahito/Desktop/LJ_prediction/tests/test_tasks.py
from __future__ import annotations

from src.tasks import resolve_target_columns, resolve_task


def test_resolve_target_columns_priority() -> None:
    cfg = {
        "task": {"target_columns": ["a"], "target_col": "b"},
        "data": {"target_col": "c"},
    }
    assert resolve_target_columns(cfg) == ["a"]


def test_resolve_task_metrics_filtered() -> None:
    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
    task = resolve_task(cfg)
    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
    assert set(metrics.keys()) == {"mae"}


def test_resolve_task_loss_precedence() -> None:
    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
    task = resolve_task(cfg)
    assert task.loss_name == "huber"
apply_patch(auto_approved=true) exited 0 in 90ms:
Success. Updated the following files:
A tests/test_tasks.py
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"

file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..ceb7855ab5bf7d52aba24c87627a4cb11a3c12ce
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, Iterable):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Refining iterable type checks in metrics resolution**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/tasks/spec.py
@@ -3,3 +3,3 @@
 from dataclasses import dataclass
-from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
+from typing import Any, Callable, Dict, List, Optional, Sequence
 
@@ -64,3 +64,3 @@
 
-    if isinstance(metrics_cfg, Iterable):
+    if isinstance(metrics_cfg, (list, tuple)):
         names = [str(item) for item in metrics_cfg]
apply_patch(auto_approved=true) exited 0 in 88ms:
Success. Updated the following files:
M src/tasks/spec.py
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"

file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Updating plan step statuses**
Plan update
  ✓ Review current task/target handling in configs and training/evaluation/audit code to identify integration points for Task I/F.
  ✓ Introduce src/tasks TaskSpec/resolution helpers and wire them into train/evaluate/audit/validate_config, keeping backward compatibility.
  ✓ Add new task config for a second property and adjust existing task config; update tests for target resolution.
  → Update queue status and summarize changes with verification guidance.
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Verifying all instances of target_col usage**
exec
/bin/zsh -lc 'rg -n "target_col" src -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 308ms:
src/utils/validate_config.py:6:from src.tasks import resolve_target_columns
src/utils/validate_config.py:16:    return bool(resolve_target_columns(cfg))
src/utils/validate_config.py:32:            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
src/fp/evaluate.py:43:    target_col = task_spec.primary_target()
src/fp/evaluate.py:44:    if target_col is None:
src/fp/evaluate.py:114:        y = split_df[target_col].astype(float).to_numpy()
src/data/audit.py:13:from src.tasks import resolve_target_columns
src/data/audit.py:47:def _resolve_target_col(cfg: Dict[str, Any], cols: Dict[str, str]) -> Optional[str]:
src/data/audit.py:49:    if audit_cfg.get("target_col"):
src/data/audit.py:50:        return str(audit_cfg.get("target_col"))
src/data/audit.py:53:    task_targets = resolve_target_columns(cfg)
src/data/audit.py:137:    target_col = _resolve_target_col(cfg, cols)
src/data/audit.py:264:    if target_col and target_col in df.columns:
src/data/audit.py:265:        target_vals = pd.to_numeric(df[target_col], errors="coerce")
src/gnn/evaluate.py:66:    target_col: str,
src/gnn/evaluate.py:73:    for cas, y in zip(split_df[cas_col].astype(str).tolist(), split_df[target_col].astype(float).tolist()):
src/gnn/evaluate.py:108:    target_col = task_spec.primary_target()
src/gnn/evaluate.py:109:    if target_col is None:
src/gnn/evaluate.py:152:    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
src/gnn/evaluate.py:153:    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..5f27d98a568c3515ebb793bae0490c51942702a1
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Updating error message in validation**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/utils/validate_config.py
@@ -31,3 +31,3 @@
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
apply_patch(auto_approved=true) exited 0 in 66ms:
Success. Updated the following files:
M src/utils/validate_config.py
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"

file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Verifying imports and testing tasks**
exec
/bin/zsh -lc 'pytest -q tests/test_tasks.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 3.11s:
...                                                                      [100%]
3 passed in 2.49s
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Updating task status in JSON**
exec
/bin/zsh -lc "python - <<'PY'
import json
from datetime import datetime, timezone
from pathlib import Path

path = Path('work/queue.json')
data = json.loads(path.read_text())
now = datetime.now(timezone.utc).isoformat().replace('+00:00','Z')
data['updated_at'] = now
for task in data.get('tasks', []):
    if task.get('id') == '040':
        task['status'] = 'done'
        break
path.write_text(json.dumps(data, indent=2, ensure_ascii=False) + '\\n')
PY" in /Users/kawahito/Desktop/LJ_prediction succeeded in 317ms:

file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Confirming JSON ASCII encoding**
Plan update
  ✓ Review current task/target handling in configs and training/evaluation/audit code to identify integration points for Task I/F.
  ✓ Introduce src/tasks TaskSpec/resolution helpers and wire them into train/evaluate/audit/validate_config, keeping backward compatibility.
  ✓ Add new task config for a second property and adjust existing task config; update tests for target resolution.
  ✓ Update queue status and summarize changes with verification guidance.
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Preparing to capture code changes**
exec
/bin/zsh -lc 'git status -sb' in /Users/kawahito/Desktop/LJ_prediction succeeded in 282ms:
## test_chemkit
 M README.md
 M configs/dataset.yaml
 M configs/dataset_quick.yaml
 M configs/fp/predict.yaml
 M configs/fp/predict_quick.yaml
 M configs/fp/train.yaml
 M configs/fp/train_quick.yaml
 M configs/gnn/predict.yaml
 M configs/gnn/train.yaml
 M configs/gnn/train_mpnn_quick.yaml
 M configs/gnn/train_quick.yaml
 M objective.md
 M scripts/build_dataset.py
 D scripts/fp_predict.py
 D scripts/fp_train.py
 D scripts/gnn_predict.py
 D scripts/gnn_train.py
 M src/common/config.py
 M src/common/splitters.py
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Summarizing task interface updates and integration plan**

There's a lot going on here as I clarify how the new task specification handles target columns, metrics, and loss configuration—especially managing precedence between task.loss and train.loss without breaking existing defaults. I’m planning to wire this cleanly into train, evaluate, and audit code, add relevant configs like lj_sigma, update tests, and produce a detailed final response including change plans, diff snippets, verification commands, compatibility notes, and queue status. I'll also mention the skills used and confirm tests passed to fully satisfy the user’s instructions.
exec
/bin/zsh -lc "sed -n '1,200p' src/tasks/spec.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 288ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Sequence

from src.common.metrics import regression_metrics

MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]


def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
    return {}


def _normalize_columns(value: Any) -> List[str]:
    if value is None:
        return []
    if isinstance(value, (list, tuple)):
        out: List[str] = []
        for item in value:
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"


thinking
**Preparing detailed change summary**

I'm organizing a structured final message outlining the task resolution changes, including a clear change plan, implementation diffs with code snippets, test results and commands, compatibility notes, queue updates, and next steps—all formatted in line with the user's requirements and expectations.
codex
TaskSpecベースでターゲット/メトリクス/ロス解決を導入し、train/evaluate/auditに反映、lj_sigmaタスクとテストを追加しました。  
Using skills S20_dataset_loader (target column resolution/audit), S60_trainer_loop (loss/metrics wiring), S95_tests_ci (tests).

**変更計画**
- `src/tasks/spec.py`, `src/tasks/__init__.py`: Task I/F（target_columns/metrics/loss）追加
- `src/fp/train.py`, `src/fp/evaluate.py`, `src/gnn/train.py`, `src/gnn/evaluate.py`, `src/data/audit.py`, `src/utils/validate_config.py`: TaskSpec 利用へ切替
- `configs/task/lj_epsilon.yaml`, `configs/task/lj_sigma.yaml`: タスク定義更新/追加
- `tests/test_tasks.py`: 解決ロジックのユニットテスト追加
- `work/queue.json`: task 040 を done に更新

**実装（差分が分かるように）**
- `src/tasks/spec.py`:
```diff
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    ...
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    ...
+@dataclass(frozen=True)
+class TaskSpec:
+    ...
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    ...
```
- `src/fp/train.py` / `src/gnn/train.py`（evaluate も同様）:
```diff
+from src.tasks import resolve_task
-    target_col = str(...)
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
-    metrics_val = regression_metrics(...)
+    metrics_val = task_spec.metrics_fn(...)
```
- `configs/task/lj_sigma.yaml`:
```diff
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+data:
+  target_col: "lj_sigma_A"
```

**追加/更新したテスト**
- `tests/test_tasks.py`（resolve_target_columns priority / metrics filtering / loss precedence）
- 実行済み: `pytest -q tests/test_tasks.py`（3 passed）

**検証コマンド**
- `pytest -q tests/test_tasks.py`

**互換性影響（config/CLI/artifact）**
- config: `task.target_columns`/`task.metrics` を新規対応、既存 `task.target_col`/`data.target_col` は継続サポート
- CLI: 変更なし、`task=lj_sigma` が追加で利用可能
- artifact: `metrics.json`/`meta.json` 形式は変更なし（metrics内容は `task.metrics` に準拠）

**キュー更新**
- `work/queue.json` の task `040` を `done` に更新済み

**次のステップ**
1) `python scripts/train.py --config configs/fp/train_quick.yaml task=lj_sigma`
2) `python scripts/train.py --config configs/gnn/train_quick.yaml task=lj_sigma`
3) `python scripts/evaluate.py --config configs/fp/evaluate_quick.yaml task=lj_sigma`
file update:
diff --git a/configs/task/lj_epsilon.yaml b/configs/task/lj_epsilon.yaml
index 39ba60ae695adc9967da6c166fa28e0f7f874f23..4434068babf30c18c6c4bcaec08c13f412fffe8c
--- a/configs/task/lj_epsilon.yaml
+++ b/configs/task/lj_epsilon.yaml
@@ -1,6 +1,9 @@
 task:
   name: "lj_epsilon"
   type: "regression"
+  target_columns:
+    - "lj_epsilon_over_k_K"
+  metrics: "regression"
   target_col: "lj_epsilon_over_k_K"
 
 data:
diff --git a/configs/task/lj_sigma.yaml b/configs/task/lj_sigma.yaml
new file mode 100644
index 0000000000000000000000000000000000000000..230417511afab51421f558a8caeb764da5940e36
--- /dev/null
+++ b/configs/task/lj_sigma.yaml
@@ -0,0 +1,10 @@
+task:
+  name: "lj_sigma"
+  type: "regression"
+  target_columns:
+    - "lj_sigma_A"
+  metrics: "regression"
+  target_col: "lj_sigma_A"
+
+data:
+  target_col: "lj_sigma_A"
diff --git a/src/data/audit.py b/src/data/audit.py
index 68d4c6eedf16b6ca2c975532c1e3dfe045a49fa3..a584036dbf5162d4fa2729a0bf16f75db2f13a14
--- a/src/data/audit.py
+++ b/src/data/audit.py
@@ -10,6 +10,7 @@
 from src.common.io import load_sdf_mol
 from src.common.splitters import load_split_indices
 from src.utils.artifacts import compute_dataset_hash
+from src.tasks import resolve_target_columns
 
 try:
     from rdkit import Chem
@@ -49,12 +50,9 @@
         return str(audit_cfg.get("target_col"))
     if cols.get("target"):
         return str(cols["target"])
-    task_cfg = cfg.get("task", {}) or {}
-    if task_cfg.get("target_col"):
-        return str(task_cfg.get("target_col"))
-    data_cfg = cfg.get("data", {}) or {}
-    if data_cfg.get("target_col"):
-        return str(data_cfg.get("target_col"))
+    task_targets = resolve_target_columns(cfg)
+    if task_targets:
+        return str(task_targets[0])
     return None
 
 
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bcebaa06140566f5d91001e8b48623f79c68880f..bd8969cf185bf08f06b49a587399c76fdfa8c329
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.config import dump_yaml, load_config
 from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.feature_utils import hash_cfg
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -39,8 +39,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -124,7 +126,7 @@
         X = pipeline.transform_features(X)
         preds = model.predict(X)
 
-        metrics_by_split[split_name] = regression_metrics(y, preds)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y, preds)
         for cid, yt, yp in zip(ids, y.tolist(), preds.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/fp/train.py b/src/fp/train.py
index 8f2bf5060456293c53efc97b7c3547e23262e7b7..e37462352d5ad407faf7f580770acc6344e49e37
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -14,13 +14,13 @@
 from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.fp.feature_utils import hash_cfg
 from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -34,8 +34,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
@@ -146,8 +148,8 @@
     pred_val = model.predict(X_val)
     pred_test = model.predict(X_test)
 
-    metrics_val = regression_metrics(y_val, pred_val)
-    metrics_test = regression_metrics(y_test, pred_test)
+    metrics_val = task_spec.metrics_fn(y_val, pred_val)
+    metrics_test = task_spec.metrics_fn(y_test, pred_test)
     logger.info(f"Val metrics: {metrics_val}")
     logger.info(f"Test metrics: {metrics_test}")
 
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 83baba26b1584063e7de2d408d75b281a1387640..c680551fc50222a1202a3e97ac1b718bf0bb4ab1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -11,10 +11,10 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
-from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -104,8 +104,10 @@
     sdf_dir = Path(data_override.get("sdf_dir", data_cfg.get("sdf_dir", "data/raw/sdf_files")))
     cas_col = str(data_override.get("cas_col", data_cfg.get("cas_col", "CAS")))
 
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_override.get("target_col", task_cfg.get("target_col", data_cfg.get("target_col", "lj_epsilon_over_k_K"))))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
 
     out_cfg = cfg.get("output", {})
     run_dir_root = Path(out_cfg.get("run_dir", "runs/evaluate"))
@@ -221,7 +223,7 @@
         if len(y_true) == 0:
             metrics_by_split[split_name] = {}
             continue
-        metrics_by_split[split_name] = regression_metrics(y_true, y_pred)
+        metrics_by_split[split_name] = task_spec.metrics_fn(y_true, y_pred)
         for cid, yt, yp in zip(ids, y_true.tolist(), y_pred.tolist()):
             rows.append({"sample_id": cid, "y_true": yt, "y_pred": yp, "split": split_name})
 
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f..4b1d8f021dad493d273b055e368225fb200bb7d6
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -14,11 +14,11 @@
 from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
-from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
 from src.gnn.models import GCNRegressor, MPNNRegressor
+from src.tasks import resolve_task
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
 
@@ -131,8 +131,10 @@
     dataset_csv = Path(data_cfg.get("dataset_csv", "data/processed/dataset_with_lj.csv"))
     indices_dir = Path(data_cfg.get("indices_dir", "data/processed/indices"))
     sdf_dir = Path(data_cfg.get("sdf_dir", "data/raw/sdf_files"))
-    task_cfg = cfg.get("task", {})
-    target_col = str(data_cfg.get("target_col", task_cfg.get("target_col", "lj_epsilon_over_k_K")))
+    task_spec = resolve_task(cfg)
+    target_col = task_spec.primary_target()
+    if target_col is None:
+        raise ValueError("No target column resolved from task/data config.")
     cas_col = str(data_cfg.get("cas_col", "CAS"))
 
     out_cfg = cfg.get("output", {})
@@ -239,7 +241,7 @@
     batch_size = int(train_cfg.get("batch_size", 64))
     lr = float(train_cfg.get("lr", 1e-3))
     weight_decay = float(train_cfg.get("weight_decay", 1e-5))
-    loss_name = str(train_cfg.get("loss", "mse")).lower()
+    loss_name = task_spec.loss_name
 
     if loss_name == "mse":
         criterion = nn.MSELoss()
@@ -283,7 +285,7 @@
             return {}, np.array([]), np.array([])
         y_true = np.concatenate(ys)
         y_pred = np.concatenate(ps)
-        return regression_metrics(y_true, y_pred), y_true, y_pred
+        return task_spec.metrics_fn(y_true, y_pred), y_true, y_pred
 
     log_interval_sec = float(train_cfg.get("log_interval_sec", 30.0))
     show_pbar = bool(train_cfg.get("progress_bar", True))
diff --git a/src/tasks/__init__.py b/src/tasks/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..33f9e39b0e0cb84bc4ed9f29b989de3ce782ab1c
--- /dev/null
+++ b/src/tasks/__init__.py
@@ -0,0 +1,3 @@
+from src.tasks.spec import TaskSpec, resolve_target_columns, resolve_task
+
+__all__ = ["TaskSpec", "resolve_target_columns", "resolve_task"]
diff --git a/src/tasks/spec.py b/src/tasks/spec.py
new file mode 100644
index 0000000000000000000000000000000000000000..165a65f7e13a97012d2f857ddd07d45b6ca442ad
--- /dev/null
+++ b/src/tasks/spec.py
@@ -0,0 +1,119 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Callable, Dict, List, Optional, Sequence
+
+from src.common.metrics import regression_metrics
+
+MetricsFn = Callable[[Sequence[float], Sequence[float]], Dict[str, float]]
+
+
+def _empty_metrics(_: Sequence[float], __: Sequence[float]) -> Dict[str, float]:
+    return {}
+
+
+def _normalize_columns(value: Any) -> List[str]:
+    if value is None:
+        return []
+    if isinstance(value, (list, tuple)):
+        out: List[str] = []
+        for item in value:
+            if item is None:
+                continue
+            item_str = str(item).strip()
+            if item_str:
+                out.append(item_str)
+        return out
+    item_str = str(value).strip()
+    return [item_str] if item_str else []
+
+
+def resolve_target_columns(cfg: Dict[str, Any]) -> List[str]:
+    task_cfg = cfg.get("task", {}) or {}
+    data_cfg = cfg.get("data", {}) or {}
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(task_cfg.get(key))
+        if cols:
+            return cols
+    cols = _normalize_columns(task_cfg.get("target_col"))
+    if cols:
+        return cols
+
+    for key in ("target_columns", "target_cols"):
+        cols = _normalize_columns(data_cfg.get(key))
+        if cols:
+            return cols
+    return _normalize_columns(data_cfg.get("target_col"))
+
+
+def _resolve_metrics_fn(metrics_cfg: Any, task_type: str) -> MetricsFn:
+    task_type = str(task_type).lower()
+    base_fn = regression_metrics if task_type == "regression" else _empty_metrics
+
+    if metrics_cfg is None:
+        return base_fn
+
+    if isinstance(metrics_cfg, str):
+        key = metrics_cfg.lower()
+        if key in {"regression", "default"}:
+            return base_fn
+        if key in {"none", "null", "off"}:
+            return _empty_metrics
+        raise ValueError(f"Unknown task.metrics: {metrics_cfg}")
+
+    if isinstance(metrics_cfg, (list, tuple)):
+        names = [str(item) for item in metrics_cfg]
+
+        def _filtered(y_true: Sequence[float], y_pred: Sequence[float]) -> Dict[str, float]:
+            metrics = base_fn(y_true, y_pred)
+            return {k: v for k, v in metrics.items() if k in names}
+
+        return _filtered
+
+    raise ValueError(f"Unsupported task.metrics type: {type(metrics_cfg).__name__}")
+
+
+def _resolve_loss_name(task_cfg: Dict[str, Any], train_cfg: Dict[str, Any], task_type: str) -> str:
+    train_loss = train_cfg.get("loss")
+    if train_loss is not None:
+        return str(train_loss).lower()
+    task_loss = task_cfg.get("loss")
+    if task_loss is not None:
+        return str(task_loss).lower()
+    if str(task_type).lower() == "regression":
+        return "mse"
+    return "mse"
+
+
+@dataclass(frozen=True)
+class TaskSpec:
+    name: str
+    task_type: str
+    target_columns: List[str]
+    metrics_fn: MetricsFn
+    loss_name: str
+
+    def primary_target(self) -> Optional[str]:
+        if not self.target_columns:
+            return None
+        return self.target_columns[0]
+
+
+def resolve_task(cfg: Dict[str, Any]) -> TaskSpec:
+    task_cfg = cfg.get("task", {}) or {}
+    train_cfg = cfg.get("train", {}) or {}
+
+    name = str(task_cfg.get("name") or cfg.get("task_name") or "task")
+    task_type = str(task_cfg.get("type", "regression")).lower()
+    target_columns = resolve_target_columns(cfg)
+    metrics_fn = _resolve_metrics_fn(task_cfg.get("metrics"), task_type)
+    loss_name = _resolve_loss_name(task_cfg, train_cfg, task_type)
+
+    return TaskSpec(
+        name=name,
+        task_type=task_type,
+        target_columns=target_columns,
+        metrics_fn=metrics_fn,
+        loss_name=loss_name,
+    )
diff --git a/src/utils/validate_config.py b/src/utils/validate_config.py
index 54a8f28b3cce70a21b306155a09be31971c3bfee..053ec999f8adf4053873f3530760c8b282109ecb
--- a/src/utils/validate_config.py
+++ b/src/utils/validate_config.py
@@ -3,6 +3,7 @@
 from typing import Any, Dict, Iterable
 
 from src.common.config import ConfigError
+from src.tasks import resolve_target_columns
 
 
 def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
@@ -12,9 +13,7 @@
 
 
 def _has_target(cfg: Dict[str, Any]) -> bool:
-    data = cfg.get("data", {})
-    task = cfg.get("task", {})
-    return bool(data.get("target_col") or task.get("target_col"))
+    return bool(resolve_target_columns(cfg))
 
 
 def validate_config(cfg: Dict[str, Any]) -> None:
@@ -30,7 +29,7 @@
     if process_name in {"fp_train", "gnn_train", "train"}:
         _require_keys(cfg, ["data", "model", "train", "output"], f"{process_name} config")
         if not _has_target(cfg):
-            raise ConfigError(f"{process_name} requires data.target_col or task.target_col")
+            raise ConfigError(f"{process_name} requires task.target_columns/target_col or data.target_columns/target_col")
         return
 
     if process_name in {"fp_predict", "gnn_predict", "predict"}:
diff --git a/tests/test_tasks.py b/tests/test_tasks.py
new file mode 100644
index 0000000000000000000000000000000000000000..93fbd1cf821e20c3109281fec91359a817cd4f59
--- /dev/null
+++ b/tests/test_tasks.py
@@ -0,0 +1,24 @@
+from __future__ import annotations
+
+from src.tasks import resolve_target_columns, resolve_task
+
+
+def test_resolve_target_columns_priority() -> None:
+    cfg = {
+        "task": {"target_columns": ["a"], "target_col": "b"},
+        "data": {"target_col": "c"},
+    }
+    assert resolve_target_columns(cfg) == ["a"]
+
+
+def test_resolve_task_metrics_filtered() -> None:
+    cfg = {"task": {"target_columns": ["y"], "metrics": ["mae"]}}
+    task = resolve_task(cfg)
+    metrics = task.metrics_fn([1.0, 2.0], [1.0, 3.0])
+    assert set(metrics.keys()) == {"mae"}
+
+
+def test_resolve_task_loss_precedence() -> None:
+    cfg = {"task": {"target_columns": ["y"], "loss": "mse"}, "train": {"loss": "huber"}}
+    task = resolve_task(cfg)
+    assert task.loss_name == "huber"

tokens used
178,650
