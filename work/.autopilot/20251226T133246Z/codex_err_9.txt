OpenAI Codex v0.77.0 (research preview)
--------
workdir: /Users/kawahito/Desktop/LJ_prediction
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: xhigh
reasoning summaries: auto
session id: 019b5b5c-f30f-7161-8351-184e122f4f86
--------
user
# ===== SESSION CONTEXT =====
# Codex Session Context（毎回貼る短文）

- このrepoは「化学構造（SDF/CSV）→ 物性予測」を行う CLI/Hydra 基盤。
- 不変条件は docs/00_INVARIANTS.md が唯一の正。
- 今回の作業範囲・要件は work/tasks/<TASK>.md が唯一の正。
- 学習と推論の特徴量/前処理は必ず一致させる（skew禁止）。
- 成果物（artifact）は docs/04_ARTIFACTS_AND_VERSIONING.md を守る。
- 破壊的変更が必要なら work/rfc → docs/adr を先に作る（勝手に壊さない）。
- すべての処理は Process（処理単位）としてHydraで管理し、単独実行できる形を守る。
- 将来ClearML Task化を想定し、各Processが artifact を明確に出す設計にする（実装は今しない）。

# ===== AUTOPILOT MODE (non-interactive) =====
あなたは自動実行ループの中で動いています。**停止しないための規約**として次を厳守してください：

【禁止】
- ユーザーへの確認・質問・承認依頼（"確認してください" / "進めてよいですか" / "Approve" 等）は一切しない。
- "今回は未実装" / "差分なし" のまま終了しない（必ず前進する）。

【single source of truth】
- タスクの status の唯一の真実は work/queue.json。task.md の 'Blocked' 記述は参考情報。
  - queue が todo/in_progress なら実装を進める（task.md 側の Blocked を理由に止めない）。
  - queue が blocked なら blocked-handling を行う（reason/unblock_condition/next_action を整備し、解除子タスクを起票）。

【stale Blocked の扱い】
- task.md に '## Blocked' が残っていても queue が todo/in_progress の場合、それは **stale**。
  - task.md を更新し、Blocked を Notes/History に移す or 解除した上で実装を進める。

【書き込み権限】
- **書き込みは可能**な前提。work/queue.json 更新を含む必要な変更は実施する。
  - もし実際にツール/環境エラーで書き込めない場合：
    1) エラーメッセージを短く引用
    2) 最小の対処（sandbox/approval/flags 見直し、権限確認）を提示
    3) タスクを blocked にして解除子タスクを起票（子に unblocks を付与）
    ※それでも『確認してください』は書かない。

【必ずどれかを実施（Decision Protocol）】
A) 実装を進める → テスト/検証 → work/queue.json を done に更新
B) 進められない → work/queue.json を blocked に更新し、解除子タスク（unblocks付き）を作成
C) 依存/状態がズレている → work/queue.json と task.md を修正して整合させた上で A へ

# ===== SELECTED TASK =====
- id: 070
- title: 事前学習埋め込み特徴量の導入（任意）
- priority: P2
- status: in_progress
- path: work/tasks/070_add_pretrained_embeddings.md

# ===== CONTRACTS TO FOLLOW (open & read) =====
- docs/00_INVARIANTS.md

# ===== SKILLS TO FOLLOW (open & follow) =====
- agentskills/skills/S40_featurizer.md


# ===== TASK FILE (single source of truth for *intent*) =====
# Task 070 (P2): 事前学習埋め込み特徴量の導入（任意）

## 目的
- 1万件規模でも表現力を上げる手段として、公開済み分子埋め込みを利用可能にする

## Plan
1) featurizer として `pretrained_embedding` を追加
2) まずは “外部モデルを呼ばないスタブ” で I/F を確定
3) 次に HuggingFace などの実装を追加（依存追加は最小限）

## Acceptance Criteria
- [ ] featureset で切替可能
- [ ] 推論時も同じ埋め込みが再現できる

# ===== OUTPUT REQUIREMENTS (mandatory) =====
1) 変更計画（ファイル単位）
2) 実装（差分が分かるように）
3) 追加/更新したテスト
4) 検証コマンド
5) 互換性影響（config/CLI/artifact）
6) タスク完了時：work/queue.json の status を done に更新（満たせない場合は blocked と理由）


mcp: context7 starting
mcp: context7 ready
mcp startup: ready: context7

thinking
**Preparing to review documentation and skills**
exec
/bin/zsh -lc ls in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
README.md
agentskills
codex
configs
data
docs
learning_curve_val.png
objective.md
parity_test.png
parity_val.png
requirements.txt
residual_test.png
residual_val.png
runs
scripts
src
tests
tools
work
y_train_hist.png

thinking
**Reviewing invariants**
exec
/bin/zsh -lc 'cat docs/00_INVARIANTS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
# Invariants（不変条件）

このドキュメントは **開発中に変えてはいけない契約（Codexも遵守）** です。  
迷ったらここへ戻り、ここに書かれていない“勝手な最適化”をしないでください。

---

## 0. 基本目的（この基盤のゴール）
- **入力**: 化学構造（主に **CSV + SDF**。将来もこの形式を軸にする）
- **出力**: 物性（LJパラメータに限らず、**同じCSV/SDF形式のまま目的変数を差し替える**）
- **操作**: **CLI**（**Hydra**で設定管理）
- **要求**:
  - 前処理・特徴量化・学習・評価・推論・可視化を **拡張しやすい**
  - モデル/特徴量の手法が複数あり、**選択して実行・比較評価しやすい**

---

## 1. もっとも重要な不変条件（破るなら RFC→ADR→更新）

### 1.1 「処理単位（Process）」が第一級（最重要）

thinking
**Opening feature skill documentation**
exec
/bin/zsh -lc 'cat agentskills/skills/S40_featurizer.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 260ms:
# S40 Featurizer

## Purpose
特徴量（FP/記述子/3D/埋め込み）を追加し、設定で切替できるようにする。

## Inputs
- docs/02_DATA_CONTRACTS.md
- docs/03_CONFIG_CONVENTIONS.md
- work/tasks/030_featurepipeline_unify.md または NEW_FEATURIZER タスク

## Allowed Changes
- src/common/**（FeaturePipeline）
- src/fp/**, src/gnn/**（必要なら）
- configs/features/**
- tests/**

## Steps
1) featurizer I/F を確認（fit/transform or transformのみ）
2) 学習で fit した状態を artifact 保存
3) 推論で load して transform

thinking
**Checking relevant documentation files**
exec
/bin/zsh -lc 'cat work/tasks/070_add_pretrained_embeddings.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
# Task 070 (P2): 事前学習埋め込み特徴量の導入（任意）

## 目的
- 1万件規模でも表現力を上げる手段として、公開済み分子埋め込みを利用可能にする

## Plan
1) featurizer として `pretrained_embedding` を追加
2) まずは “外部モデルを呼ばないスタブ” で I/F を確定
3) 次に HuggingFace などの実装を追加（依存追加は最小限）

## Acceptance Criteria
- [ ] featureset で切替可能
- [ ] 推論時も同じ埋め込みが再現できる

thinking
**Opening additional documentation files**
exec
/bin/zsh -lc 'cat docs/02_DATA_CONTRACTS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
# Data Contracts（データ契約）

このドキュメントは **CSV/SDF を中心としたデータ形式の共通契約** を定義します。  
目的変数（物性）が増えても、ここを守れば基盤が壊れないようにします。

## 1. 入力データの基本形（推奨）
### 1.1 CSV（テーブル）
- 1行 = 1分子（サンプル）
- 必須列（推奨名）
  - `sample_id`：一意ID（無ければ生成）
  - `cas`：CAS番号（ある場合）
  - `smiles`：SMILES（ある場合）
  - `formula`：分子式（ある場合）
  - `sdf_path`：SDF ファイルへのパス（行ごと or 共通）
  - `target.<name>`：目的変数（例 `target.lj_sigma`, `target.lj_epsilon`）
- 任意列
  - `meta.*`：データソース、温度条件、参考値など

※ 既存CSVの列名が異なる場合は `configs/dataset/*.yaml` でマッピングする。


thinking
**Identifying configuration needs**
exec
/bin/zsh -lc 'cat docs/03_CONFIG_CONVENTIONS.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
# Hydra Config Conventions（設定規約）

このドキュメントは Hydra を用いた設定管理の規約です。  
**Process（処理単位）をHydraで統一管理**し、手法の切替・比較評価をしやすくします。

---

## 1. 設定のゴール
- **各Processが単独で実行可能**
- モデル/特徴量/タスクの組合せを安全に切替可能
- 比較評価（multirun）しても成果物が追跡可能
- 将来ClearMLで Task 化しても、同じconfigを使い回せる

---

## 2. config group 構造（必須）
- `process/`：どのProcessを実行するか（脚本ごとに固定でもよいが、共通鍵は持つ）
- `dataset/`：入力データ・列マッピング・split・フィルタ
- `task/`：目的変数/タスクタイプ（regression/classification/multitask）
- `preprocess/`：正規化/3D生成/欠損処理

thinking
**Noting need for docs/04**
exec
/bin/zsh -lc 'cat docs/04_ARTIFACTS_AND_VERSIONING.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
# Artifacts & Versioning（成果物・再現性）

このドキュメントは「各Processが独立して実行でき、比較・再現できる」ための成果物（artifact）契約です。  
将来ClearML Task化を見越し、**Process単位で入力/出力が追跡できる**形にします。

---

## 1. 出力ディレクトリ（推奨形）
Hydraの出力ディレクトリ（例：`runs/<process>/<date>/<time>_<experiment>/`）に保存する。

- `<process>` は `train`/`predict`/`evaluate` 等の処理単位
- 1 run = 1 Process 実行

---

## 2. 必須ファイル（全Process共通）
- `config.yaml`：最終 config（Hydra合成結果）
- `meta.json`：メタ情報（下記必須キー）
- `logs/`：ログ（形式は自由だが存在推奨）


thinking
**Searching for feature pipeline**
exec
/bin/zsh -lc 'rg -n "Featur" src/common src -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 286ms:
src/fp/featurizer_fp.py:21:class FeaturizerError(ValueError):
src/fp/featurizer_fp.py:39:        raise FeaturizerError("mol is None")
src/fp/featurizer_fp.py:65:        raise FeaturizerError(f"Unknown fingerprint type: {cfg.fingerprint}")
src/fp/evaluate.py:12:from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
src/fp/evaluate.py:84:    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
src/fp/evaluate.py:85:        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
src/common/feature_pipeline.py:14:from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
src/common/feature_pipeline.py:20:class FeaturePipelineError(RuntimeError):
src/common/feature_pipeline.py:43:def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
src/common/feature_pipeline.py:49:    return GraphFeaturizerConfig(
src/common/feature_pipeline.py:58:class FingerprintFeaturePipeline:
src/common/feature_pipeline.py:69:    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
src/common/feature_pipeline.py:110:            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
src/common/feature_pipeline.py:132:            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
src/common/feature_pipeline.py:155:class GraphFeaturePipeline:
src/common/feature_pipeline.py:156:    graph_cfg: GraphFeaturizerConfig
src/common/feature_pipeline.py:161:    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
src/common/feature_pipeline.py:166:    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
src/common/feature_pipeline.py:173:        if not isinstance(gcfg, GraphFeaturizerConfig):
src/common/feature_pipeline.py:174:            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")

thinking
**Inspecting feature pipeline and featurizers**
exec
/bin/zsh -lc "sed -n '1,240p' src/common/feature_pipeline.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from __future__ import annotations

import pickle
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

from src.fp.feature_utils import build_features
from src.fp.featurizer_fp import FPConfig, featurize_mol
from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg

PIPELINE_VERSION = 1
FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"


class FeaturePipelineError(RuntimeError):

thinking
**Checking feature configurations**
exec
/bin/zsh -lc 'ls configs/features' in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
fp_morgan_desc.yaml
fp_morgan_fixture.yaml
fp_morgan_quick.yaml
gnn_graph.yaml
gnn_graph_quick.yaml

thinking
**Searching for featureset**
exec
/bin/zsh -lc 'rg -n "featureset|featurizer" configs -g"*.yaml"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
configs/features/fp_morgan_desc.yaml:1:featurizer:
configs/leaderboard/default.yaml:10:    featureset_name: null
configs/features/gnn_graph_quick.yaml:1:featurizer:
configs/features/fp_morgan_quick.yaml:1:featurizer:
configs/features/fp_morgan_fixture.yaml:1:featurizer:
configs/features/gnn_graph.yaml:1:featurizer:
exec
/bin/zsh -lc 'rg -n "featureset" -S' in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
src/common/leaderboard.py:24:    "featureset_name",
src/common/leaderboard.py:42:    "featureset_name",
src/common/leaderboard.py:171:            "featureset_name": meta.get("featureset_name"),
src/common/leaderboard.py:233:        "featureset_name",
src/fp/evaluate.py:75:            "featureset_name": train_context.get("featureset_name"),
src/gnn/evaluate.py:140:            "featureset_name": train_context.get("featureset_name"),
src/fp/predict.py:83:            "featureset_name": train_context.get("featureset_name"),
src/gnn/predict.py:87:            "featureset_name": train_context.get("featureset_name"),
src/utils/artifacts.py:23:    "featureset_name",
src/utils/artifacts.py:106:def resolve_featureset_name(cfg: Optional[Dict[str, Any]]) -> Optional[str]:
src/utils/artifacts.py:109:    if cfg.get("featureset_name"):
src/utils/artifacts.py:110:        return str(cfg["featureset_name"])
src/utils/artifacts.py:111:    featureset_cfg = cfg.get("featureset", {})
src/utils/artifacts.py:112:    if isinstance(featureset_cfg, dict) and featureset_cfg.get("name"):
src/utils/artifacts.py:113:        return str(featureset_cfg["name"])
src/utils/artifacts.py:195:            "featureset_name": resolve_featureset_name(cfg),
src/utils/artifacts.py:238:        "featureset_name": train_meta.get("featureset_name") or resolve_featureset_name(train_cfg),
tests/contract/test_artifacts_contract.py:50:    assert meta["featureset_name"] == "fp_morgan_desc"
configs/leaderboard/default.yaml:10:    featureset_name: null
tests/test_leaderboard.py:20:        "featureset_name": overrides.get("featureset_name", "fp_morgan"),

thinking
**Reviewing feature pipeline configs**
exec
/bin/zsh -lc 'ls configs' in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
audit
audit_dataset.yaml
audit_dataset_fixture.yaml
audit_dataset_quick.yaml
collect_data.yaml
config.yaml
config_quick.yaml
data_collection
data_source
dataset
dataset.yaml
dataset_fixture.yaml
dataset_quick.yaml
eval
features
fp
gnn
hydra
infer
leaderboard

thinking
**Checking for config.yaml**
exec
/bin/zsh -lc "sed -n '1,200p' configs/config.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_desc
  - model: fp_lightgbm
  - train: fp_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"

thinking
**Checking feature configurations**
exec
/bin/zsh -lc 'cat configs/features/fp_morgan_desc.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
featurizer:
  fingerprint: "morgan"
  morgan_radius: 2
  n_bits: 2048
  use_counts: false
  add_descriptors: ["MolWt","TPSA","MolLogP","HBD","HBA","NumRotatableBonds","RingCount","AromaticRings"]

data:
  cache_dir: "data/processed/cache/fp"

thinking
**Investigating feature pipeline usage**
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.plots import save_parity_plot, save_residual_plot, save_hist
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json, set_seed
from src.fp.feature_utils import hash_cfg
exec
/bin/zsh -lc "sed -n '220,520p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 262ms:
    train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
    train_fps = [p[0] for p in train_pairs]
    train_ids_for_ad = [p[1] for p in train_pairs]

    training_elements = sorted({el for e_str in el_train for el in e_str.split(",") if el})
    heavy_atoms_train = df_train["n_heavy_atoms"].dropna().astype(int).tolist()
    heavy_min = int(min(heavy_atoms_train)) if heavy_atoms_train else 0
    heavy_max = int(max(heavy_atoms_train)) if heavy_atoms_train else 0

    ad_artifact = {
        "training_elements": training_elements,
        "heavy_atom_range": [heavy_min, heavy_max],
        "morgan_radius": fp_cfg.morgan_radius,
        "n_bits": fp_cfg.n_bits,
        "train_ids": train_ids_for_ad,
        "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
    }
    with open(artifacts_dir / "ad.pkl", "wb") as f:

thinking
**Verifying feature pipeline integration**
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Tuple

import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import morgan_bitvect
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config


thinking
**Inspecting feature utilities for fingerprinting**
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/feature_utils.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
from __future__ import annotations

import hashlib
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.common.io import load_sdf_mol, sdf_path_from_cas
from src.common.utils import ensure_dir
from src.fp.featurizer_fp import FPConfig, featurize_mol


def hash_cfg(obj: Dict[str, Any]) -> str:
    s = repr(obj).encode("utf-8")
    return hashlib.sha256(s).hexdigest()[:12]


thinking
**Locating Featurizer FP implementation**
exec
/bin/zsh -lc "sed -n '1,240p' src/fp/featurizer_fp.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence, Tuple

import numpy as np

from src.common.descriptors import calc_descriptors, descriptors_to_array

try:
    from rdkit import Chem, DataStructs
    from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors
except Exception:  # pragma: no cover
    Chem = None
    DataStructs = None
    AllChem = None
    MACCSkeys = None
    rdMolDescriptors = None



thinking
**Planning pretrained_embedding feature integration**
exec
/bin/zsh -lc 'ls configs/process' in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
audit_dataset.yaml
build_dataset.yaml
collect_data.yaml
evaluate.yaml
leaderboard.yaml
predict.yaml
train.yaml
visualize.yaml

thinking
**Planning addition of pretrained embedding feature**
exec
/bin/zsh -lc 'rg -n "featureset" configs src -g"*.yaml" -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 293ms:
configs/leaderboard/default.yaml:10:    featureset_name: null
src/common/leaderboard.py:24:    "featureset_name",
src/common/leaderboard.py:42:    "featureset_name",
src/common/leaderboard.py:171:            "featureset_name": meta.get("featureset_name"),
src/common/leaderboard.py:233:        "featureset_name",
src/fp/evaluate.py:75:            "featureset_name": train_context.get("featureset_name"),
src/gnn/evaluate.py:140:            "featureset_name": train_context.get("featureset_name"),
src/fp/predict.py:83:            "featureset_name": train_context.get("featureset_name"),
src/gnn/predict.py:87:            "featureset_name": train_context.get("featureset_name"),
src/utils/artifacts.py:23:    "featureset_name",
src/utils/artifacts.py:106:def resolve_featureset_name(cfg: Optional[Dict[str, Any]]) -> Optional[str]:
src/utils/artifacts.py:109:    if cfg.get("featureset_name"):
src/utils/artifacts.py:110:        return str(cfg["featureset_name"])
src/utils/artifacts.py:111:    featureset_cfg = cfg.get("featureset", {})
src/utils/artifacts.py:112:    if isinstance(featureset_cfg, dict) and featureset_cfg.get("name"):
src/utils/artifacts.py:113:        return str(featureset_cfg["name"])
src/utils/artifacts.py:195:            "featureset_name": resolve_featureset_name(cfg),
src/utils/artifacts.py:238:        "featureset_name": train_meta.get("featureset_name") or resolve_featureset_name(train_cfg),

thinking
**Checking configuration options**
exec
/bin/zsh -lc 'rg -n "featureset" configs -g"*.yaml"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
configs/leaderboard/default.yaml:10:    featureset_name: null

thinking
**Checking featureset_name in meta build**
exec
/bin/zsh -lc "sed -n '1,240p' src/common/meta.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
from __future__ import annotations

from src.utils.artifacts import build_meta, save_meta, utc_now_iso

__all__ = ["build_meta", "save_meta", "utc_now_iso"]

thinking
**Checking utility artifact functions**
exec
/bin/zsh -lc "sed -n '1,280p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
from __future__ import annotations

import csv
import hashlib
import json
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Optional
from uuid import uuid4

from src.common.utils import save_json

REQUIRED_META_KEYS = [
    "run_id",
    "process_name",
    "created_at",
    "git_sha",
    "dataset_hash",
    "config_hash",

thinking
**Analyzing featureset and config handling**
exec
/bin/zsh -lc 'cat configs/features/gnn_graph.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
featurizer:
  node_features: ["atomic_num","degree","formal_charge","aromatic","num_h","in_ring"]
  edge_features: ["bond_type","conjugated","aromatic"]
  use_3d_pos: true
  add_global_descriptors: ["MolWt","TPSA","MolLogP","HBD","HBA"]

thinking
**Planning feature pipeline implementation**
exec
/bin/zsh -lc 'cat work/tasks/030_featurepipeline_unify.md' in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
# Task 030 (P0): FeaturePipeline の統一（train/infer skew 排除）

## 目的
- 学習と推論で特徴量がズレる問題を防ぐ（精度と運用の地雷）

## Plan
1) `src/common/feature_pipeline.py`（案）を作り、前処理→特徴量化の一本化
2) 学習側は pipeline を fit/transform、推論側は保存済み pipeline を load/transform
3) FP と GNN で共通化できる範囲を決め、差分は adapter で吸収
4) smoke test：同一サンプルで train→save→load→predict が一致すること

## Acceptance Criteria
- [x] 推論時に学習と同じ pipeline が使われる
- [x] pipeline 状態が artifact に保存される

## Implementation Notes
- `src/common/feature_pipeline.py` に FP/GNN の共通パイプラインを追加
- train/evaluate/predict で pipeline の save/load を統一し、旧 artifact も互換対応
- `tests/test_feature_pipeline.py` を追加して roundtrip を検証


thinking
**Preparing to add embedding featurizer**
exec
/bin/zsh -lc "sed -n '1,240p' tests/test_feature_pipeline.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 300ms:
from __future__ import annotations

from pathlib import Path

import numpy as np
import pytest

from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
from src.common.io import load_sdf_mol, sdf_path_from_cas


def _load_fixture_mol(cas: str):
    root = Path(__file__).resolve().parents[1]
    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))


def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
    pytest.importorskip("rdkit")


thinking
**Considering test coverage for embeddings**
exec
/bin/zsh -lc 'rg -n "pretrained|embedding" src tests configs' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 291ms:


thinking
**Planning new component addition**
exec
/bin/zsh -lc 'rg -n "featurize" src/scripts -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction exited 2 in 263ms:
rg: src/scripts: IO error for operation on src/scripts: No such file or directory (os error 2)

thinking
**Searching for scripts directory**
exec
/bin/zsh -lc 'rg -n "featurize" src scripts -g"*.py"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
src/gnn/evaluate.py:78:            data = pipeline.featurize_mol(mol, y=y)
src/common/feature_pipeline.py:13:from src.fp.featurizer_fp import FPConfig, featurize_mol
src/common/feature_pipeline.py:14:from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
src/common/feature_pipeline.py:70:        feat_cfg = cfg.get("featurizer", {}) or {}
src/common/feature_pipeline.py:120:    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
src/common/feature_pipeline.py:121:        return featurize_mol(mol, self.fp_cfg)
src/common/feature_pipeline.py:124:        x, meta = self.featurize_mol(mol)
src/common/feature_pipeline.py:162:        feat_cfg = cfg.get("featurizer", {}) or {}
src/common/feature_pipeline.py:168:        cfg_path = artifacts_dir / "graph_featurizer.pkl"
src/common/feature_pipeline.py:170:            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
src/common/feature_pipeline.py:174:            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
src/common/feature_pipeline.py:177:    def featurize_mol(self, mol, y: Optional[float] = None):
src/common/feature_pipeline.py:178:        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
src/fp/evaluate.py:87:    feat_cfg = train_cfg.get("featurizer", {})
src/fp/evaluate.py:92:    cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
src/gnn/featurizer_graph.py:55:def featurize_mol_to_pyg(mol, y: Optional[float], cfg: GraphFeaturizerConfig) -> "Data":
src/fp/feature_utils.py:14:from src.fp.featurizer_fp import FPConfig, featurize_mol
src/fp/feature_utils.py:64:        x, meta = featurize_mol(mol, fp_cfg)
src/fp/feature_utils.py:70:        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
src/gnn/train.py:179:        for cas, y in tqdm(zip(split_df[cas_col].astype(str).tolist(), split_df[target_col].astype(float).tolist()), total=len(split_df), desc=f"featurize {split_name}"):

thinking
**Adding embedding featurizer integration**
exec
/bin/zsh -lc "sed -n '1,200p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.feature_utils import hash_cfg
from src.tasks import resolve_task
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config


thinking
**Reviewing feature pipeline and config validation**
exec
/bin/zsh -lc "sed -n '1,240p' src/utils/validate_config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from __future__ import annotations

from typing import Any, Dict, Iterable

from src.common.config import ConfigError
from src.tasks import resolve_target_columns


def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
    missing = [k for k in keys if k not in cfg]
    if missing:
        raise ConfigError(f"{context} missing keys: {', '.join(missing)}")


def _has_target(cfg: Dict[str, Any]) -> bool:
    return bool(resolve_target_columns(cfg))


def validate_config(cfg: Dict[str, Any]) -> None:
    process = cfg.get("process", {})

thinking
**Designing deterministic pretrained embedding stub**
exec
/bin/zsh -lc 'rg -n "featurizer_state" src tests' in /Users/kawahito/Desktop/LJ_prediction succeeded in 311ms:
src/fp/train.py:270:        model_dir / "featurizer_state.json",
src/gnn/train.py:414:    save_json(model_dir / "featurizer_state.json", {"config": asdict(gcfg)})

thinking
**Designing embedding feature pipeline and integration**

I'm outlining how to add a pretrained embedding featurizer alongside the existing fingerprint pipeline. This involves creating new config dataclasses, embedding computation functions using RDKit, a specialized build function with caching, and a parallel pipeline class modeled after the fingerprint pipeline. I’m also planning configuration, pipeline resolution helpers, and tests to ensure proper caching, saving/loading, and deterministic embeddings. It’s a detailed design to smoothly integrate embeddings as a first-class featurizer option.
exec
/bin/zsh -lc 'ls tests/fixtures' in /Users/kawahito/Desktop/LJ_prediction succeeded in 277ms:
data
exec
/bin/zsh -lc 'ls tests/fixtures/data' in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
processed
raw
exec
/bin/zsh -lc 'ls tests/fixtures/data/raw' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
sdf_files
tc_pc_tb_fixture.csv

thinking
**Planning test for pretrained embedding pipeline**

I’m thinking about adding tests using existing SDF fixtures and pytest’s importorskip to handle RDKit’s optional presence. The key test will verify that the embedding pipeline returns consistent features after save/load and possibly after fitting with missing data. I’ll probably create a new test file or extend an existing one, checking both pickled pipelines and preprocessing artifact loading to ensure round-trip consistency. Also considering whether to update docs or other feature resolution, but it seems unnecessary. Next, I’ll look into the work queue to plan the update accordingly.
exec
/bin/zsh -lc 'cat work/queue.json' in /Users/kawahito/Desktop/LJ_prediction succeeded in 273ms:
{
  "version": 1,
  "updated_at": "2025-12-26T15:52:56.655610Z",
  "policy": {
    "priority_order": [
      "P0",
      "P1",
      "P2",
      "P3"
    ],
    "status_order": [
      "todo",
      "in_progress",
      "blocked",
      "done"
    ],
    "auto_set_in_progress_on_next": true
  },
  "tasks": [
    {

thinking
**Planning embedding feature pipeline integration**

I'm mapping out the changes to add a new embedding-based featurizer alongside the existing fingerprint pipeline. This includes creating new modules for embedding features, updating utilities for feature building and hashing, modifying the pipeline loader to support both types, and adjusting training code to handle applicability domain calculations depending on the pipeline used. I want to keep caching and config handling consistent while enabling flexible switching via the features config group. Let's explore these carefully!
exec
/bin/zsh -lc 'rg -n "ad" configs -g"*.yaml"' in /Users/kawahito/Desktop/LJ_prediction succeeded in 302ms:
configs/process/leaderboard.yaml:2:  name: "leaderboard"
configs/process/leaderboard.yaml:3:  kind: "leaderboard"
configs/process/leaderboard.yaml:6:  run_dir: "runs/leaderboard"
configs/eval/default.yaml:1:ad:
configs/features/fp_morgan_desc.yaml:3:  morgan_radius: 2
configs/features/fp_morgan_desc.yaml:6:  add_descriptors: ["MolWt","TPSA","MolLogP","HBD","HBA","NumRotatableBonds","RingCount","AromaticRings"]
configs/eval/quick.yaml:1:ad:
configs/features/gnn_graph_quick.yaml:5:  add_global_descriptors: ["MolWt","TPSA","MolLogP"]
configs/features/fp_morgan_fixture.yaml:3:  morgan_radius: 2
configs/features/fp_morgan_fixture.yaml:6:  add_descriptors: []
configs/features/fp_morgan_quick.yaml:3:  morgan_radius: 2
configs/features/fp_morgan_quick.yaml:6:  add_descriptors: ["MolWt","TPSA","MolLogP"]
configs/features/gnn_graph.yaml:5:  add_global_descriptors: ["MolWt","TPSA","MolLogP","HBD","HBA"]
configs/leaderboard/default.yaml:1:leaderboard:
configs/leaderboard.yaml:2:  - process: leaderboard
configs/leaderboard.yaml:3:  - leaderboard: default
configs/leaderboard.yaml:8:  name: "leaderboard"
configs/leaderboard.yaml:11:  exp_name: "leaderboard"

thinking
**Opening eval default**
exec
/bin/zsh -lc 'cat configs/eval/default.yaml' in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
ad:
  tanimoto_warn_threshold: 0.5
  top_k: 5

output:
  plots: false

thinking
**Investigating ad config location**
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/train.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_desc
  - model: fp_lightgbm
  - train: fp_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"

thinking
**Designing embedding featurizer integration**

This is a sizable plan to add a new embedding-based feature pipeline alongside the existing fingerprint pipeline. I’m setting up configs, pipeline classes, helper functions, and training/loading updates so both pipeline types can coexist and save consistent featurizer state. I’m also designing a deterministic embedding stub to avoid external models and ensuring AD fingerprinting works with sensible defaults for both pipelines. This careful layering should keep backward compatibility while enabling new embedding features. Let’s explore how best to implement and unify these components!
exec
/bin/zsh -lc "sed -n '1,200p' configs/config_quick.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 307ms:
defaults:
  - process: train
  - dataset: quick
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_quick
  - model: fp_lightgbm_quick
  - train: fp_default
  - eval: quick
  - hydra: default
  - _self_

experiment:
  name: "quick_lgbm_morgan_v1"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"

thinking
**Planning featurizer embedding changes**
exec
/bin/zsh -lc "cat <<'PY' > src/fp/featurizer_embedding.py
from __future__ import annotations

import hashlib
from dataclasses import dataclass
from typing import Any, Dict, Tuple

import numpy as np

try:
    from rdkit import Chem
except Exception:  # pragma: no cover
    Chem = None


class EmbeddingFeaturizerError(ValueError):
    pass


@dataclass
class PretrainedEmbeddingConfig:
    name: str = \"pretrained_embedding\"
    backend: str = \"stub\"
    embedding_dim: int = 256
    seed: int = 0
    normalize: bool = True


def _hash_seed(smiles: str, seed: int) -> int:
    payload = f\"{seed}:{smiles}\".encode(\"utf-8\")
    digest = hashlib.sha256(payload).digest()
    return int.from_bytes(digest[:8], \"big\", signed=False)


def _stub_embed_smiles(smiles: str, dim: int, seed: int) -> np.ndarray:
    rng = np.random.default_rng(_hash_seed(smiles, seed))
    vec = rng.normal(0.0, 1.0, size=dim)
    return vec.astype(float)


def featurize_mol(mol, cfg: PretrainedEmbeddingConfig) -> Tuple[np.ndarray, Dict[str, Any]]:
    if Chem is None:
        raise ImportError(\"RDKit is required for embedding featurization.\")
    if mol is None:
        raise EmbeddingFeaturizerError(\"mol is None\")

    backend = str(cfg.backend).lower()
    if backend "'!= "stub":
        raise EmbeddingFeaturizerError(f"Unknown embedding backend: {cfg.backend}")

    smiles = Chem.MolToSmiles(mol, canonical=True)
    vec = _stub_embed_smiles(smiles, int(cfg.embedding_dim), int(cfg.seed))
    if cfg.normalize:
        norm = float(np.linalg.norm(vec))
        if norm > 0:
            vec = vec / norm

    meta: Dict[str, Any] = {
        "embedding_backend": backend,
        "embedding_dim": int(cfg.embedding_dim),
        "embedding_stub": True,
    }
    return vec.astype(float), meta
PY' in /Users/kawahito/Desktop/LJ_prediction succeeded in 281ms:


thinking
**Updating feature_utils.py with new function**

thinking
**Checking patch application error**
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/feature_utils.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 278ms:
from __future__ import annotations

import hashlib
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.common.io import load_sdf_mol, sdf_path_from_cas
from src.common.utils import ensure_dir
from src.fp.featurizer_fp import FPConfig, featurize_mol


def hash_cfg(obj: Dict[str, Any]) -> str:
    s = repr(obj).encode("utf-8")
    return hashlib.sha256(s).hexdigest()[:12]


thinking
**Updating imports and adding feature function**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/feature_utils.py
@@ -13,2 +13,3 @@
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
@@ -98 +99,59 @@
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
apply_patch(auto_approved=true) exited 0 in 33ms:
Success. Updated the following files:
M src/fp/feature_utils.py
file update:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Planning updates to feature_pipeline.py**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -3,3 +3,3 @@
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
@@ -11,3 +11,4 @@
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
apply_patch(auto_approved=true) exited 0 in 30ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..4cbc7c8a86053eb3a246aeb494c92be02b8339c1
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..4cbc7c8a86053eb3a246aeb494c92be02b8339c1
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Adding embedding-related helpers**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -32,2 +32,14 @@
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
apply_patch(auto_approved=true) exited 0 in 24ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..13e50525a459da73ce07365400b438705e54949a
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..13e50525a459da73ce07365400b438705e54949a
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Inserting build_embedding_config function**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -55,2 +55,12 @@
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
apply_patch(auto_approved=true) exited 0 in 36ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..031038b7e70f5cc654f64b1718797dc17ac7d7a4
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..031038b7e70f5cc654f64b1718797dc17ac7d7a4
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Preparing to add featurizer_state method**
exec
/bin/zsh -lc "sed -n '40,200p' src/common/feature_pipeline.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 320ms:
    if feat_cfg.get("embedding_dim") is not None:
        return True
    return False


def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
    return FPConfig(
        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
        n_bits=int(feat_cfg.get("n_bits", 2048)),
        use_counts=bool(feat_cfg.get("use_counts", False)),
        add_descriptors=add_desc,
    )


def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
    return PretrainedEmbeddingConfig(
        name=str(feat_cfg.get("name", "pretrained_embedding")),
        backend=str(feat_cfg.get("backend", "stub")),
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..031038b7e70f5cc654f64b1718797dc17ac7d7a4
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Adding featurizer_state method and EmbeddingFeaturePipeline class**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -175,2 +175,5 @@
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
 
apply_patch(auto_approved=true) exited 0 in 47ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d572fa4c08f33fbc6fe2e6f605c9acb386f6502d
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,9 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d572fa4c08f33fbc6fe2e6f605c9acb386f6502d
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,9 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Preparing insertion of EmbeddingFeaturePipeline**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -180,2 +180,102 @@
 @dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
 class GraphFeaturePipeline:
apply_patch(auto_approved=true) exited 0 in 55ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..0b91ea2a072cd55108094bcaef9bb146ad998ce4
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..0b91ea2a072cd55108094bcaef9bb146ad998ce4
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Planning helper function insertion**
exec
/bin/zsh -lc "sed -n '200,340p' src/common/feature_pipeline.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 298ms:

    def build_features(
        self,
        df,
        sdf_dir: Path,
        cas_col: str,
        cache_dir: Optional[Path],
        cache_key: str,
        logger,
    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
        X, ids, elements, meta = build_embedding_features(
            df=df,
            sdf_dir=sdf_dir,
            cas_col=cas_col,
            emb_cfg=self.emb_cfg,
            cache_dir=cache_dir,
            cache_key=cache_key,
            logger=logger,
        )
        self.feature_meta = meta
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..0b91ea2a072cd55108094bcaef9bb146ad998ce4
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Inserting resolve_tabular_pipeline function**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -306,2 +306,9 @@
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
apply_patch(auto_approved=true) exited 0 in 56ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..88b468aa0b69ff352566eacd566ae85024f9da5c
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,8 +173,111 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
 class GraphFeaturePipeline:
     graph_cfg: GraphFeaturizerConfig
     pipeline_type: str = field(default="gnn", init=False)
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..88b468aa0b69ff352566eacd566ae85024f9da5c
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,8 +173,111 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
 class GraphFeaturePipeline:
     graph_cfg: GraphFeaturizerConfig
     pipeline_type: str = field(default="gnn", init=False)
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Adding load_tabular_pipeline function**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
@@ -329 +329,12 @@
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
apply_patch(auto_approved=true) exited 0 in 41ms:
Success. Updated the following files:
M src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Preparing to update training script imports**
exec
/bin/zsh -lc "sed -n '1,60p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 278ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.plots import save_parity_plot, save_residual_plot, save_hist
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json, set_seed
from src.fp.feature_utils import hash_cfg
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last


thinking
**Planning pipeline import and usage updates**

I'm preparing to replace the current `FingerprintFeaturePipeline` import and usage with `resolve_tabular_pipeline` while keeping `FingerprintFeaturePipeline` for type checks. I'll clean up unused imports like `asdict` and revise the config handling to accommodate AD settings depending on the pipeline type, then update saving logic for the featurizer state accordingly.
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -4,3 +4,2 @@
 import pickle
-from dataclasses import asdict
 from pathlib import Path
@@ -13,3 +12,3 @@
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
apply_patch(auto_approved=true) exited 0 in 34ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..8143d1de6ea038f3e3405bdcabfe924509c157d7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..8143d1de6ea038f3e3405bdcabfe924509c157d7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist


thinking
**Updating pipeline creation and AD**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -82,4 +82,3 @@
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
apply_patch(auto_approved=true) exited 0 in 44ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..143174a84625726f4e8212cfa0f81e8542df1804
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..143174a84625726f4e8212cfa0f81e8542df1804
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))


thinking
**Modifying AD fingerprint configuration**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -212,5 +212,13 @@
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
@@ -229,8 +237,8 @@
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
apply_patch(auto_approved=true) exited 0 in 51ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..1cc1cc99e4244790256b3ea4e1c49ebe602ce7b0
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..1cc1cc99e4244790256b3ea4e1c49ebe602ce7b0
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)


thinking
**Modifying featurizer state saving**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -274,6 +274,3 @@
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
apply_patch(auto_approved=true) exited 0 in 40ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..9ba57cf8db146c198f37e0c89dd90352557c59d9
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..9ba57cf8db146c198f37e0c89dd90352557c59d9
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")


thinking
**Cleaning up unused variables**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -90,3 +90,3 @@
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
apply_patch(auto_approved=true) exited 0 in 56ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")


thinking
**Updating evaluate and predict imports**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/evaluate.py
@@ -11,3 +11,3 @@
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
@@ -82,6 +82,3 @@
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
apply_patch(auto_approved=true) exited 0 in 38ms:
Success. Updated the following files:
M src/fp/evaluate.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")


thinking
**Updating import and pipeline initialization**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/predict.py
@@ -12,3 +12,3 @@
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
@@ -95,6 +95,3 @@
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
apply_patch(auto_approved=true) exited 0 in 42ms:
Success. Updated the following files:
M src/fp/predict.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")


thinking
**Updating imports and adding embedding pipeline test**
file update
M /Users/kawahito/Desktop/LJ_prediction/tests/test_feature_pipeline.py
@@ -7,3 +7,3 @@
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
@@ -51 +51,38 @@
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]
apply_patch(auto_approved=true) exited 0 in 45ms:
Success. Updated the following files:
M tests/test_feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Creating config file for feature stub**
exec
/bin/zsh -lc "cat <<'YAML' > configs/features/pretrained_embedding_stub.yaml
featurizer:
  name: \"pretrained_embedding\"
  backend: \"stub\"
  embedding_dim: 256
  seed: 0
  normalize: true

data:
  cache_dir: \"data/processed/cache/embedding\"
YAML" in /Users/kawahito/Desktop/LJ_prediction succeeded in 281ms:

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Enhancing feature set name resolution**

I'm updating several files to integrate `resolve_tabular_pipeline` and clean unused imports, then focusing on making `resolve_featureset_name` more robust by adding a fallback to detect embeddings via `embedding_dim` in the config, ensuring featureset naming stays consistent even if `name` isn't explicitly set. This small patch should improve inference stability without breaking existing behavior.
file update
M /Users/kawahito/Desktop/LJ_prediction/src/utils/artifacts.py
@@ -119,2 +119,4 @@
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
apply_patch(auto_approved=true) exited 0 in 54ms:
Success. Updated the following files:
M src/utils/artifacts.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Updating feature pipeline imports and types**
exec
/bin/zsh -lc 'rg -n "fp_cfg" src/fp/train.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 295ms:
215:        ad_radius = pipeline.fp_cfg.morgan_radius
216:        ad_n_bits = pipeline.fp_cfg.n_bits
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Reviewing imports for cleanup**
exec
/bin/zsh -lc "sed -n '1,40p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import load_tabular_pipeline
from src.common.meta import build_meta, save_meta
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.feature_utils import hash_cfg
from src.tasks import resolve_task
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Reviewing prediction file**
exec
/bin/zsh -lc "sed -n '1,40p' src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 250ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Tuple

import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import load_tabular_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import morgan_bitvect
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..25935255d4da1c5a1faae782c8c103e1677baff4
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,61 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Updating embedding feature pipeline and validation**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/feature_utils.py
@@ -143,2 +143,4 @@
     X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
     meta_last["feature_dim"] = int(X.shape[1])
apply_patch(auto_approved=true) exited 0 in 59ms:
Success. Updated the following files:
M src/fp/feature_utils.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Summarizing feature pipeline updates**

I’m reviewing and updating various source files to integrate `build_embedding_config` and ensure the embedding pipeline is used correctly throughout training, evaluation, and prediction. I check that `load_tabular_pipeline` is imported and used properly, confirm related tests are updated, and ensure consistency in saving/loading pipeline artifacts like the imputer and scaler. I’m also planning updates to the queue status and timestamps in `work/queue.json` after implementation, aiming to keep everything compatible without breaking existing flows. Finally, I intend to run ripgrep to verify no missing references remain.
exec
/bin/zsh -lc 'rg -n "load_tabular_pipeline|resolve_tabular_pipeline|EmbeddingFeaturePipeline" src tests' in /Users/kawahito/Desktop/LJ_prediction succeeded in 301ms:
tests/test_feature_pipeline.py:8:from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
tests/test_feature_pipeline.py:68:    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
tests/test_feature_pipeline.py:84:    assert isinstance(loaded, EmbeddingFeaturePipeline)
src/common/feature_pipeline.py:181:class EmbeddingFeaturePipeline:
src/common/feature_pipeline.py:192:    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
src/common/feature_pipeline.py:233:            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
src/common/feature_pipeline.py:307:def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
src/common/feature_pipeline.py:310:        return EmbeddingFeaturePipeline.from_config(cfg)
src/common/feature_pipeline.py:332:def load_tabular_pipeline(
src/common/feature_pipeline.py:334:) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
src/common/feature_pipeline.py:336:    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
src/common/feature_pipeline.py:338:    pipeline = resolve_tabular_pipeline(train_cfg)
src/fp/evaluate.py:12:from src.common.feature_pipeline import load_tabular_pipeline
src/fp/evaluate.py:83:    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
src/fp/predict.py:13:from src.common.feature_pipeline import load_tabular_pipeline
src/fp/predict.py:96:    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
src/fp/train.py:13:from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
src/fp/train.py:83:    pipeline = resolve_tabular_pipeline(cfg)
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Updating queue and marking task done**

I’m preparing to update `work/queue.json` using a Python script to mark task 070 as done and set timestamps, being careful to preserve the file’s existing Japanese characters by disabling ASCII escaping and keeping the two-space indentation and trailing newline. This ensures the queue properly reflects the task status without disrupting formatting or content.
exec
/bin/zsh -lc "python - <<'PY'
from __future__ import annotations
import json
from datetime import datetime, timezone
from pathlib import Path

path = Path('work/queue.json')
with path.open('r', encoding='utf-8') as f:
    data = json.load(f)

now = datetime.now(timezone.utc).isoformat(timespec='microseconds').replace('+00:00', 'Z')
data['updated_at'] = now
for task in data.get('tasks', []):
    if task.get('id') == '070':
        task['status'] = 'done'
        task['last_presented_at'] = now
        break

path.write_text(json.dumps(data, ensure_ascii=False, indent=2) + \"\\n\", encoding='utf-8')
PY" in /Users/kawahito/Desktop/LJ_prediction succeeded in 314ms:

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Verifying updated queue contents**
exec
/bin/zsh -lc "rg -n \"\\\"id\\\": \\\"070\\\"\" -C 6 work/queue.json" in /Users/kawahito/Desktop/LJ_prediction succeeded in 325ms:
210-        "docs/03_CONFIG_CONVENTIONS.md"
211-      ],
212-      "started_at": "2025-12-26T15:35:31.322697Z",
213-      "last_presented_at": "2025-12-26T15:50:13.728307Z"
214-    },
215-    {
216:      "id": "070",
217-      "priority": "P2",
218-      "status": "done",
219-      "title": "事前学習埋め込み特徴量の導入（任意）",
220-      "path": "work/tasks/070_add_pretrained_embeddings.md",
221-      "skills": [
222-        "S40_featurizer"
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Verifying update and checking git status**
exec
/bin/zsh -lc 'git status -sb' in /Users/kawahito/Desktop/LJ_prediction succeeded in 320ms:
## test_chemkit
 M README.md
 M configs/dataset.yaml
 M configs/dataset_quick.yaml
 M configs/fp/predict.yaml
 M configs/fp/predict_quick.yaml
 M configs/fp/train.yaml
 M configs/fp/train_quick.yaml
 M configs/gnn/predict.yaml
 M configs/gnn/train.yaml
 M configs/gnn/train_mpnn_quick.yaml
 M configs/gnn/train_quick.yaml
 M objective.md
 M scripts/build_dataset.py
 D scripts/fp_predict.py
 D scripts/fp_train.py
 D scripts/gnn_predict.py
 D scripts/gnn_train.py
 M src/common/config.py
 M src/common/splitters.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]


thinking
**Reviewing and verifying pipeline updates**

I’m carefully walking through the recent pipeline changes to check for any inconsistencies or potential issues, especially with embedding configurations and handling of imputers. I’m also considering test coverage and whether to run targeted tests to verify imports and detect syntax errors, all while ensuring the modifications fit smoothly with existing structures.
exec
/bin/zsh -lc 'python -m pytest tests/test_feature_pipeline.py -k embedding' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 309ms:
/Users/kawahito/Desktop/LJ_prediction/.venv/bin/python: No module named pytest
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]



thinking
**Summarizing implementation and final response plan**

I’m gathering all the file changes made—new featurizer, config, pipeline resolver, and test updates—and preparing a structured final response with plan, diff snippets, tests, verification commands, compatibility impacts, and queue status; also noting pytest is missing so tests couldn’t run. This will clearly explain how the pretrained embedding pipeline was integrated and provide the user with a detailed but concise summary.
thinking
**Finalizing structured response outline**

I’m organizing the final response with clear headers and bullet lists, ensuring no blank line before first bullets, including plan, implementation with diff snippets, tests with pytest failure noted, verification commands, compatibility details, queue update, and a next steps section with numbered suggestions. This will give a polished and comprehensive summary that follows all formatting rules.
codex
Using skill S40_featurizer, I added a stub pretrained-embedding featurizer plus a tabular pipeline selector so FP train/eval/predict can switch featuresets without train/infer skew. The stub uses deterministic SMILES hashing and is saved/loaded via existing feature pipeline artifacts, with AD falling back to default Morgan settings when embeddings are used.

**Plan**
- `src/fp/featurizer_embedding.py`: add deterministic stub embedding featurizer.
- `src/common/feature_pipeline.py`: add embedding pipeline plus tabular pipeline resolver/loader.
- `src/fp/train.py`: select tabular pipeline and keep AD defaults when embeddings are used.
- `src/fp/evaluate.py`, `src/fp/predict.py`: load the tabular pipeline from artifacts/config.
- `src/fp/feature_utils.py`: add embedding feature builder and cache path.
- `configs/features/pretrained_embedding_stub.yaml`: expose featureset selection.
- `tests/test_feature_pipeline.py`: add embedding roundtrip test.
- `src/utils/artifacts.py`: resolve featureset name for embeddings.

**Implementation**
- `src/fp/featurizer_embedding.py`
```diff
+@dataclass
+class PretrainedEmbeddingConfig:
+    name: str = "pretrained_embedding"
+    backend: str = "stub"
+    embedding_dim: int = 256
+    seed: int = 0
+    normalize: bool = True
+
+def featurize_mol(mol, cfg: PretrainedEmbeddingConfig) -> Tuple[np.ndarray, Dict[str, Any]]:
+    smiles = Chem.MolToSmiles(mol, canonical=True)
+    vec = _stub_embed_smiles(smiles, int(cfg.embedding_dim), int(cfg.seed))
+    ...
```
- `src/common/feature_pipeline.py`, `src/fp/train.py`
```diff
+class EmbeddingFeaturePipeline:
+    ...
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    pipeline = resolve_tabular_pipeline(cfg)
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
```
- `configs/features/pretrained_embedding_stub.yaml`
```diff
+featurizer:
+  name: "pretrained_embedding"
+  backend: "stub"
+  embedding_dim: 256
+  seed: 0
+  normalize: true
```

**Tests**
- `tests/test_feature_pipeline.py`: added `test_embedding_pipeline_roundtrip`.
- `python -m pytest tests/test_feature_pipeline.py -k embedding` failed with `No module named pytest`.

**Verification**
- `python -m pytest tests/test_feature_pipeline.py -k embedding`

**Compatibility**
- Config: new features group `configs/features/pretrained_embedding_stub.yaml` (select via `features=pretrained_embedding_stub`); optional `ad.morgan_radius`/`ad.n_bits` apply for embedding AD.
- CLI: no new flags; FP train/eval/predict auto-select tabular pipeline from `featurizer`.
- Artifacts: `feature_pipeline.pkl` may store `EmbeddingFeaturePipeline`; `model/featurizer_state.json` now includes a `type` key.

**Queue**
- `work/queue.json`: task 070 set to done.

**Next Steps**
1. Run pytest in the project venv once `pytest` is available.
2. Try a baseline run with `features=pretrained_embedding_stub` to validate end-to-end behavior.
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
index 875ff04d04a809c34283e06c5058613b29799611..d986486f28f5f144207d644f4dbad0b195619efa
--- a/src/common/feature_pipeline.py
+++ b/src/common/feature_pipeline.py
@@ -1,7 +1,7 @@
 from __future__ import annotations
 
 import pickle
-from dataclasses import dataclass, field
+from dataclasses import asdict, dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional, Tuple
 
@@ -9,7 +9,8 @@
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler
 
-from src.fp.feature_utils import build_features
+from src.fp.feature_utils import build_embedding_features, build_features
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 
@@ -29,6 +30,18 @@
     return [str(value)]
 
 
+def is_pretrained_embedding(feat_cfg: Dict[str, Any]) -> bool:
+    name = str(feat_cfg.get("name", "")).lower()
+    if name in {"pretrained_embedding", "pretrained-embedding", "embedding"}:
+        return True
+    kind = str(feat_cfg.get("type", "")).lower()
+    if kind in {"pretrained_embedding", "embedding"}:
+        return True
+    if feat_cfg.get("embedding_dim") is not None:
+        return True
+    return False
+
+
 def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
     add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
     return FPConfig(
@@ -40,6 +53,16 @@
     )
 
 
+def build_embedding_config(feat_cfg: Dict[str, Any]) -> PretrainedEmbeddingConfig:
+    return PretrainedEmbeddingConfig(
+        name=str(feat_cfg.get("name", "pretrained_embedding")),
+        backend=str(feat_cfg.get("backend", "stub")),
+        embedding_dim=int(feat_cfg.get("embedding_dim", 256)),
+        seed=int(feat_cfg.get("seed", 0)),
+        normalize=bool(feat_cfg.get("normalize", True)),
+    )
+
+
 def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
     node_features = feat_cfg.get(
         "node_features",
@@ -150,6 +173,109 @@
         else:
             self.scaler = None
 
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "fingerprint", "config": asdict(self.fp_cfg), "feature_meta": self.feature_meta}
+
+
+@dataclass
+class EmbeddingFeaturePipeline:
+    emb_cfg: PretrainedEmbeddingConfig = field(default_factory=PretrainedEmbeddingConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="embedding", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "EmbeddingFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            emb_cfg=build_embedding_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_embedding_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            emb_cfg=self.emb_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("EmbeddingFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_embedding_mol(mol, self.emb_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+    def featurizer_state(self) -> Dict[str, Any]:
+        return {"type": "pretrained_embedding", "config": asdict(self.emb_cfg), "feature_meta": self.feature_meta}
+
 
 @dataclass
 class GraphFeaturePipeline:
@@ -178,6 +304,13 @@
         return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
 
 
+def resolve_tabular_pipeline(cfg: Dict[str, Any]) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    feat_cfg = cfg.get("featurizer", {}) or {}
+    if is_pretrained_embedding(feat_cfg):
+        return EmbeddingFeaturePipeline.from_config(cfg)
+    return FingerprintFeaturePipeline.from_config(cfg)
+
+
 def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
     artifacts_dir = Path(artifacts_dir)
     artifacts_dir.mkdir(parents=True, exist_ok=True)
@@ -194,3 +327,14 @@
         return None
     with open(path, "rb") as f:
         return pickle.load(f)
+
+
+def load_tabular_pipeline(
+    artifacts_dir: Path, train_cfg: Dict[str, Any]
+) -> FingerprintFeaturePipeline | EmbeddingFeaturePipeline:
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if isinstance(pipeline, (FingerprintFeaturePipeline, EmbeddingFeaturePipeline)) and pipeline.imputer is not None:
+        return pipeline
+    pipeline = resolve_tabular_pipeline(train_cfg)
+    pipeline.load_preprocess_artifacts(artifacts_dir)
+    return pipeline
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index bd8969cf185bf08f06b49a587399c76fdfa8c329..63a4616d403aadb4181d1709ce31abd5b28125ed
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,7 +9,7 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -80,10 +80,7 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
     feat_cfg = train_cfg.get("featurizer", {})
 
     cache_dir = data_cfg.get("cache_dir", None)
diff --git a/src/fp/feature_utils.py b/src/fp/feature_utils.py
index 121b8ccce8478126114cb0fe27bb7ccdfb6e0e45..8c2554ba9dae796cf4326533d5cb86b332b10e23
--- a/src/fp/feature_utils.py
+++ b/src/fp/feature_utils.py
@@ -11,6 +11,7 @@
 
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 from src.common.utils import ensure_dir
+from src.fp.featurizer_embedding import PretrainedEmbeddingConfig, featurize_mol as featurize_embedding_mol
 from src.fp.featurizer_fp import FPConfig, featurize_mol
 
 
@@ -96,3 +97,63 @@
             pickle.dump(meta_last, f)
 
     return X, ids, elements, meta_last
+
+
+def build_embedding_features(
+    df: pd.DataFrame,
+    sdf_dir: Path,
+    cas_col: str,
+    emb_cfg: PretrainedEmbeddingConfig,
+    cache_dir: Optional[Path],
+    cache_key: str,
+    logger,
+) -> Tuple[np.ndarray, List[Any], List[str], Dict[str, Any]]:
+    """Return X (N,D), ids (CAS), elements_list, meta for embedding features."""
+    ensure_dir(cache_dir) if cache_dir is not None else None
+    cache_path = None
+    meta_path = None
+    if cache_dir is not None:
+        cache_path = cache_dir / f"emb_features_{cache_key}.npz"
+        meta_path = cache_dir / f"emb_features_{cache_key}_meta.pkl"
+        if cache_path.exists() and meta_path.exists():
+            logger.info(f"Loading cached features: {cache_path}")
+            npz = np.load(cache_path, allow_pickle=True)
+            X = npz["X"]
+            ids = npz["ids"].tolist()
+            elements = npz["elements"].tolist()
+            with open(meta_path, "rb") as f:
+                meta = pickle.load(f)
+            return X, ids, elements, meta
+
+    ids = df[cas_col].astype(str).tolist()
+    elements = df.get("elements", pd.Series([""] * len(df))).astype(str).tolist()
+    X_list: List[np.ndarray] = []
+    meta_last: Dict[str, Any] = {}
+
+    logger.info("Featurizing molecules (pretrained embedding stub) ...")
+    for cas in tqdm(ids, total=len(ids)):
+        mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+        if mol is None:
+            X_list.append(np.full((int(emb_cfg.embedding_dim),), np.nan, dtype=float))
+            continue
+        x, meta = featurize_embedding_mol(mol, emb_cfg)
+        meta_last = meta
+        X_list.append(x)
+
+    X = np.vstack(X_list).astype(float)
+    if np.isnan(X).all():
+        raise RuntimeError("No valid molecules could be featurized. Check sdf_dir and SDF files.")
+    meta_last["feature_dim"] = int(X.shape[1])
+
+    if cache_path is not None:
+        logger.info(f"Saving feature cache: {cache_path}")
+        np.savez_compressed(
+            cache_path,
+            X=X,
+            ids=np.array(ids, dtype=object),
+            elements=np.array(elements, dtype=object),
+        )
+        with open(meta_path, "wb") as f:
+            pickle.dump(meta_last, f)
+
+    return X, ids, elements, meta_last
diff --git a/src/fp/predict.py b/src/fp/predict.py
index b84913479f7331f3b58f3d39dcd918fd7e8ec4cd..7f7079ea27ed0e323e077d234b61c2725a6f7d93
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -10,7 +10,7 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
+from src.common.feature_pipeline import load_tabular_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
@@ -93,10 +93,7 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    pipeline = load_feature_pipeline(artifacts_dir)
-    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
-        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
-        pipeline.load_preprocess_artifacts(artifacts_dir)
+    pipeline = load_tabular_pipeline(artifacts_dir, train_cfg)
 
     x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
diff --git a/src/fp/train.py b/src/fp/train.py
index e37462352d5ad407faf7f580770acc6344e49e37..958940efd8b3cf474338db6ab4893fb7228488c1
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -2,7 +2,6 @@
 
 import argparse
 import pickle
-from dataclasses import asdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
@@ -11,7 +10,7 @@
 
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
-from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
+from src.common.feature_pipeline import FingerprintFeaturePipeline, resolve_tabular_pipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
@@ -81,8 +80,7 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    pipeline = FingerprintFeaturePipeline.from_config(cfg)
-    fp_cfg = pipeline.fp_cfg
+    pipeline = resolve_tabular_pipeline(cfg)
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -90,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -212,9 +210,17 @@
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
+    ad_cfg = cfg.get("ad", {}) or {}
+    if isinstance(pipeline, FingerprintFeaturePipeline):
+        ad_radius = pipeline.fp_cfg.morgan_radius
+        ad_n_bits = pipeline.fp_cfg.n_bits
+    else:
+        ad_radius = int(ad_cfg.get("morgan_radius", 2))
+        ad_n_bits = int(ad_cfg.get("n_bits", 2048))
+
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
     train_fps = [
-        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+        morgan_bitvect(m, radius=ad_radius, n_bits=ad_n_bits) if m is not None else None for m in train_mols
     ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
@@ -229,12 +235,12 @@
     ad_artifact = {
         "training_elements": training_elements,
         "heavy_atom_range": [heavy_min, heavy_max],
-        "morgan_radius": fp_cfg.morgan_radius,
-        "n_bits": fp_cfg.n_bits,
+        "morgan_radius": ad_radius,
+        "n_bits": ad_n_bits,
         "train_ids": train_ids_for_ad,
         "train_fps": train_fps,  # RDKit ExplicitBitVect list (pickleable)
-        "tanimoto_warn_threshold": float(cfg.get("ad", {}).get("tanimoto_warn_threshold", 0.5)),
-        "top_k": int(cfg.get("ad", {}).get("top_k", 5)),
+        "tanimoto_warn_threshold": float(ad_cfg.get("tanimoto_warn_threshold", 0.5)),
+        "top_k": int(ad_cfg.get("top_k", 5)),
     }
     with open(artifacts_dir / "ad.pkl", "wb") as f:
         pickle.dump(ad_artifact, f)
@@ -266,10 +272,7 @@
             },
             f,
         )
-    save_json(
-        model_dir / "featurizer_state.json",
-        {"config": asdict(fp_cfg), "feature_meta": feat_meta},
-    )
+    save_json(model_dir / "featurizer_state.json", pipeline.featurizer_state())
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
 
     logger.info(f"Saved model to {model_path}")
diff --git a/src/utils/artifacts.py b/src/utils/artifacts.py
index 5b9ddf130d8636c3a4580a9fda070600ae442862..b4dea8b94bdb4cf43b1c022207fe5a4686c42bd4
--- a/src/utils/artifacts.py
+++ b/src/utils/artifacts.py
@@ -117,6 +117,8 @@
     featurizer_cfg = cfg.get("featurizer", {})
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("name"):
         return str(featurizer_cfg["name"])
+    if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("embedding_dim") is not None:
+        return "pretrained_embedding"
 
     if isinstance(featurizer_cfg, dict) and featurizer_cfg.get("fingerprint"):
         fp_name = str(featurizer_cfg.get("fingerprint", "fp"))
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
index e10e98b1ecd06cedca4489d04ca0fbff7367b54c..301e5cfd2e6c422d2b06b30df33127e838a762d9
--- a/tests/test_feature_pipeline.py
+++ b/tests/test_feature_pipeline.py
@@ -5,7 +5,7 @@
 import numpy as np
 import pytest
 
-from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.feature_pipeline import EmbeddingFeaturePipeline, FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
 from src.common.io import load_sdf_mol, sdf_path_from_cas
 
 
@@ -49,3 +49,40 @@
     x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
     assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
     assert meta1["fp_type"] == meta2["fp_type"]
+
+
+def test_embedding_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "name": "pretrained_embedding",
+            "backend": "stub",
+            "embedding_dim": 32,
+            "seed": 7,
+            "normalize": True,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = EmbeddingFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, EmbeddingFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["embedding_dim"] == meta2["embedding_dim"]

tokens used
251,926
