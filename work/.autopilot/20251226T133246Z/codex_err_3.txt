OpenAI Codex v0.77.0 (research preview)
--------
workdir: /Users/kawahito/Desktop/LJ_prediction
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: xhigh
reasoning summaries: auto
session id: 019b5afd-f292-7483-b13e-5550fbc6c366
--------
user
# ===== SESSION CONTEXT =====
# Codex Session Context（毎回貼る短文）

- このrepoは「化学構造（SDF/CSV）→ 物性予測」を行う CLI/Hydra 基盤。
- 不変条件は docs/00_INVARIANTS.md が唯一の正。
- 今回の作業範囲・要件は work/tasks/<TASK>.md が唯一の正。
- 学習と推論の特徴量/前処理は必ず一致させる（skew禁止）。
- 成果物（artifact）は docs/04_ARTIFACTS_AND_VERSIONING.md を守る。
- 破壊的変更が必要なら work/rfc → docs/adr を先に作る（勝手に壊さない）。
- すべての処理は Process（処理単位）としてHydraで管理し、単独実行できる形を守る。
- 将来ClearML Task化を想定し、各Processが artifact を明確に出す設計にする（実装は今しない）。

# ===== AUTOPILOT MODE (non-interactive) =====
あなたは自動実行ループの中で動いています。**停止しないための規約**として次を厳守してください：

【禁止】
- ユーザーへの確認・質問・承認依頼（"確認してください" / "進めてよいですか" / "Approve" 等）は一切しない。
- "今回は未実装" / "差分なし" のまま終了しない（必ず前進する）。

【single source of truth】
- タスクの status の唯一の真実は work/queue.json。task.md の 'Blocked' 記述は参考情報。
  - queue が todo/in_progress なら実装を進める（task.md 側の Blocked を理由に止めない）。
  - queue が blocked なら blocked-handling を行う（reason/unblock_condition/next_action を整備し、解除子タスクを起票）。

【stale Blocked の扱い】
- task.md に '## Blocked' が残っていても queue が todo/in_progress の場合、それは **stale**。
  - task.md を更新し、Blocked を Notes/History に移す or 解除した上で実装を進める。

【書き込み権限】
- **書き込みは可能**な前提。work/queue.json 更新を含む必要な変更は実施する。
  - もし実際にツール/環境エラーで書き込めない場合：
    1) エラーメッセージを短く引用
    2) 最小の対処（sandbox/approval/flags 見直し、権限確認）を提示
    3) タスクを blocked にして解除子タスクを起票（子に unblocks を付与）
    ※それでも『確認してください』は書かない。

【必ずどれかを実施（Decision Protocol）】
A) 実装を進める → テスト/検証 → work/queue.json を done に更新
B) 進められない → work/queue.json を blocked に更新し、解除子タスク（unblocks付き）を作成
C) 依存/状態がズレている → work/queue.json と task.md を修正して整合させた上で A へ

# ===== SELECTED TASK =====
- id: 030
- title: FeaturePipeline 統一（train/infer skew排除）
- priority: P0
- status: in_progress
- path: work/tasks/030_featurepipeline_unify.md

# ===== CONTRACTS TO FOLLOW (open & read) =====
- docs/00_INVARIANTS.md
- docs/02_DATA_CONTRACTS.md
- docs/10_PROCESS_CATALOG.md

# ===== SKILLS TO FOLLOW (open & follow) =====
- agentskills/skills/S40_featurizer.md
- agentskills/skills/S30_preprocess_pipeline.md
- agentskills/skills/S95_tests_ci.md


# ===== TASK FILE (single source of truth for *intent*) =====
# Task 030 (P0): FeaturePipeline の統一（train/infer skew 排除）

## 目的
- 学習と推論で特徴量がズレる問題を防ぐ（精度と運用の地雷）

## Plan
1) `src/common/feature_pipeline.py`（案）を作り、前処理→特徴量化の一本化
2) 学習側は pipeline を fit/transform、推論側は保存済み pipeline を load/transform
3) FP と GNN で共通化できる範囲を決め、差分は adapter で吸収
4) smoke test：同一サンプルで train→save→load→predict が一致すること

## Acceptance Criteria
- [ ] 推論時に学習と同じ pipeline が使われる
- [ ] pipeline 状態が artifact に保存される

# ===== OUTPUT REQUIREMENTS (mandatory) =====
1) 変更計画（ファイル単位）
2) 実装（差分が分かるように）
3) 追加/更新したテスト
4) 検証コマンド
5) 互換性影響（config/CLI/artifact）
6) タスク完了時：work/queue.json の status を done に更新（満たせない場合は blocked と理由）


mcp: context7 starting
mcp: context7 ready
mcp startup: ready: context7

thinking
**Preparing to explore skill and contract files**
exec
/bin/zsh -lc ls in /Users/kawahito/Desktop/LJ_prediction succeeded in 312ms:
README.md
agentskills
codex
configs
data
docs
learning_curve_val.png
objective.md
parity_test.png
parity_val.png
requirements.txt
residual_test.png
residual_val.png
runs
scripts
src
tests
tools
work
y_train_hist.png

thinking
**Preparing to review key docs and skills**
exec
/bin/zsh -lc "sed -n '1,200p' docs/00_INVARIANTS.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
# Invariants（不変条件）

このドキュメントは **開発中に変えてはいけない契約（Codexも遵守）** です。  
迷ったらここへ戻り、ここに書かれていない“勝手な最適化”をしないでください。

---

## 0. 基本目的（この基盤のゴール）
- **入力**: 化学構造（主に **CSV + SDF**。将来もこの形式を軸にする）
- **出力**: 物性（LJパラメータに限らず、**同じCSV/SDF形式のまま目的変数を差し替える**）
- **操作**: **CLI**（**Hydra**で設定管理）
- **要求**:
  - 前処理・特徴量化・学習・評価・推論・可視化を **拡張しやすい**
  - モデル/特徴量の手法が複数あり、**選択して実行・比較評価しやすい**

---

## 1. もっとも重要な不変条件（破るなら RFC→ADR→更新）

### 1.1 「処理単位（Process）」が第一級（最重要）
exec
/bin/zsh -lc "sed -n '1,200p' docs/02_DATA_CONTRACTS.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
# Data Contracts（データ契約）

このドキュメントは **CSV/SDF を中心としたデータ形式の共通契約** を定義します。  
目的変数（物性）が増えても、ここを守れば基盤が壊れないようにします。

## 1. 入力データの基本形（推奨）
### 1.1 CSV（テーブル）
- 1行 = 1分子（サンプル）
- 必須列（推奨名）
  - `sample_id`：一意ID（無ければ生成）
  - `cas`：CAS番号（ある場合）
  - `smiles`：SMILES（ある場合）
  - `formula`：分子式（ある場合）
  - `sdf_path`：SDF ファイルへのパス（行ごと or 共通）
  - `target.<name>`：目的変数（例 `target.lj_sigma`, `target.lj_epsilon`）
- 任意列
  - `meta.*`：データソース、温度条件、参考値など

※ 既存CSVの列名が異なる場合は `configs/dataset/*.yaml` でマッピングする。

exec
/bin/zsh -lc "sed -n '1,200p' docs/10_PROCESS_CATALOG.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
# Process Catalog（処理単位の一覧とI/O）

このドキュメントは **Process（処理単位）** のカタログです。  
「どのProcessが存在し、何を入力として、何を出力するか」を固定し、拡張してもブレないようにします。

---

## 1. Process一覧（推奨）
| Process | Script（推奨） | 主入力 | 主出力（artifact） | ClearML化の単位 |
|---|---|---|---|---|
| build_dataset | scripts/build_dataset.py | raw CSV/SDF | dataset index/split, dataset_hash | Dataset Task |
| audit_dataset | scripts/audit_dataset.py | raw CSV/SDF or processed dataset | audit_report.json, audit_report.md, plots | Audit Task |
| featurize | scripts/featurize.py | dataset artifact | features cache/manifest | Task |
| train | scripts/train.py | dataset(+features), task, model, train cfg | model.ckpt, metrics | Training Task |
| evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
| predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
| visualize | scripts/visualize.py | predictions/metrics | plots | Reporting Task |
| collect_data | scripts/collect_data.py | API cfg | raw CSV/SDF | Data Collection Task |

※ 現在のrepoに scripts が揃っていない場合は、まず “入口を分ける” ことを P0 とする。
exec
/bin/zsh -lc "sed -n '1,200p' work/tasks/030_featurepipeline_unify.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 248ms:
# Task 030 (P0): FeaturePipeline の統一（train/infer skew 排除）

## 目的
- 学習と推論で特徴量がズレる問題を防ぐ（精度と運用の地雷）

## Plan
1) `src/common/feature_pipeline.py`（案）を作り、前処理→特徴量化の一本化
2) 学習側は pipeline を fit/transform、推論側は保存済み pipeline を load/transform
3) FP と GNN で共通化できる範囲を決め、差分は adapter で吸収
4) smoke test：同一サンプルで train→save→load→predict が一致すること

## Acceptance Criteria
- [ ] 推論時に学習と同じ pipeline が使われる
- [ ] pipeline 状態が artifact に保存される
exec
/bin/zsh -lc "sed -n '1,200p' agentskills/skills/S40_featurizer.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 255ms:
# S40 Featurizer

## Purpose
特徴量（FP/記述子/3D/埋め込み）を追加し、設定で切替できるようにする。

## Inputs
- docs/02_DATA_CONTRACTS.md
- docs/03_CONFIG_CONVENTIONS.md
- work/tasks/030_featurepipeline_unify.md または NEW_FEATURIZER タスク

## Allowed Changes
- src/common/**（FeaturePipeline）
- src/fp/**, src/gnn/**（必要なら）
- configs/features/**
- tests/**

## Steps
1) featurizer I/F を確認（fit/transform or transformのみ）
2) 学習で fit した状態を artifact 保存
3) 推論で load して transform
exec
/bin/zsh -lc "sed -n '1,200p' agentskills/skills/S30_preprocess_pipeline.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 274ms:
# S30 Preprocess Pipeline

## Purpose
Mol の前処理（sanitize、欠損処理、3D生成など）をパイプライン化し、学習/推論で共通化する。

## Inputs
- docs/00_INVARIANTS.md（skew禁止）
- work/tasks/030_featurepipeline_unify.md（または該当）

## Allowed Changes
- src/common/** or src/*/preprocess/**
- configs/preprocess/**
- tests/**

## Common Pitfalls
- 推論だけ別の前処理をしてしまう
- 3D生成の乱数・再現性を考慮しない
exec
/bin/zsh -lc "sed -n '1,200p' agentskills/skills/S95_tests_ci.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
# S95 Tests & CI

## Purpose
pytest + smoke + contract test を整備し、改修で壊れないようにする。

## Allowed Changes
- tests/**
- pyproject.toml（任意）
- .github/workflows/**（任意）

## Pitfalls
- smoke が重くなりすぎる
- 乱数により flaky になる

thinking
**Planning to review configuration docs**
exec
/bin/zsh -lc "sed -n '1,200p' docs/03_CONFIG_CONVENTIONS.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
# Hydra Config Conventions（設定規約）

このドキュメントは Hydra を用いた設定管理の規約です。  
**Process（処理単位）をHydraで統一管理**し、手法の切替・比較評価をしやすくします。

---

## 1. 設定のゴール
- **各Processが単独で実行可能**
- モデル/特徴量/タスクの組合せを安全に切替可能
- 比較評価（multirun）しても成果物が追跡可能
- 将来ClearMLで Task 化しても、同じconfigを使い回せる

---

## 2. config group 構造（必須）
- `process/`：どのProcessを実行するか（脚本ごとに固定でもよいが、共通鍵は持つ）
- `dataset/`：入力データ・列マッピング・split・フィルタ
- `task/`：目的変数/タスクタイプ（regression/classification/multitask）
- `preprocess/`：正規化/3D生成/欠損処理
exec
/bin/zsh -lc "sed -n '1,200p' docs/04_ARTIFACTS_AND_VERSIONING.md" in /Users/kawahito/Desktop/LJ_prediction succeeded in 269ms:
# Artifacts & Versioning（成果物・再現性）

このドキュメントは「各Processが独立して実行でき、比較・再現できる」ための成果物（artifact）契約です。  
将来ClearML Task化を見越し、**Process単位で入力/出力が追跡できる**形にします。

---

## 1. 出力ディレクトリ（推奨形）
Hydraの出力ディレクトリ（例：`runs/<process>/<date>/<time>_<experiment>/`）に保存する。

- `<process>` は `train`/`predict`/`evaluate` 等の処理単位
- 1 run = 1 Process 実行

---

## 2. 必須ファイル（全Process共通）
- `config.yaml`：最終 config（Hydra合成結果）
- `meta.json`：メタ情報（下記必須キー）
- `logs/`：ログ（形式は自由だが存在推奨）


thinking
**Checking task queue file**
exec
/bin/zsh -lc 'cat work/queue.json' in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
{
  "version": 1,
  "updated_at": "2025-12-26T14:09:10.649769Z",
  "policy": {
    "priority_order": [
      "P0",
      "P1",
      "P2",
      "P3"
    ],
    "status_order": [
      "todo",
      "in_progress",
      "blocked",
      "done"
    ],
    "auto_set_in_progress_on_next": true
  },
  "tasks": [
    {

thinking
**Searching codebase for preprocessing methods**
exec
/bin/zsh -lc 'rg -n "featur" src' in /Users/kawahito/Desktop/LJ_prediction succeeded in 282ms:
src/gnn/evaluate.py:17:from src.gnn.featurizer_graph import featurize_mol_to_pyg
src/gnn/evaluate.py:79:            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
src/gnn/evaluate.py:139:            "featureset_name": train_context.get("featureset_name"),
src/gnn/evaluate.py:147:    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
src/gnn/predict.py:17:from src.fp.featurizer_fp import morgan_bitvect
src/gnn/predict.py:18:from src.gnn.featurizer_graph import featurize_mol_to_pyg
src/gnn/predict.py:88:            "featureset_name": train_context.get("featureset_name"),
src/gnn/predict.py:101:    # Load featurizer config
src/gnn/predict.py:102:    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
src/gnn/predict.py:105:    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
src/utils/artifacts.py:23:    "featureset_name",
src/utils/artifacts.py:106:def resolve_featureset_name(cfg: Optional[Dict[str, Any]]) -> Optional[str]:
src/utils/artifacts.py:109:    if cfg.get("featureset_name"):
src/utils/artifacts.py:110:        return str(cfg["featureset_name"])
src/utils/artifacts.py:111:    featureset_cfg = cfg.get("featureset", {})
src/utils/artifacts.py:112:    if isinstance(featureset_cfg, dict) and featureset_cfg.get("name"):
src/utils/artifacts.py:113:        return str(featureset_cfg["name"])
src/utils/artifacts.py:114:    features_cfg = cfg.get("features", {})
src/utils/artifacts.py:115:    if isinstance(features_cfg, dict) and features_cfg.get("name"):
src/utils/artifacts.py:116:        return str(features_cfg["name"])

thinking
**Exploring preprocess pipeline**
exec
/bin/zsh -lc 'rg -n "preprocess|sanitize|3D" src' in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
src/gnn/featurizer_graph.py:105:    # 3D positions
src/gnn/featurizer_graph.py:109:        if conf is not None and conf.Is3D():
src/fp/train.py:13:from sklearn.preprocessing import StandardScaler
src/fp/train.py:92:    preprocess_cfg = cfg.get("preprocess", {})
src/fp/train.py:93:    standardize = bool(preprocess_cfg.get("standardize", False))
src/fp/train.py:94:    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
src/fp/train.py:278:    with open(model_dir / "preprocess.pkl", "wb") as f:
src/fp/predict.py:107:    # Load preprocessing + model

thinking
**Inspecting training and prediction files**
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

from src.common.ad import applicability_domain
from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.metrics import regression_metrics
from src.common.plots import save_parity_plot, save_residual_plot, save_hist
exec
/bin/zsh -lc "sed -n '1,240p' src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 260ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

exec
/bin/zsh -lc "sed -n '1,240p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.metrics import regression_metrics
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.feature_utils import build_features, hash_cfg
from src.fp.featurizer_fp import FPConfig
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

exec
/bin/zsh -lc "sed -n '1,220p' src/gnn/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
from __future__ import annotations

import argparse
import pickle
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.metrics import regression_metrics
from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json, set_seed
from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
exec
/bin/zsh -lc "sed -n '1,200p' src/gnn/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 256ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import morgan_bitvect
from src.gnn.featurizer_graph import featurize_mol_to_pyg
from src.gnn.models import GCNRegressor, MPNNRegressor
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
exec
/bin/zsh -lc "sed -n '1,200p' src/gnn/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 242ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd

from src.common.config import dump_yaml, load_config
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.meta import build_meta, save_meta
from src.common.metrics import regression_metrics
from src.common.splitters import load_split_indices
from src.common.utils import ensure_dir, get_logger, save_json
from src.gnn.featurizer_graph import featurize_mol_to_pyg
from src.gnn.models import GCNRegressor, MPNNRegressor
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

thinking
**Continuing GNN training exploration**
exec
/bin/zsh -lc "sed -n '220,520p' src/gnn/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:
            num_layers=num_layers,
            dropout=dropout,
            global_dim=global_dim,
            edge_mlp_hidden_dim=edge_mlp_hidden_dim,
        )
    else:
        raise ValueError(f"Unknown model.name: {model_name}")

    train_cfg = cfg.get("train", {})
    device = _select_device(train_cfg)
    num_threads = train_cfg.get("num_threads", None)
    if num_threads is not None:
        try:
            torch.set_num_threads(int(num_threads))
        except Exception:
            pass
    n_params = int(sum(p.numel() for p in model.parameters() if p.requires_grad))
    logger.info(
        f"Model params: {n_params/1e6:.2f}M (~{(n_params*4)/1e6:.1f} MB fp32, excluding optimizer state)"
    )
exec
/bin/zsh -lc "sed -n '200,420p' src/gnn/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
    print("=" * 70)

    result = {
        "cas": cas,
        "query": query,
        "prediction": pred,
        "resolve_meta": resolve_meta,
        "ad": None if ad_res is None else ad_res.to_dict(),
    }
    save_json(out_dir / f"prediction_{cas}.json", result)
    logger.info(f"Saved prediction json to {out_dir / f'prediction_{cas}.json'}")
    return out_dir


def main() -> None:
    ap = argparse.ArgumentParser(description="Predict LJ parameter using GNN model with applicability-domain diagnostics.")
    ap.add_argument("--config", required=True, help="Path to configs/gnn/predict.yaml")
    ap.add_argument("--query", required=True, help="CAS or formula depending on config.input.mode")
    args = ap.parse_args()

exec
/bin/zsh -lc "sed -n '200,520p' src/gnn/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 266ms:
            return np.array([]), np.array([])
        loader = DataLoader(data_list, **loader_kwargs)
        ys, ps = [], []
        with torch.no_grad():
            for batch in loader:
                batch = batch.to(device)
                pred = model(batch).detach().cpu().numpy().reshape(-1)
                y = batch.y.view(-1).detach().cpu().numpy()
                ys.extend(y.tolist())
                ps.extend(pred.tolist())
        return np.asarray(ys, dtype=float), np.asarray(ps, dtype=float)

    rows = []
    metrics_by_split: Dict[str, Dict[str, float]] = {}

    for split_name, data_list, ids in [
        ("train", train_data, train_ids),
        ("val", val_data, val_ids),
        ("test", test_data, test_ids),
    ]:

thinking
**Investigating pipeline steps in fp train**
exec
/bin/zsh -lc "sed -n '220,520p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 267ms:

    # Save artifacts
    artifacts_dir = ensure_dir(run_dir / "artifacts")
    model_path = artifacts_dir / "model.pkl"
    with open(model_path, "wb") as f:
        pickle.dump(model, f)

    with open(artifacts_dir / "imputer.pkl", "wb") as f:
        pickle.dump(imputer, f)
    if scaler is not None:
        with open(artifacts_dir / "scaler.pkl", "wb") as f:
            pickle.dump(scaler, f)

    # AD artifacts: training fingerprints and training elements
    # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
    # We'll store only training set for speed.
    train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
    # Filter None
    train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]

thinking
**Evaluating preprocessing approaches**
exec
/bin/zsh -lc 'rg --files src/common' in /Users/kawahito/Desktop/LJ_prediction succeeded in 281ms:
src/common/meta.py
src/common/ad.py
src/common/chemistry.py
src/common/utils.py
src/common/plots.py
src/common/descriptors.py
src/common/splitters.py
src/common/__init__.py
src/common/io.py
src/common/dataset_selectors.py
src/common/lj.py
src/common/config.py
src/common/metrics.py

thinking
**Investigating preprocessing components**
exec
/bin/zsh -lc "sed -n '1,240p' src/fp/feature_utils.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
from __future__ import annotations

import hashlib
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from src.common.io import load_sdf_mol, sdf_path_from_cas
from src.common.utils import ensure_dir
from src.fp.featurizer_fp import FPConfig, featurize_mol


def hash_cfg(obj: Dict[str, Any]) -> str:
    s = repr(obj).encode("utf-8")
    return hashlib.sha256(s).hexdigest()[:12]

exec
/bin/zsh -lc "sed -n '1,260p' src/fp/featurizer_fp.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Sequence, Tuple

import numpy as np

from src.common.descriptors import calc_descriptors, descriptors_to_array

try:
    from rdkit import Chem, DataStructs
    from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors
except Exception:  # pragma: no cover
    Chem = None
    DataStructs = None
    AllChem = None
    MACCSkeys = None
    rdMolDescriptors = None


exec
/bin/zsh -lc "sed -n '1,260p' src/gnn/featurizer_graph.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence

import numpy as np

from src.common.descriptors import calc_descriptors, descriptors_to_array

try:
    import torch
    from torch_geometric.data import Data
except Exception:  # pragma: no cover
    torch = None
    Data = None

try:
    from rdkit import Chem
except Exception:  # pragma: no cover
    Chem = None

thinking
**Designing unified preprocessing pipeline**
exec
/bin/zsh -lc 'rg --files configs | head -n 200' in /Users/kawahito/Desktop/LJ_prediction succeeded in 277ms:
configs/process/visualize.yaml
configs/process/evaluate.yaml
configs/process/build_dataset.yaml
configs/process/train.yaml
configs/process/predict.yaml
configs/process/audit_dataset.yaml
configs/audit_dataset_quick.yaml
configs/hydra/default.yaml
configs/dataset.yaml
configs/eval/default.yaml
configs/eval/quick.yaml
configs/infer/fp_default.yaml
configs/infer/gnn_default.yaml
configs/infer/fp_quick.yaml
configs/infer/gnn_quick.yaml
configs/train/gnn_mpnn_quick.yaml
configs/train/fp_default.yaml
configs/train/gnn_default.yaml
configs/train/gnn_quick.yaml
configs/model/fp_lightgbm_quick.yaml

thinking
**Inspecting preprocessing configs**
exec
/bin/zsh -lc "sed -n '1,200p' configs/preprocess/fp_default.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 240ms:
preprocess:
  impute_nan: "mean"
  standardize: false
exec
/bin/zsh -lc "sed -n '1,200p' configs/preprocess/none.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 257ms:
preprocess: {}
exec
/bin/zsh -lc "sed -n '1,200p' configs/features/fp_morgan_desc.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 297ms:
featurizer:
  fingerprint: "morgan"
  morgan_radius: 2
  n_bits: 2048
  use_counts: false
  add_descriptors: ["MolWt","TPSA","MolLogP","HBD","HBA","NumRotatableBonds","RingCount","AromaticRings"]

data:
  cache_dir: "data/processed/cache/fp"
exec
/bin/zsh -lc "sed -n '1,200p' configs/features/gnn_graph.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
featurizer:
  node_features: ["atomic_num","degree","formal_charge","aromatic","num_h","in_ring"]
  edge_features: ["bond_type","conjugated","aromatic"]
  use_3d_pos: true
  add_global_descriptors: ["MolWt","TPSA","MolLogP","HBD","HBA"]

thinking
**Unifying preprocessing pipeline**
exec
/bin/zsh -lc 'rg -n "preprocess.pkl|imputer.pkl|scaler.pkl" -S' in /Users/kawahito/Desktop/LJ_prediction succeeded in 319ms:
src/fp/evaluate.py:104:    with open(artifacts_dir / "imputer.pkl", "rb") as f:
src/fp/evaluate.py:107:    scaler_path = artifacts_dir / "scaler.pkl"
src/fp/train.py:227:    with open(artifacts_dir / "imputer.pkl", "wb") as f:
src/fp/train.py:230:        with open(artifacts_dir / "scaler.pkl", "wb") as f:
src/fp/train.py:278:    with open(model_dir / "preprocess.pkl", "wb") as f:
src/fp/predict.py:108:    with open(artifacts_dir / "imputer.pkl", "rb") as f:
src/fp/predict.py:111:    scaler_path = artifacts_dir / "scaler.pkl"
docs/04_ARTIFACTS_AND_VERSIONING.md:39:  - `preprocess.pkl`（必要なら：scaler/imputer等）
work/REPO_ORIENTATION.md:43:| scripts/train.py | configs/fp/train*.yaml | data/processed/dataset_with_lj.csv, data/processed/indices, data/raw/sdf_files | runs/train/fp/<exp>/artifacts/{model.pkl,imputer.pkl,scaler.pkl?,ad.pkl}, runs/train/fp/<exp>/plots/*.png, runs/train/fp/<exp>/metrics_{val,test}.json, runs/train/fp/<exp>/config_snapshot.yaml, runs/train/fp/<exp>/train.log |
README.md:113:  - `runs/train/fp/<exp_name>/artifacts/imputer.pkl`（欠損対策）
objective.md:61:  - `artifacts/imputer.pkl`（欠損補完）
objective.md:62:  - `artifacts/scaler.pkl`（標準化を有効にした場合）

thinking
**Planning feature pipeline unification**
exec
/bin/zsh -lc 'rg --files tests' in /Users/kawahito/Desktop/LJ_prediction succeeded in 275ms:
tests/test_fixture_dataset.py
tests/fixtures/data/raw/tc_pc_tb_fixture.csv
tests/fixtures/data/raw/sdf_files/64-17-5.sdf
tests/fixtures/data/raw/sdf_files/67-64-1.sdf
tests/fixtures/data/raw/sdf_files/124-38-9.sdf
tests/fixtures/data/raw/sdf_files/110-82-7.sdf
tests/fixtures/data/raw/sdf_files/74-98-6.sdf
tests/fixtures/data/raw/sdf_files/74-85-1.sdf
tests/fixtures/data/processed/dataset_config_snapshot.yaml
tests/fixtures/data/processed/dataset_with_lj.csv
tests/fixtures/data/processed/indices/val.txt
tests/fixtures/data/processed/indices/test.txt
tests/fixtures/data/processed/indices/train.txt
tests/test_config_loader.py
tests/contract/test_artifacts_contract.py
tests/conftest.py
tests/test_audit_dataset.py

thinking
**Checking test coverage for pipelines**
exec
/bin/zsh -lc "sed -n '1,220p' tests/conftest.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 256ms:
from __future__ import annotations

import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))
exec
/bin/zsh -lc "sed -n '1,220p' tests/test_fixture_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
from __future__ import annotations

from pathlib import Path

import pandas as pd


def test_fixture_dataset_files_exist() -> None:
    root = Path(__file__).resolve().parents[1]
    csv_path = root / "tests" / "fixtures" / "data" / "raw" / "tc_pc_tb_fixture.csv"
    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"

    assert csv_path.exists()
    assert sdf_dir.exists()

    df = pd.read_csv(csv_path)
    required_cols = {"CAS", "MolecularFormula", "Tc [K]", "Pc [Pa]", "Tb [K]"}
    assert required_cols.issubset(set(df.columns))

    missing = []
exec
/bin/zsh -lc "sed -n '1,260p' tests/test_config_loader.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 258ms:
from pathlib import Path

from src.common.config import load_config


def test_load_config_resolves_groups_from_root() -> None:
    cfg = load_config(Path("configs/fp/train_fixture.yaml"))

    assert cfg["process"]["name"] == "train"
    assert cfg["process"]["backend"] == "fp"
    assert cfg["output"]["run_dir"] == "runs/train/fp"
    assert cfg["paths"]["raw_csv"] == "tests/fixtures/data/raw/tc_pc_tb_fixture.csv"
exec
/bin/zsh -lc "sed -n '1,260p' tests/contract/test_artifacts_contract.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 259ms:
from __future__ import annotations

from pathlib import Path

from src.utils.artifacts import (
    REQUIRED_META_KEYS,
    build_meta,
    compute_config_hash,
    compute_dataset_hash,
    save_meta,
    validate_evaluate_artifacts,
    validate_predict_artifacts,
    validate_train_artifacts,
)


def _write_indices(indices_dir: Path) -> None:
    indices_dir.mkdir(parents=True, exist_ok=True)
    (indices_dir / "train.txt").write_text("0\n", encoding="utf-8")
    (indices_dir / "val.txt").write_text("0\n", encoding="utf-8")
exec
/bin/zsh -lc "sed -n '1,260p' tests/test_audit_dataset.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 268ms:
from __future__ import annotations

from pathlib import Path

import numpy as np
import pandas as pd
import pytest

rdkit = pytest.importorskip("rdkit")
from rdkit import Chem

from src.data.audit import audit_dataset


def _write_sdf(path: Path, smiles: str) -> None:
    mol = Chem.MolFromSmiles(smiles)
    assert mol is not None
    writer = Chem.SDWriter(str(path))
    writer.write(mol)
    writer.close()

thinking
**Checking pipeline test coverage**
exec
/bin/zsh -lc "sed -n '1,260p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
from __future__ import annotations

import csv
import hashlib
import json
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Optional
from uuid import uuid4

from src.common.utils import save_json

REQUIRED_META_KEYS = [
    "run_id",
    "process_name",
    "created_at",
    "git_sha",
    "dataset_hash",
    "config_hash",
exec
/bin/zsh -lc "sed -n '260,520p' src/utils/artifacts.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:
    if not header:
        raise ValueError(f"predictions.csv missing header: {path}")
    missing = [c for c in REQUIRED_PREDICTION_COLUMNS if c not in header]
    if missing:
        raise ValueError(f"predictions.csv missing columns: {', '.join(missing)}")


def validate_common_artifacts(run_dir: str | Path) -> None:
    run_dir = Path(run_dir)
    config_path = run_dir / "config.yaml"
    meta_path = run_dir / "meta.json"
    if not config_path.exists():
        raise FileNotFoundError(f"config.yaml not found: {config_path}")
    if not meta_path.exists():
        raise FileNotFoundError(f"meta.json not found: {meta_path}")
    validate_meta(load_meta(meta_path))


def validate_train_artifacts(run_dir: str | Path) -> None:
    run_dir = Path(run_dir)

thinking
**Designing unified feature pipeline**

I'm planning a new feature pipeline to unify preprocessing and featurization for fingerprint (FP) and GNN workflows, saving pipeline state as an artifact. This involves creating base and specialized pipeline classes to handle dataset and single-molecule inputs, integrating existing featurizers, preprocessors, and ensuring compatibility with current configs and training scripts. I'll explore how to structure this for extensibility and consistency without breaking existing CLI or caching.
exec
/bin/zsh -lc "rg -n \"fp\\.train|gnn\\.train|scripts/train.py|scripts/predict.py|scripts/evaluate.py\" -S" in /Users/kawahito/Desktop/LJ_prediction succeeded in 303ms:
docs/03_CONFIG_CONVENTIONS.md:36:ただし実装は自由（`scripts/train.py` に固定でもよい）。  
docs/03_CONFIG_CONVENTIONS.md:52:  - `python scripts/train.py -m model=mpnn,gcn features=morgan,rdkit train.seed=0`
scripts/evaluate.py:7:# Allow running as `python scripts/evaluate.py ...` without installing the package.
docs/10_PROCESS_CATALOG.md:14:| train | scripts/train.py | dataset(+features), task, model, train cfg | model.ckpt, metrics | Training Task |
docs/10_PROCESS_CATALOG.md:15:| evaluate | scripts/evaluate.py | model.ckpt + dataset | metrics, predictions | Evaluation Task |
docs/10_PROCESS_CATALOG.md:16:| predict | scripts/predict.py | model.ckpt + new input | predictions | Inference Task |
scripts/train.py:8:# Allow running as `python scripts/train.py ...` without installing the package.
docs/01_ARCHITECTURE.md:49:  - 例: `python scripts/train.py -m model=mpnn,gcn features=morgan,rdkit`
scripts/predict.py:8:# Allow running as `python scripts/predict.py ...` without installing the package.
docs/adr/0001_hydra_cli_entrypoints.md:10:- 入口は `scripts/train.py`, `scripts/predict.py`, `scripts/evaluate.py` を基本とする
README.md:108:python3 scripts/train.py --config configs/fp/train.yaml
README.md:119:python3 scripts/predict.py --config configs/fp/predict.yaml --query C6H6
README.md:121:python3 scripts/predict.py --config configs/fp/predict.yaml --query 71-43-2
README.md:132:python3 scripts/train.py --config configs/gnn/train.yaml
README.md:137:python3 scripts/predict.py --config configs/gnn/predict.yaml --query C6H6
README.md:217:python scripts/train.py --config configs/fp/train_fixture.yaml
work/REPO_ORIENTATION.md:28:  -> scripts/train.py (FP config)
work/REPO_ORIENTATION.md:30:  -> scripts/train.py (GNN config)
work/REPO_ORIENTATION.md:34:  -> scripts/predict.py
work/REPO_ORIENTATION.md:43:| scripts/train.py | configs/fp/train*.yaml | data/processed/dataset_with_lj.csv, data/processed/indices, data/raw/sdf_files | runs/train/fp/<exp>/artifacts/{model.pkl,imputer.pkl,scaler.pkl?,ad.pkl}, runs/train/fp/<exp>/plots/*.png, runs/train/fp/<exp>/metrics_{val,test}.json, runs/train/fp/<exp>/config_snapshot.yaml, runs/train/fp/<exp>/train.log |

thinking
**Inspecting training and prediction scripts**
exec
/bin/zsh -lc "sed -n '1,200p' scripts/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict

# Allow running as `python scripts/train.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import train as fp_train
from src.gnn import train as gnn_train


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}

exec
/bin/zsh -lc "sed -n '1,200p' scripts/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 244ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict

# Allow running as `python scripts/predict.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import predict as fp_predict
from src.gnn import predict as gnn_predict


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}

exec
/bin/zsh -lc "sed -n '1,200p' scripts/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 255ms:
from __future__ import annotations

import argparse
import sys
from pathlib import Path

# Allow running as `python scripts/evaluate.py ...` without installing the package.
REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from src.common.config import load_config
from src.fp import evaluate as fp_evaluate
from src.gnn import evaluate as gnn_evaluate


_FP_MODELS = {"lightgbm", "lgbm", "rf", "catboost", "gpr"}
_GNN_MODELS = {"gcn", "mpnn"}



thinking
**Analyzing pipeline integration and config**
exec
/bin/zsh -lc "sed -n '1,260p' src/common/io.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 311ms:
from __future__ import annotations

from pathlib import Path
from typing import Optional

import pandas as pd

try:
    from rdkit import Chem
except Exception:  # pragma: no cover
    Chem = None


def read_csv(path: str | Path) -> pd.DataFrame:
    return pd.read_csv(Path(path))


def write_csv(df: pd.DataFrame, path: str | Path) -> None:
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)

thinking
**Considering preprocessing options**
exec
/bin/zsh -lc 'rg -n "AllChem|Embed|ETKDG|UFF|sanitize" src' in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
src/fp/featurizer_fp.py:12:    from rdkit.Chem import AllChem, MACCSkeys, rdMolDescriptors
src/fp/featurizer_fp.py:16:    AllChem = None

thinking
**Considering MolPreprocessor integration and pipeline unification**

I'm thinking about implementing a minimal MolPreprocessor with optional sanitize and 3D generation flags, making it an optional part of the existing preprocess config group without changing defaults. I want to unify the feature pipeline so training and inference use consistent processing, avoiding skew especially around fingerprinting and graph featurization. There’s a plan to create a common pipeline object to formalize state and guarantee parity. I’ll look into related docs and config validation to see if modifications are needed, and will use the planning tool for this moderate-complexity task since it involves multiple steps and must align with autopilot instructions.
exec
/bin/zsh -lc "sed -n '1,260p' src/utils/validate_config.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 272ms:
from __future__ import annotations

from typing import Any, Dict, Iterable

from src.common.config import ConfigError


def _require_keys(cfg: Dict[str, Any], keys: Iterable[str], context: str) -> None:
    missing = [k for k in keys if k not in cfg]
    if missing:
        raise ConfigError(f"{context} missing keys: {', '.join(missing)}")


def _has_target(cfg: Dict[str, Any]) -> bool:
    data = cfg.get("data", {})
    task = cfg.get("task", {})
    return bool(data.get("target_col") or task.get("target_col"))


def validate_config(cfg: Dict[str, Any]) -> None:

thinking
**Designing unified feature pipeline module**

I'm outlining a new `feature_pipeline.py` module to unify fingerprint and graph feature processing with consistent save/load, fit/transform, and config management while keeping backward compatibility and handling optional dependencies carefully. I'll also consider updating docs and tests to integrate this clean design.
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/train.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 327ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_desc
  - model: fp_lightgbm
  - train: fp_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/predict.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 256ms:
defaults:
  - process: predict
  - dataset: default
  - infer: fp_default
  - hydra: default
  - _self_

model_artifact_dir: "runs/train/fp/lgbm_morgan_desc_v1"

process:
  backend: "fp"

output:
  exp_name: "fp_predict"
exec
/bin/zsh -lc "sed -n '1,200p' configs/gnn/train.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 265ms:
defaults:
  - process: train
  - dataset: default
  - task: lj_epsilon
  - preprocess: none
  - features: gnn_graph
  - model: gnn_mpnn
  - train: gnn_default
  - eval: default
  - hydra: default
  - _self_

experiment:
  name: "mpnn_desc_v1"

process:
  backend: "gnn"

output:
  run_dir: "runs/train/gnn"
exec
/bin/zsh -lc "sed -n '1,200p' configs/gnn/predict.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 271ms:
defaults:
  - process: predict
  - dataset: default
  - infer: gnn_default
  - hydra: default
  - _self_

model_artifact_dir: "runs/train/gnn/mpnn_desc_v1"

process:
  backend: "gnn"

output:
  exp_name: "gnn_predict"
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/train_fixture.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
defaults:
  - process: train
  - dataset: fixture
  - task: lj_epsilon
  - preprocess: fp_default
  - features: fp_morgan_fixture
  - model: fp_rf_fixture
  - train: fp_default
  - eval: quick
  - hydra: default
  - _self_

experiment:
  name: "fixture_fp_rf"

process:
  backend: "fp"

output:
  run_dir: "runs/train/fp"
exec
/bin/zsh -lc "sed -n '1,200p' configs/fp/predict_fixture.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 264ms:
defaults:
  - process: predict
  - dataset: fixture
  - infer: fp_quick
  - hydra: default
  - _self_

model_artifact_dir: "runs/train/fp/fixture_fp_rf"

process:
  backend: "fp"

experiment:
  name: "fixture_fp_predict"

output:
  exp_name: "fixture_fp_predict"

thinking
**Inspecting configuration files**
exec
/bin/zsh -lc "sed -n '1,200p' configs/infer/fp_default.yaml" in /Users/kawahito/Desktop/LJ_prediction succeeded in 276ms:
input:
  mode: "formula"



thinking
**Designing feature pipeline integration**

I’m unpacking how to unify and save the feature pipelines for fingerprint and graph models within training artifacts, ensuring backward compatibility with existing imputer/scaler files. I’ll focus on saving/loading `feature_pipeline.pkl` in the artifacts directory, updating training to fit and save pipelines properly without data leakage, and planning to add tests to verify pipeline save/load consistency. This way, inference can reliably use the saved pipeline and maintain consistent feature processing across all stages.
thinking
**Extending pipeline interface and artifact handling**

I’m detailing enhancements to feature pipelines—adding meta handling, unified save/load functions, and consistent config parsing to support fingerprint and graph models. This includes methods like `from_config`, `build_features`, and fallback logic to maintain backwards compatibility, plus plans to integrate these cleanly into training and inference workflows. I’m also ensuring the pipeline objects remain picklable and versioned for safe artifact management.
thinking
**Designing test for feature pipeline roundtrip**

I’m sketching a test to verify the fingerprint pipeline can build features from a small molecule dataset, fit preprocessing steps, save and reload correctly, and produce consistent transformed features before and after loading. I’ll use fixture molecules with RDKit, apply imputation and scaling, and ensure outputs match closely to confirm reliable persistence and transformation.
Plan update
Unify preprocessing/featurization via a shared pipeline, then wire it through train/evaluate/predict and cover with a lightweight pipeline roundtrip test.
  → Create feature pipeline module with FP/GNN adapters, state save/load helpers, and backward-compatible artifact loading hooks.
  • Refactor FP/GNN train/evaluate/predict to use the pipeline for featurization + preprocessing and to persist pipeline artifacts.
  • Add a smoke test for pipeline save/load consistency and update queue status.

thinking
**Designing feature_pipeline module**
exec
/bin/zsh -lc 'cat requirements.txt' in /Users/kawahito/Desktop/LJ_prediction succeeded in 288ms:
# Core
numpy
pandas
pyyaml
tqdm
matplotlib
scikit-learn
joblib

# Chemistry
rdkit  # usually installed via conda-forge; pip availability depends on platform

# FP models
lightgbm
catboost

# GNN (optional; install matching versions manually)
torch
# torch-geometric  # install via official instructions

thinking
**Preparing to implement patch**
file update
A /Users/kawahito/Desktop/LJ_prediction/src/common/feature_pipeline.py
from __future__ import annotations

import pickle
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

from src.fp.feature_utils import build_features
from src.fp.featurizer_fp import FPConfig, featurize_mol
from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg

PIPELINE_VERSION = 1
FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"


class FeaturePipelineError(RuntimeError):
    pass


def _normalize_list(value: Any) -> Optional[list[str]]:
    if value is None:
        return None
    if isinstance(value, (list, tuple)):
        return [str(v) for v in value]
    return [str(value)]


def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
    return FPConfig(
        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
        n_bits=int(feat_cfg.get("n_bits", 2048)),
        use_counts=bool(feat_cfg.get("use_counts", False)),
        add_descriptors=add_desc,
    )


def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
    node_features = feat_cfg.get(
        "node_features",
        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
    )
    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
    return GraphFeaturizerConfig(
        node_features=[str(v) for v in node_features],
        edge_features=[str(v) for v in edge_features],
        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
    )


@dataclass
class FingerprintFeaturePipeline:
    fp_cfg: FPConfig = field(default_factory=FPConfig)
    impute_strategy: str = "mean"
    standardize: bool = False
    imputer: Optional[SimpleImputer] = None
    scaler: Optional[StandardScaler] = None
    feature_meta: Optional[Dict[str, Any]] = None
    pipeline_type: str = field(default="fp", init=False)
    version: int = field(default=PIPELINE_VERSION, init=False)

    @classmethod
    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
        feat_cfg = cfg.get("featurizer", {}) or {}
        preprocess_cfg = cfg.get("preprocess", {}) or {}
        return cls(
            fp_cfg=build_fp_config(feat_cfg),
            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
            standardize=bool(preprocess_cfg.get("standardize", False)),
        )

    def build_features(
        self,
        df,
        sdf_dir: Path,
        cas_col: str,
        cache_dir: Optional[Path],
        cache_key: str,
        logger,
    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
        X, ids, elements, meta = build_features(
            df=df,
            sdf_dir=sdf_dir,
            cas_col=cas_col,
            fp_cfg=self.fp_cfg,
            cache_dir=cache_dir,
            cache_key=cache_key,
            logger=logger,
        )
        self.feature_meta = meta
        return X, ids, elements, meta

    def fit(self, X: np.ndarray) -> None:
        self.imputer = SimpleImputer(strategy=self.impute_strategy)
        X_imp = self.imputer.fit_transform(X)
        if self.standardize:
            self.scaler = StandardScaler()
            self.scaler.fit(X_imp)
        else:
            self.scaler = None

    def transform_features(self, X: np.ndarray) -> np.ndarray:
        if self.imputer is None:
            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
        X_imp = self.imputer.transform(X)
        if self.scaler is not None:
            X_imp = self.scaler.transform(X_imp)
        return X_imp

    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
        self.fit(X)
        return self.transform_features(X)

    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
        return featurize_mol(mol, self.fp_cfg)

    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
        x, meta = self.featurize_mol(mol)
        X = self.transform_features(x.reshape(1, -1))
        return X.reshape(-1), meta

    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
        artifacts_dir = Path(artifacts_dir)
        artifacts_dir.mkdir(parents=True, exist_ok=True)
        if self.imputer is None:
            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
        with open(artifacts_dir / "imputer.pkl", "wb") as f:
            pickle.dump(self.imputer, f)
        if self.scaler is not None:
            with open(artifacts_dir / "scaler.pkl", "wb") as f:
                pickle.dump(self.scaler, f)

    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
        artifacts_dir = Path(artifacts_dir)
        imputer_path = artifacts_dir / "imputer.pkl"
        if not imputer_path.exists():
            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
        with open(imputer_path, "rb") as f:
            self.imputer = pickle.load(f)
        scaler_path = artifacts_dir / "scaler.pkl"
        if scaler_path.exists():
            with open(scaler_path, "rb") as f:
                self.scaler = pickle.load(f)
        else:
            self.scaler = None


@dataclass
class GraphFeaturePipeline:
    graph_cfg: GraphFeaturizerConfig
    pipeline_type: str = field(default="gnn", init=False)
    version: int = field(default=PIPELINE_VERSION, init=False)

    @classmethod
    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
        feat_cfg = cfg.get("featurizer", {}) or {}
        return cls(build_graph_config(feat_cfg))

    @classmethod
    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
        artifacts_dir = Path(artifacts_dir)
        cfg_path = artifacts_dir / "graph_featurizer.pkl"
        if not cfg_path.exists():
            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
        with open(cfg_path, "rb") as f:
            gcfg = pickle.load(f)
        if not isinstance(gcfg, GraphFeaturizerConfig):
            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
        return cls(graph_cfg=gcfg)

    def featurize_mol(self, mol, y: Optional[float] = None):
        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)


def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
    artifacts_dir = Path(artifacts_dir)
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
    with open(path, "wb") as f:
        pickle.dump(pipeline, f)
    return path


def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
    artifacts_dir = Path(artifacts_dir)
    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
    if not path.exists():
        return None
    with open(path, "rb") as f:
        return pickle.load(f)
apply_patch(auto_approved=true) exited 0 in 240ms:
Success. Updated the following files:
A src/common/feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)


thinking
**Updating plan progress**
Plan update
  ✓ Create feature pipeline module with FP/GNN adapters, state save/load helpers, and backward-compatible artifact loading hooks.
  → Refactor FP/GNN train/evaluate/predict to use the pipeline for featurization + preprocessing and to persist pipeline artifacts.
  • Add a smoke test for pipeline save/load consistency and update queue status.
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)


thinking
**Refactoring fingerprint training pipeline imports and setup**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -11,7 +11,5 @@
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
@@ -22,4 +20,4 @@
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
@@ -83,13 +81,4 @@
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
@@ -101,3 +90,3 @@
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
@@ -139,15 +128,8 @@
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
-
     model = get_model(model_name, model_params)
@@ -222,2 +204,4 @@
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
@@ -225,8 +209,2 @@
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
@@ -236,3 +214,5 @@
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
@@ -280,6 +260,6 @@
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
apply_patch(auto_approved=true) exited 0 in 38ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..d3c9eae106907f6c7355e6319ecbf64de1809e1b
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -137,19 +126,12 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
-
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
     fit_kwargs = {}
@@ -220,21 +202,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +258,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..d3c9eae106907f6c7355e6319ecbf64de1809e1b
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -137,19 +126,12 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
-
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
     fit_kwargs = {}
@@ -220,21 +202,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +258,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Verifying parameter consistency in build_features call**
exec
/bin/zsh -lc "rg -n \"build_features\\(\" -n src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 331ms:
91:    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..d3c9eae106907f6c7355e6319ecbf64de1809e1b
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -137,19 +126,12 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
-
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
     fit_kwargs = {}
@@ -220,21 +202,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +258,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )

exec
/bin/zsh -lc "sed -n '80,120p' src/fp/train.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 281ms:

    feat_cfg = cfg.get("featurizer", {})
    pipeline = FingerprintFeaturePipeline.from_config(cfg)
    fp_cfg = pipeline.fp_cfg

    model_cfg = cfg.get("model", {})
    model_name = str(model_cfg.get("name", "lightgbm"))
    model_params = model_cfg.get("params", {}) or {}

    # Build features for all rows once (for caching + AD)
    cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
        df=df,
        sdf_dir=sdf_dir,
        cas_col=cas_col,
        fp_cfg=fp_cfg,
        cache_dir=cache_dir,
        cache_key=cache_key,
        logger=logger,
    )
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..d3c9eae106907f6c7355e6319ecbf64de1809e1b
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,7 +88,7 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
@@ -137,19 +126,12 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
-
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
     fit_kwargs = {}
@@ -220,21 +202,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +258,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Removing unused fp_cfg parameter**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/train.py
@@ -94,3 +94,2 @@
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
apply_patch(auto_approved=true) exited 0 in 45ms:
Success. Updated the following files:
M src/fp/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Refactoring feature pipeline imports and usage**

I’m cleaning up `src/fp/predict.py` by removing unused imports like `FPConfig` and `featurize_mol` while switching to use the fingerprint pipeline object for feature transformation. I’ll update pipeline loading with a safe `isinstance` check to fallback to config loading if needed and simplify the AD fingerprinting part by keeping only what's necessary. This should streamline feature handling and remove obsolete variables.
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Reopening code portion for patching**
exec
/bin/zsh -lc "sed -n '1,220p' src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 305ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Applying precise patch to context**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/predict.py
@@ -13,2 +13,3 @@
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
@@ -16,3 +17,3 @@
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
@@ -95,30 +96,11 @@
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
apply_patch(auto_approved=true) exited 0 in 29ms:
Success. Updated the following files:
M src/fp/predict.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Updating FP evaluate with pipeline integration**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/evaluate.py
@@ -11,2 +11,3 @@
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
@@ -15,4 +16,3 @@
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
@@ -80,10 +80,7 @@
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
@@ -93,3 +90,3 @@
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
@@ -97,3 +94,2 @@
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
@@ -102,10 +98,2 @@
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
@@ -135,5 +123,3 @@
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
apply_patch(auto_approved=true) exited 0 in 38ms:
Success. Updated the following files:
M src/fp/evaluate.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )


thinking
**Replacing direct featurizer imports with pipeline usage**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/train.py
@@ -13,2 +13,3 @@
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
@@ -19,3 +20,2 @@
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
@@ -170,9 +170,4 @@
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
@@ -186,3 +181,3 @@
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
@@ -388,2 +383,3 @@
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
apply_patch(auto_approved=true) exited 0 in 40ms:
Success. Updated the following files:
M src/gnn/train.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Refactoring graph featurizer loading**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/predict.py
@@ -13,2 +13,3 @@
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
@@ -17,3 +18,2 @@
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
@@ -100,7 +100,8 @@
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    gcfg = pipeline.graph_cfg
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    data = pipeline.featurize_mol(mol, y=None)
 
apply_patch(auto_approved=true) exited 0 in 42ms:
Success. Updated the following files:
M src/gnn/predict.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..9a9c129396b9f32f2c1c6467c3585af5231158bf
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +98,12 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    gcfg = pipeline.graph_cfg
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..9a9c129396b9f32f2c1c6467c3585af5231158bf
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +98,12 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    gcfg = pipeline.graph_cfg
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Investigating variable usage**
exec
/bin/zsh -lc 'rg -n "gcfg" src/gnn/predict.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 299ms:
104:    gcfg = pipeline.graph_cfg
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..9a9c129396b9f32f2c1c6467c3585af5231158bf
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +98,12 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    gcfg = pipeline.graph_cfg
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Removing unused gcfg variable**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/predict.py
@@ -103,4 +103,2 @@
         pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
-    gcfg = pipeline.graph_cfg
-
     data = pipeline.featurize_mol(mol, y=None)
apply_patch(auto_approved=true) exited 0 in 49ms:
Success. Updated the following files:
M src/gnn/predict.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Refactoring evaluation to use feature pipeline**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/evaluate.py
@@ -11,2 +11,3 @@
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
@@ -16,3 +17,2 @@
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
@@ -67,3 +67,3 @@
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
@@ -78,3 +78,3 @@
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
@@ -146,8 +146,9 @@
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
apply_patch(auto_approved=true) exited 0 in 55ms:
Success. Updated the following files:
M src/gnn/evaluate.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Cleaning up unused imports**
exec
/bin/zsh -lc 'rg -n "FPConfig" src/fp/predict.py src/fp/evaluate.py' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 318ms:

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Checking import usage in fp/predict**
exec
/bin/zsh -lc "rg -n \"np\\.\" src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction exited 1 in 278ms:

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Analyzing unused imports for cleanup**
exec
/bin/zsh -lc 'rg -n "pickle" src/gnn/evaluate.py' in /Users/kawahito/Desktop/LJ_prediction succeeded in 283ms:
4:import pickle
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Cleaning up unused imports**
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Checking import order**
exec
/bin/zsh -lc "sed -n '1,40p' src/fp/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 284ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import morgan_bitvect
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
from src.utils.validate_config import validate_config
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..dc0dfb5a77d04314b856ef83ccc28cdff4e853ac
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -11,10 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +94,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    x, meta = featurize_mol(mol, fp_cfg)
-
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Removing Optional and numpy dependencies**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/fp/predict.py
@@ -5,5 +5,4 @@
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
apply_patch(auto_approved=true) exited 0 in 57ms:
Success. Updated the following files:
M src/fp/predict.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..933edff20cb97d9256d267834bd0860cd5b4d4f1
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +65,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +76,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +144,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Cleaning up unused imports**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/evaluate.py
@@ -3,3 +3,2 @@
 import argparse
-import pickle
 from pathlib import Path
apply_patch(auto_approved=true) exited 0 in 51ms:
Success. Updated the following files:
M src/gnn/evaluate.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Reviewing imports and variable usage across modules**
exec
/bin/zsh -lc 'rg -n "fp_cfg" src/fp/evaluate.py' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 293ms:

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Checking pipeline definition**
exec
/bin/zsh -lc "sed -n '60,160p' src/fp/evaluate.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 261ms:

    train_meta = load_meta(model_artifact_dir)
    train_context = resolve_training_context(train_cfg, train_meta, model_artifact_dir)
    dataset_hash = train_context.get("dataset_hash") or compute_dataset_hash(dataset_csv, indices_dir)
    meta = build_meta(
        process_name=str(cfg.get("process", {}).get("name", "evaluate")),
        cfg=cfg,
        upstream_artifacts=[str(model_artifact_dir)],
        dataset_hash=dataset_hash,
        model_version=train_context.get("model_version"),
        extra={
            "task_name": train_context.get("task_name"),
            "model_name": train_context.get("model_name"),
            "featureset_name": train_context.get("featureset_name"),
        },
    )
    save_meta(run_dir, meta)

    df = pd.read_csv(dataset_csv)
    indices = load_split_indices(indices_dir)
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Verifying pipeline usage and imports**
exec
/bin/zsh -lc "sed -n '1,30p' src/gnn/predict.py" in /Users/kawahito/Desktop/LJ_prediction succeeded in 270ms:
from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd

from src.common.ad import applicability_domain
from src.common.chemistry import get_elements_from_mol
from src.common.config import dump_yaml, load_config
from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
from src.common.meta import build_meta, save_meta
from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
from src.common.utils import ensure_dir, get_logger, save_json
from src.fp.featurizer_fp import morgan_bitvect
from src.gnn.models import GCNRegressor, MPNNRegressor
from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Checking numpy usage in gnn predict.py**
exec
/bin/zsh -lc "rg -n \"np\\.\" src/gnn/predict.py" in /Users/kawahito/Desktop/LJ_prediction exited 1 in 287ms:

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..005f85f7ee4f15f73c94303d9160a05a3d75ec23
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -11,11 +11,11 @@
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -97,12 +97,11 @@
     mol = load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
-
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
 
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Removing unused numpy import**
file update
M /Users/kawahito/Desktop/LJ_prediction/src/gnn/predict.py
@@ -7,3 +7,2 @@
 
-import numpy as np
 import pandas as pd
apply_patch(auto_approved=true) exited 0 in 40ms:
Success. Updated the following files:
M src/gnn/predict.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Reviewing imports and feature pipeline usage**
exec
/bin/zsh -lc 'rg -n "feature_pipeline" -S src' in /Users/kawahito/Desktop/LJ_prediction succeeded in 311ms:
src/gnn/predict.py:13:from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
src/gnn/predict.py:100:    pipeline = load_feature_pipeline(artifacts_dir)
src/gnn/evaluate.py:11:from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
src/gnn/evaluate.py:146:    pipeline = load_feature_pipeline(artifacts_dir)
src/gnn/train.py:14:from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
src/gnn/train.py:384:    save_feature_pipeline(pipeline, artifacts_dir)
src/fp/evaluate.py:12:from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
src/fp/evaluate.py:81:    pipeline = load_feature_pipeline(artifacts_dir)
src/common/feature_pipeline.py:17:FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
src/common/feature_pipeline.py:181:def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
src/common/feature_pipeline.py:184:    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
src/common/feature_pipeline.py:190:def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
src/common/feature_pipeline.py:192:    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
src/fp/train.py:14:from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
src/fp/train.py:204:    save_feature_pipeline(pipeline, artifacts_dir)
src/fp/predict.py:13:from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
src/fp/predict.py:96:    pipeline = load_feature_pipeline(artifacts_dir)
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Updating evaluation script usage**
Plan update
  ✓ Create feature pipeline module with FP/GNN adapters, state save/load helpers, and backward-compatible artifact loading hooks.
  ✓ Refactor FP/GNN train/evaluate/predict to use the pipeline for featurization + preprocessing and to persist pipeline artifacts.
  → Add a smoke test for pipeline save/load consistency and update queue status.
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)


thinking
**Designing and outlining feature pipeline tests**
file update
A /Users/kawahito/Desktop/LJ_prediction/tests/test_feature_pipeline.py
from __future__ import annotations

from pathlib import Path

import numpy as np
import pytest

from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
from src.common.io import load_sdf_mol, sdf_path_from_cas


def _load_fixture_mol(cas: str):
    root = Path(__file__).resolve().parents[1]
    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))


def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
    pytest.importorskip("rdkit")

    cfg = {
        "featurizer": {
            "fingerprint": "morgan",
            "morgan_radius": 2,
            "n_bits": 256,
            "use_counts": False,
        },
        "preprocess": {"impute_nan": "mean", "standardize": True},
    }

    pipeline = FingerprintFeaturePipeline.from_config(cfg)
    mol_a = _load_fixture_mol("64-17-5")
    mol_b = _load_fixture_mol("67-64-1")
    assert mol_a is not None
    assert mol_b is not None

    x1, _ = pipeline.featurize_mol(mol_a)
    x2, _ = pipeline.featurize_mol(mol_b)
    X_train = np.vstack([x1, x2]).astype(float)
    X_train[0, 0] = np.nan

    pipeline.fit(X_train)
    x1_t, meta1 = pipeline.transform_mol(mol_a)

    save_feature_pipeline(pipeline, tmp_path)
    loaded = load_feature_pipeline(tmp_path)
    assert isinstance(loaded, FingerprintFeaturePipeline)

    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
    assert meta1["fp_type"] == meta2["fp_type"]
apply_patch(auto_approved=true) exited 0 in 81ms:
Success. Updated the following files:
A tests/test_feature_pipeline.py
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]


thinking
**Preparing to update task queue status**
exec
/bin/zsh -lc "python - <<'PY'
from datetime import datetime, timezone
print(datetime.now(timezone.utc).isoformat().replace('+00:00','Z'))
PY" in /Users/kawahito/Desktop/LJ_prediction succeeded in 290ms:
2025-12-26T14:22:47.066549Z
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]


thinking
**Updating task status in JSON**
file update
M /Users/kawahito/Desktop/LJ_prediction/work/queue.json
@@ -2,3 +2,3 @@
   "version": 1,
-  "updated_at": "2025-12-26T14:09:10.649769Z",
+  "updated_at": "2025-12-26T14:22:47.066549Z",
   "policy": {
@@ -127,3 +127,3 @@
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "FeaturePipeline 統一（train/infer skew排除）",
apply_patch(auto_approved=true) exited 0 in 58ms:
Success. Updated the following files:
M work/queue.json
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]
diff --git a/work/queue.json b/work/queue.json
index 275e7b908798ee05301d71c1c4ba2452dfdabfdd..cd2e139ae86d7c570dcd0d3f900c8db2bb79d5a3
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:09:10.649769Z",
+  "updated_at": "2025-12-26T14:22:47.066549Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -125,7 +125,7 @@
     {
       "id": "030",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "FeaturePipeline 統一（train/infer skew排除）",
       "path": "work/tasks/030_featurepipeline_unify.md",
       "skills": [

file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]
diff --git a/work/queue.json b/work/queue.json
index 275e7b908798ee05301d71c1c4ba2452dfdabfdd..cd2e139ae86d7c570dcd0d3f900c8db2bb79d5a3
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:09:10.649769Z",
+  "updated_at": "2025-12-26T14:22:47.066549Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -125,7 +125,7 @@
     {
       "id": "030",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "FeaturePipeline 統一（train/infer skew排除）",
       "path": "work/tasks/030_featurepipeline_unify.md",
       "skills": [


thinking
**Updating plan with completed step**
Plan update
  ✓ Create feature pipeline module with FP/GNN adapters, state save/load helpers, and backward-compatible artifact loading hooks.
  ✓ Refactor FP/GNN train/evaluate/predict to use the pipeline for featurization + preprocessing and to persist pipeline artifacts.
  ✓ Add a smoke test for pipeline save/load consistency and update queue status.
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]
diff --git a/work/queue.json b/work/queue.json
index 275e7b908798ee05301d71c1c4ba2452dfdabfdd..cd2e139ae86d7c570dcd0d3f900c8db2bb79d5a3
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:09:10.649769Z",
+  "updated_at": "2025-12-26T14:22:47.066549Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -125,7 +125,7 @@
     {
       "id": "030",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "FeaturePipeline 統一（train/infer skew排除）",
       "path": "work/tasks/030_featurepipeline_unify.md",
       "skills": [


thinking
**Consolidating final plan and testing approach**

I'm wrapping up the change plan, including files affected and compatibility considerations, especially about the new pipeline artifact. I’m carefully checking type safety and fallback logic in pipeline loading across both FP and GNN contexts. Finally, I’m preparing to run targeted tests despite possible missing dependencies, to verify the feature pipeline changes and provide a verification command in the output.
exec
/bin/zsh -lc 'python -m pytest tests/test_feature_pipeline.py' in /Users/kawahito/Desktop/LJ_prediction exited 1 in 337ms:
/Users/kawahito/Desktop/LJ_prediction/.venv/bin/python: No module named pytest
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]
diff --git a/work/queue.json b/work/queue.json
index 275e7b908798ee05301d71c1c4ba2452dfdabfdd..cd2e139ae86d7c570dcd0d3f900c8db2bb79d5a3
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:09:10.649769Z",
+  "updated_at": "2025-12-26T14:22:47.066549Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -125,7 +125,7 @@
     {
       "id": "030",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "FeaturePipeline 統一（train/infer skew排除）",
       "path": "work/tasks/030_featurepipeline_unify.md",
       "skills": [


thinking
**Preparing detailed final report**

I’m putting together a comprehensive final message that includes a clear explanation of unifying the FP/GNN feature pipeline, notes on skills used, and structured sections covering the change plan, implementation highlights, tests attempted, verification commands, compatibility impact, and queue updates. Since pytest isn’t installed and can’t be added due to network restrictions, I’ll include that failure and provide suggested next steps in a numbered list. This detailed layout will help the user understand the full scope and state of the change.
codex
Unified FP/GNN preprocessing+featurization behind a shared feature pipeline so train/evaluate/predict reuse the same fitted state, and added a pipeline roundtrip smoke test.  
Using S30_preprocess_pipeline, S40_featurizer, S95_tests_ci to enforce train/infer parity and add a minimal test.

**Change Plan**
- `src/common/feature_pipeline.py`: new shared pipeline module for FP/GNN, with save/load helpers.
- `src/fp/train.py`, `src/fp/evaluate.py`, `src/fp/predict.py`: switch to pipeline fit/transform + artifact save/load.
- `src/gnn/train.py`, `src/gnn/evaluate.py`, `src/gnn/predict.py`: switch to pipeline featurizer + artifact save/load.
- `tests/test_feature_pipeline.py`: add pipeline save/load consistency test.
- `work/queue.json`: mark task 030 done.

**Implementation**
- `src/common/feature_pipeline.py`: added `FingerprintFeaturePipeline` and `GraphFeaturePipeline`, plus `save_feature_pipeline`/`load_feature_pipeline` and backward-compatible artifact loaders for imputer/scaler or `graph_featurizer.pkl`.
- `src/fp/train.py`: build features and preprocessing through the pipeline and persist `artifacts/feature_pipeline.pkl` while keeping `imputer.pkl`/`scaler.pkl` for compatibility.
- `src/fp/evaluate.py` and `src/fp/predict.py`: load the saved pipeline first, with fallback to legacy artifacts when `feature_pipeline.pkl` is missing.
- `src/gnn/train.py`: featurize via pipeline and persist `artifacts/feature_pipeline.pkl` alongside `graph_featurizer.pkl`.
- `src/gnn/evaluate.py` and `src/gnn/predict.py`: use pipeline-first loading with fallback to `graph_featurizer.pkl`.

**Tests**
- Added `tests/test_feature_pipeline.py` to verify pipeline save/load consistency.
- Test run failed: `python -m pytest tests/test_feature_pipeline.py` → `No module named pytest`.

**Verification Command**
- `python -m pytest tests/test_feature_pipeline.py`

**Compatibility Impact**
- Artifacts: new `artifacts/feature_pipeline.pkl` is written on train; existing `artifacts/imputer.pkl`, `artifacts/scaler.pkl`, and `artifacts/graph_featurizer.pkl` remain for backward compatibility.
- Config/CLI: no changes.
- Inference: pipeline is loaded from training artifacts; legacy artifacts are still supported if the new pipeline file is absent.

**Queue**
- Updated `work/queue.json` to set task 030 to `done`.

Next steps (optional):
1) Install pytest in the venv and re-run the test command.  
2) Run a quick train→predict pass with `configs/fp/train_fixture.yaml` and `configs/fp/predict_fixture.yaml` to confirm pipeline artifacts in real runs.
file update:
diff --git a/src/common/feature_pipeline.py b/src/common/feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..875ff04d04a809c34283e06c5058613b29799611
--- /dev/null
+++ b/src/common/feature_pipeline.py
@@ -0,0 +1,196 @@
+from __future__ import annotations
+
+import pickle
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, Optional, Tuple
+
+import numpy as np
+from sklearn.impute import SimpleImputer
+from sklearn.preprocessing import StandardScaler
+
+from src.fp.feature_utils import build_features
+from src.fp.featurizer_fp import FPConfig, featurize_mol
+from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
+
+PIPELINE_VERSION = 1
+FEATURE_PIPELINE_FILENAME = "feature_pipeline.pkl"
+
+
+class FeaturePipelineError(RuntimeError):
+    pass
+
+
+def _normalize_list(value: Any) -> Optional[list[str]]:
+    if value is None:
+        return None
+    if isinstance(value, (list, tuple)):
+        return [str(v) for v in value]
+    return [str(value)]
+
+
+def build_fp_config(feat_cfg: Dict[str, Any]) -> FPConfig:
+    add_desc = _normalize_list(feat_cfg.get("add_descriptors", None))
+    return FPConfig(
+        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
+        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
+        n_bits=int(feat_cfg.get("n_bits", 2048)),
+        use_counts=bool(feat_cfg.get("use_counts", False)),
+        add_descriptors=add_desc,
+    )
+
+
+def build_graph_config(feat_cfg: Dict[str, Any]) -> GraphFeaturizerConfig:
+    node_features = feat_cfg.get(
+        "node_features",
+        ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"],
+    )
+    edge_features = feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])
+    return GraphFeaturizerConfig(
+        node_features=[str(v) for v in node_features],
+        edge_features=[str(v) for v in edge_features],
+        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
+        add_global_descriptors=_normalize_list(feat_cfg.get("add_global_descriptors", None)),
+    )
+
+
+@dataclass
+class FingerprintFeaturePipeline:
+    fp_cfg: FPConfig = field(default_factory=FPConfig)
+    impute_strategy: str = "mean"
+    standardize: bool = False
+    imputer: Optional[SimpleImputer] = None
+    scaler: Optional[StandardScaler] = None
+    feature_meta: Optional[Dict[str, Any]] = None
+    pipeline_type: str = field(default="fp", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "FingerprintFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        preprocess_cfg = cfg.get("preprocess", {}) or {}
+        return cls(
+            fp_cfg=build_fp_config(feat_cfg),
+            impute_strategy=str(preprocess_cfg.get("impute_nan", "mean")),
+            standardize=bool(preprocess_cfg.get("standardize", False)),
+        )
+
+    def build_features(
+        self,
+        df,
+        sdf_dir: Path,
+        cas_col: str,
+        cache_dir: Optional[Path],
+        cache_key: str,
+        logger,
+    ) -> Tuple[np.ndarray, list[Any], list[str], Dict[str, Any]]:
+        X, ids, elements, meta = build_features(
+            df=df,
+            sdf_dir=sdf_dir,
+            cas_col=cas_col,
+            fp_cfg=self.fp_cfg,
+            cache_dir=cache_dir,
+            cache_key=cache_key,
+            logger=logger,
+        )
+        self.feature_meta = meta
+        return X, ids, elements, meta
+
+    def fit(self, X: np.ndarray) -> None:
+        self.imputer = SimpleImputer(strategy=self.impute_strategy)
+        X_imp = self.imputer.fit_transform(X)
+        if self.standardize:
+            self.scaler = StandardScaler()
+            self.scaler.fit(X_imp)
+        else:
+            self.scaler = None
+
+    def transform_features(self, X: np.ndarray) -> np.ndarray:
+        if self.imputer is None:
+            raise FeaturePipelineError("FingerprintFeaturePipeline is not fitted (imputer is missing).")
+        X_imp = self.imputer.transform(X)
+        if self.scaler is not None:
+            X_imp = self.scaler.transform(X_imp)
+        return X_imp
+
+    def fit_transform_features(self, X: np.ndarray) -> np.ndarray:
+        self.fit(X)
+        return self.transform_features(X)
+
+    def featurize_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        return featurize_mol(mol, self.fp_cfg)
+
+    def transform_mol(self, mol) -> Tuple[np.ndarray, Dict[str, Any]]:
+        x, meta = self.featurize_mol(mol)
+        X = self.transform_features(x.reshape(1, -1))
+        return X.reshape(-1), meta
+
+    def save_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        artifacts_dir.mkdir(parents=True, exist_ok=True)
+        if self.imputer is None:
+            raise FeaturePipelineError("Cannot save preprocess artifacts: imputer is missing.")
+        with open(artifacts_dir / "imputer.pkl", "wb") as f:
+            pickle.dump(self.imputer, f)
+        if self.scaler is not None:
+            with open(artifacts_dir / "scaler.pkl", "wb") as f:
+                pickle.dump(self.scaler, f)
+
+    def load_preprocess_artifacts(self, artifacts_dir: Path) -> None:
+        artifacts_dir = Path(artifacts_dir)
+        imputer_path = artifacts_dir / "imputer.pkl"
+        if not imputer_path.exists():
+            raise FileNotFoundError(f"imputer.pkl not found: {imputer_path}")
+        with open(imputer_path, "rb") as f:
+            self.imputer = pickle.load(f)
+        scaler_path = artifacts_dir / "scaler.pkl"
+        if scaler_path.exists():
+            with open(scaler_path, "rb") as f:
+                self.scaler = pickle.load(f)
+        else:
+            self.scaler = None
+
+
+@dataclass
+class GraphFeaturePipeline:
+    graph_cfg: GraphFeaturizerConfig
+    pipeline_type: str = field(default="gnn", init=False)
+    version: int = field(default=PIPELINE_VERSION, init=False)
+
+    @classmethod
+    def from_config(cls, cfg: Dict[str, Any]) -> "GraphFeaturePipeline":
+        feat_cfg = cfg.get("featurizer", {}) or {}
+        return cls(build_graph_config(feat_cfg))
+
+    @classmethod
+    def from_artifacts(cls, artifacts_dir: Path) -> "GraphFeaturePipeline":
+        artifacts_dir = Path(artifacts_dir)
+        cfg_path = artifacts_dir / "graph_featurizer.pkl"
+        if not cfg_path.exists():
+            raise FileNotFoundError(f"graph_featurizer.pkl not found: {cfg_path}")
+        with open(cfg_path, "rb") as f:
+            gcfg = pickle.load(f)
+        if not isinstance(gcfg, GraphFeaturizerConfig):
+            raise FeaturePipelineError("graph_featurizer.pkl does not contain a GraphFeaturizerConfig.")
+        return cls(graph_cfg=gcfg)
+
+    def featurize_mol(self, mol, y: Optional[float] = None):
+        return featurize_mol_to_pyg(mol, y=y, cfg=self.graph_cfg)
+
+
+def save_feature_pipeline(pipeline: Any, artifacts_dir: Path) -> Path:
+    artifacts_dir = Path(artifacts_dir)
+    artifacts_dir.mkdir(parents=True, exist_ok=True)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    with open(path, "wb") as f:
+        pickle.dump(pipeline, f)
+    return path
+
+
+def load_feature_pipeline(artifacts_dir: Path) -> Optional[Any]:
+    artifacts_dir = Path(artifacts_dir)
+    path = artifacts_dir / FEATURE_PIPELINE_FILENAME
+    if not path.exists():
+        return None
+    with open(path, "rb") as f:
+        return pickle.load(f)
diff --git a/src/fp/evaluate.py b/src/fp/evaluate.py
index e986882bd1266692c2bd6157e552759501260e64..bcebaa06140566f5d91001e8b48623f79c68880f
--- a/src/fp/evaluate.py
+++ b/src/fp/evaluate.py
@@ -9,12 +9,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig
+from src.fp.feature_utils import hash_cfg
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -78,36 +78,24 @@
     df = pd.read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
     feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
 
     cache_dir = data_cfg.get("cache_dir", None)
     cache_dir = Path(cache_dir) if cache_dir else None
 
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, _, _, _ = build_features(
+    X_all, _, _, _ = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
     )
-
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
 
@@ -133,9 +121,7 @@
             metrics_by_split[split_name] = {}
             continue
 
-        X = imputer.transform(X)
-        if scaler is not None:
-            X = scaler.transform(X)
+        X = pipeline.transform_features(X)
         preds = model.predict(X)
 
         metrics_by_split[split_name] = regression_metrics(y, preds)
diff --git a/src/fp/predict.py b/src/fp/predict.py
index c1044d000850069b1a5c5742c59101cae4873e6f..b84913479f7331f3b58f3d39dcd918fd7e8ec4cd
--- a/src/fp/predict.py
+++ b/src/fp/predict.py
@@ -3,18 +3,18 @@
 import argparse
 import pickle
 from pathlib import Path
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.fp.featurizer_fp import FPConfig, featurize_mol, morgan_bitvect
+from src.fp.featurizer_fp import morgan_bitvect
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
 
@@ -93,34 +93,15 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    feat_cfg = train_cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    x, meta = featurize_mol(mol, fp_cfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, FingerprintFeaturePipeline) or pipeline.imputer is None:
+        pipeline = FingerprintFeaturePipeline.from_config(train_cfg)
+        pipeline.load_preprocess_artifacts(artifacts_dir)
 
-    # Load preprocessing + model
-    with open(artifacts_dir / "imputer.pkl", "rb") as f:
-        imputer = pickle.load(f)
-    scaler = None
-    scaler_path = artifacts_dir / "scaler.pkl"
-    if scaler_path.exists():
-        with open(scaler_path, "rb") as f:
-            scaler = pickle.load(f)
+    x, meta = pipeline.transform_mol(mol)
     with open(artifacts_dir / "model.pkl", "rb") as f:
         model = pickle.load(f)
-
-    X = x.reshape(1, -1)
-    X = imputer.transform(X)
-    if scaler is not None:
-        X = scaler.transform(X)
-
-    pred = float(model.predict(X).reshape(-1)[0])
+    pred = float(model.predict(x.reshape(1, -1)).reshape(-1)[0])
     pred_df = pd.DataFrame(
         [
             {
diff --git a/src/fp/train.py b/src/fp/train.py
index 3b3c82e1b950c20f64ff8de6a6b7021261a4a65f..8f2bf5060456293c53efc97b7c3547e23262e7b7
--- a/src/fp/train.py
+++ b/src/fp/train.py
@@ -9,19 +9,17 @@
 import numpy as np
 import pandas as pd
 
-from sklearn.impute import SimpleImputer
-from sklearn.preprocessing import StandardScaler
-
 from src.common.ad import applicability_domain
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import FingerprintFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.fp.feature_utils import build_features, hash_cfg
-from src.fp.featurizer_fp import FPConfig, morgan_bitvect
+from src.fp.feature_utils import hash_cfg
+from src.fp.featurizer_fp import morgan_bitvect
 from src.fp.models import get_model
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -81,17 +79,8 @@
     logger.info(f"Split sizes: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
 
     feat_cfg = cfg.get("featurizer", {})
-    fp_cfg = FPConfig(
-        fingerprint=str(feat_cfg.get("fingerprint", "morgan")),
-        morgan_radius=int(feat_cfg.get("morgan_radius", 2)),
-        n_bits=int(feat_cfg.get("n_bits", 2048)),
-        use_counts=bool(feat_cfg.get("use_counts", False)),
-        add_descriptors=feat_cfg.get("add_descriptors", None),
-    )
-
-    preprocess_cfg = cfg.get("preprocess", {})
-    standardize = bool(preprocess_cfg.get("standardize", False))
-    impute_strategy = str(preprocess_cfg.get("impute_nan", "mean"))
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    fp_cfg = pipeline.fp_cfg
 
     model_cfg = cfg.get("model", {})
     model_name = str(model_cfg.get("name", "lightgbm"))
@@ -99,11 +88,10 @@
 
     # Build features for all rows once (for caching + AD)
     cache_key = hash_cfg({"featurizer": feat_cfg, "dataset": str(dataset_csv)})
-    X_all, ids_all, elements_all, feat_meta = build_features(
+    X_all, ids_all, elements_all, feat_meta = pipeline.build_features(
         df=df,
         sdf_dir=sdf_dir,
         cas_col=cas_col,
-        fp_cfg=fp_cfg,
         cache_dir=cache_dir,
         cache_key=cache_key,
         logger=logger,
@@ -137,18 +125,11 @@
     if len(y_train) < 50:
         logger.warning("Training set is very small; consider loosening selectors or split strategy.")
 
-    # Impute NaNs (descriptors may yield NaN)
-    imputer = SimpleImputer(strategy=impute_strategy)
-    X_train = imputer.fit_transform(X_train)
-    X_val = imputer.transform(X_val)
-    X_test = imputer.transform(X_test)
-
-    scaler = None
-    if standardize:
-        scaler = StandardScaler()
-        X_train = scaler.fit_transform(X_train)
-        X_val = scaler.transform(X_val)
-        X_test = scaler.transform(X_test)
+    # Impute NaNs (descriptors may yield NaN) + optional standardization
+    pipeline.fit(X_train)
+    X_train = pipeline.transform_features(X_train)
+    X_val = pipeline.transform_features(X_val)
+    X_test = pipeline.transform_features(X_test)
 
     model = get_model(model_name, model_params)
     logger.info(f"Training model: {model_name} with params={model_params}")
@@ -220,21 +201,19 @@
 
     # Save artifacts
     artifacts_dir = ensure_dir(run_dir / "artifacts")
+    save_feature_pipeline(pipeline, artifacts_dir)
+    pipeline.save_preprocess_artifacts(artifacts_dir)
     model_path = artifacts_dir / "model.pkl"
     with open(model_path, "wb") as f:
         pickle.dump(model, f)
-
-    with open(artifacts_dir / "imputer.pkl", "wb") as f:
-        pickle.dump(imputer, f)
-    if scaler is not None:
-        with open(artifacts_dir / "scaler.pkl", "wb") as f:
-            pickle.dump(scaler, f)
 
     # AD artifacts: training fingerprints and training elements
     # Compute Morgan bitvect for all training mols in ORIGINAL dataset order to simplify indexing.
     # We'll store only training set for speed.
     train_mols = [load_sdf_mol(sdf_path_from_cas(sdf_dir, cas)) for cas in ids_train]
-    train_fps = [morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols]
+    train_fps = [
+        morgan_bitvect(m, radius=fp_cfg.morgan_radius, n_bits=fp_cfg.n_bits) if m is not None else None for m in train_mols
+    ]
     # Filter None
     train_pairs = [(fp, cas) for fp, cas in zip(train_fps, ids_train) if fp is not None]
     train_fps = [p[0] for p in train_pairs]
@@ -278,10 +257,10 @@
     with open(model_dir / "preprocess.pkl", "wb") as f:
         pickle.dump(
             {
-                "imputer": imputer,
-                "scaler": scaler,
-                "standardize": standardize,
-                "impute_strategy": impute_strategy,
+                "imputer": pipeline.imputer,
+                "scaler": pipeline.scaler,
+                "standardize": pipeline.standardize,
+                "impute_strategy": pipeline.impute_strategy,
             },
             f,
         )
diff --git a/src/gnn/evaluate.py b/src/gnn/evaluate.py
index 57fb15bdbe4b4e44a853f78874258afbf7528800..83baba26b1584063e7de2d408d75b281a1387640
--- a/src/gnn/evaluate.py
+++ b/src/gnn/evaluate.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import argparse
-import pickle
 from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
@@ -9,12 +8,12 @@
 import pandas as pd
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.meta import build_meta, save_meta
 from src.common.metrics import regression_metrics
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -65,7 +64,7 @@
     sdf_dir: Path,
     cas_col: str,
     target_col: str,
-    gcfg,
+    pipeline: GraphFeaturePipeline,
 ) -> Tuple[List[Any], List[str]]:
     split_idx = indices.get(split_name, [])
     split_df = df.loc[split_idx]
@@ -76,7 +75,7 @@
         if mol is None or not np.isfinite(y):
             continue
         try:
-            data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+            data = pipeline.featurize_mol(mol, y=y)
             data_list.append(data)
             ids.append(cas)
         except Exception:
@@ -144,12 +143,13 @@
     df = read_csv(dataset_csv)
     indices = load_split_indices(indices_dir)
 
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
 
-    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, gcfg)
-    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, gcfg)
-    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, gcfg)
+    train_data, train_ids = _build_dataset(df, indices, "train", sdf_dir, cas_col, target_col, pipeline)
+    val_data, val_ids = _build_dataset(df, indices, "val", sdf_dir, cas_col, target_col, pipeline)
+    test_data, test_ids = _build_dataset(df, indices, "test", sdf_dir, cas_col, target_col, pipeline)
 
     logger.info(f"Data sizes: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}")
 
diff --git a/src/gnn/predict.py b/src/gnn/predict.py
index 52a33876f8b2761f45925000e6c08d2b2effb8de..e55c92f4b02f97939bc9bd6790f3af2802f30ef5
--- a/src/gnn/predict.py
+++ b/src/gnn/predict.py
@@ -5,17 +5,16 @@
 from pathlib import Path
 from typing import Any, Dict, Tuple
 
-import numpy as np
 import pandas as pd
 
 from src.common.ad import applicability_domain
 from src.common.chemistry import get_elements_from_mol
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, load_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.utils import ensure_dir, get_logger, save_json
 from src.fp.featurizer_fp import morgan_bitvect
-from src.gnn.featurizer_graph import featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash, load_meta, resolve_training_context
 from src.utils.validate_config import validate_config
@@ -98,11 +97,10 @@
     if mol is None:
         raise FileNotFoundError(f"SDF not found or invalid for CAS={cas} in {sdf_dir}")
 
-    # Load featurizer config
-    with open(artifacts_dir / "graph_featurizer.pkl", "rb") as f:
-        gcfg = pickle.load(f)
-
-    data = featurize_mol_to_pyg(mol, y=None, cfg=gcfg)
+    pipeline = load_feature_pipeline(artifacts_dir)
+    if not isinstance(pipeline, GraphFeaturePipeline):
+        pipeline = GraphFeaturePipeline.from_artifacts(artifacts_dir)
+    data = pipeline.featurize_mol(mol, y=None)
 
     model_cfg = train_cfg.get("model", {})
     model_name = str(model_cfg.get("name", "mpnn")).lower()
diff --git a/src/gnn/train.py b/src/gnn/train.py
index 50abfd1d0053f8c49a09dca7d17011a2c286c740..7b27372d7952ba8f9a3098e0713cd81b6b4f8f5f
--- a/src/gnn/train.py
+++ b/src/gnn/train.py
@@ -11,13 +11,13 @@
 from tqdm import tqdm
 
 from src.common.config import dump_yaml, load_config
+from src.common.feature_pipeline import GraphFeaturePipeline, save_feature_pipeline
 from src.common.meta import build_meta, save_meta
 from src.common.io import load_sdf_mol, read_csv, sdf_path_from_cas
 from src.common.metrics import regression_metrics
 from src.common.plots import save_learning_curve, save_parity_plot, save_residual_plot, save_hist
 from src.common.splitters import load_split_indices
 from src.common.utils import ensure_dir, get_logger, save_json, set_seed
-from src.gnn.featurizer_graph import GraphFeaturizerConfig, featurize_mol_to_pyg
 from src.gnn.models import GCNRegressor, MPNNRegressor
 from src.utils.artifacts import compute_dataset_hash
 from src.utils.validate_config import validate_config
@@ -168,13 +168,8 @@
     save_meta(run_dir, meta)
 
     # Featurizer config
-    feat_cfg = cfg.get("featurizer", {})
-    gcfg = GraphFeaturizerConfig(
-        node_features=list(feat_cfg.get("node_features", ["atomic_num", "degree", "formal_charge", "aromatic", "num_h", "in_ring"])),
-        edge_features=list(feat_cfg.get("edge_features", ["bond_type", "conjugated", "aromatic"])),
-        use_3d_pos=bool(feat_cfg.get("use_3d_pos", True)),
-        add_global_descriptors=feat_cfg.get("add_global_descriptors", None),
-    )
+    pipeline = GraphFeaturePipeline.from_config(cfg)
+    gcfg = pipeline.graph_cfg
 
     def build_dataset(split_name: str) -> List[Any]:
         split_df = df.loc[indices[split_name]]
@@ -184,7 +179,7 @@
             if mol is None or not np.isfinite(y):
                 continue
             try:
-                data = featurize_mol_to_pyg(mol, y=y, cfg=gcfg)
+                data = pipeline.featurize_mol(mol, y=y)
                 data_list.append(data)
             except Exception:
                 continue
@@ -386,6 +381,7 @@
         save_hist([d.y.item() for d in train_data], plots_dir / "y_train_hist.png", title="Target distribution (train)", xlabel=target_col)
 
     # Save artifacts
+    save_feature_pipeline(pipeline, artifacts_dir)
     with open(artifacts_dir / "graph_featurizer.pkl", "wb") as f:
         pickle.dump(gcfg, f)
     dump_yaml(run_dir / "config_snapshot.yaml", cfg)
diff --git a/tests/test_feature_pipeline.py b/tests/test_feature_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..e10e98b1ecd06cedca4489d04ca0fbff7367b54c
--- /dev/null
+++ b/tests/test_feature_pipeline.py
@@ -0,0 +1,51 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import numpy as np
+import pytest
+
+from src.common.feature_pipeline import FingerprintFeaturePipeline, load_feature_pipeline, save_feature_pipeline
+from src.common.io import load_sdf_mol, sdf_path_from_cas
+
+
+def _load_fixture_mol(cas: str):
+    root = Path(__file__).resolve().parents[1]
+    sdf_dir = root / "tests" / "fixtures" / "data" / "raw" / "sdf_files"
+    return load_sdf_mol(sdf_path_from_cas(sdf_dir, cas))
+
+
+def test_fp_pipeline_roundtrip(tmp_path: Path) -> None:
+    pytest.importorskip("rdkit")
+
+    cfg = {
+        "featurizer": {
+            "fingerprint": "morgan",
+            "morgan_radius": 2,
+            "n_bits": 256,
+            "use_counts": False,
+        },
+        "preprocess": {"impute_nan": "mean", "standardize": True},
+    }
+
+    pipeline = FingerprintFeaturePipeline.from_config(cfg)
+    mol_a = _load_fixture_mol("64-17-5")
+    mol_b = _load_fixture_mol("67-64-1")
+    assert mol_a is not None
+    assert mol_b is not None
+
+    x1, _ = pipeline.featurize_mol(mol_a)
+    x2, _ = pipeline.featurize_mol(mol_b)
+    X_train = np.vstack([x1, x2]).astype(float)
+    X_train[0, 0] = np.nan
+
+    pipeline.fit(X_train)
+    x1_t, meta1 = pipeline.transform_mol(mol_a)
+
+    save_feature_pipeline(pipeline, tmp_path)
+    loaded = load_feature_pipeline(tmp_path)
+    assert isinstance(loaded, FingerprintFeaturePipeline)
+
+    x1_t_loaded, meta2 = loaded.transform_mol(mol_a)
+    assert np.allclose(x1_t, x1_t_loaded, equal_nan=True)
+    assert meta1["fp_type"] == meta2["fp_type"]
diff --git a/work/queue.json b/work/queue.json
index 275e7b908798ee05301d71c1c4ba2452dfdabfdd..cd2e139ae86d7c570dcd0d3f900c8db2bb79d5a3
--- a/work/queue.json
+++ b/work/queue.json
@@ -1,6 +1,6 @@
 {
   "version": 1,
-  "updated_at": "2025-12-26T14:09:10.649769Z",
+  "updated_at": "2025-12-26T14:22:47.066549Z",
   "policy": {
     "priority_order": [
       "P0",
@@ -125,7 +125,7 @@
     {
       "id": "030",
       "priority": "P0",
-      "status": "in_progress",
+      "status": "done",
       "title": "FeaturePipeline 統一（train/infer skew排除）",
       "path": "work/tasks/030_featurepipeline_unify.md",
       "skills": [

tokens used
154,859
